{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Qw0fb540Fcke",
        "pbeiQJEyP5NI",
        "0D00Gc9oP8fw",
        "4VnDx2P3XIXJ",
        "gKNnhIQb8cc9",
        "epclaj6T4U6a",
        "WPLsDv8E4crK",
        "gjRpZkWu8JUf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Readme"
      ],
      "metadata": {
        "id": "qb0ilw6QZUE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-requisites\n",
        "*   Set up weight and bias account: [Wandb](https://wandb.ai/site/)\n",
        "*   Upload dataset on drive following the hierarchy below: <br>\n",
        "<pre>\n",
        "MyDrive\n",
        "|_ Final_project\n",
        "   |_ 2000dataset.zip\n",
        "</pre>\n",
        "\n",
        "*   Upload the generator and the zip containing the source code in drive following the herachy below:\n",
        "<pre>\n",
        "MyDrive\n",
        "|_ Final_project\n",
        "   |_ HAT\n",
        "      |_ HAT.zip\n",
        "      |_Real_HAT_GAN_SRx4.pth\n",
        "</pre>\n",
        "\n",
        "*   for **inference** using generator obtained after fine tuning on the dataset, ensure net_g_5000.pth is at the hierarchy below:\n",
        "\n",
        " <pre>\n",
        "MyDrive\n",
        "|_ Final_project\n",
        "   |_ HAT\n",
        "      |_ net_g_5000.pth\n",
        "</pre>\n",
        "\n",
        "\n",
        "**Fine-Tuning**: Follow steps 1 to 4 in order. <br> <br>\n",
        "**Inference**:\n",
        "*   The HAT code and the generator is required for inference.\n",
        "*   Run step 1, step 2 and Step 5. *Note: At step 5.2, replace the dataroot_lq with correct LR image path to run inference on and prertain_network_g with correct generator path.*"
      ],
      "metadata": {
        "id": "tynv1wg-9amm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 Install dependencies and login to wandb"
      ],
      "metadata": {
        "id": "_ye5C6A-Pt0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq\n",
        "!pip install basicsr==1.3.4.9 -qqq\n",
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MumR80xdX1g4",
        "outputId": "03cfe2a7-d08d-44aa-d87f-5e71f24b7cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/161.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.2/161.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.0/303.0 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for basicsr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files, drive\n",
        "import os\n",
        "import wandb\n",
        "import shutil"
      ],
      "metadata": {
        "id": "N9RX-eq-QAIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XVrKozUZJ1o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a5526af-8b8c-4604-98d7-32150cf36743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mount drive to colab\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Corrects import line from degradations.py as this version is obsolete\n",
        "#https://github.com/xinntao/Real-ESRGAN/issues/801\n",
        "#Note: if python version was updated on colab, change the file_path to correct one.\n",
        "\n",
        "file_path = \"/usr/local/lib/python3.12/dist-packages/basicsr/data/degradations.py\"\n",
        "line_to_replace = \"from torchvision.transforms.functional_tensor import rgb_to_grayscale\"\n",
        "new_line = \"from torchvision.transforms.functional import rgb_to_grayscale\"\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "lines = [new_line if line.strip() == line_to_replace else line for line in lines]\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    file.writelines(lines)\n",
        "\n",
        "print(f\"Replacing line '{line_to_replace}' with '{new_line}' in file {file_path}\")"
      ],
      "metadata": {
        "id": "QlHzbRq7YvBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d8a2bd6-8562-459f-b4ba-d78dac36eaae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replacing line 'from torchvision.transforms.functional_tensor import rgb_to_grayscale' with 'from torchvision.transforms.functional import rgb_to_grayscale' in file /usr/local/lib/python3.12/dist-packages/basicsr/data/degradations.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and log in to Weights & Biases; not required if doing inference only\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "kJ1nyj8kZJpR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "4763ed4c-0e4f-4302-b677-106395e03336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdsha43925\u001b[0m (\u001b[33mdsha43925-middlesex-university-mauritius\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Get code"
      ],
      "metadata": {
        "id": "Qw0fb540Fcke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get code from drive and set up environment for HAT\n",
        "code_zip_path = '/content/drive/MyDrive/Final_project/HAT/HAT.zip'\n",
        "unzip_path = '/content'\n",
        "!unzip -q $code_zip_path -d $unzip_path\n",
        "!ls -l $unzip_path\n",
        "%cd /content/HAT\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop"
      ],
      "metadata": {
        "id": "Ofxyulr1bJnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3a2959f-92cc-4350-9d48-4ed9f38bd948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12\n",
            "drwx------ 5 root root 4096 Sep 23 19:40 drive\n",
            "drwxrwxrwx 7 root root 4096 Sep  4 06:15 HAT\n",
            "drwxr-xr-x 1 root root 4096 Sep 19 13:40 sample_data\n",
            "/content/HAT\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.8.0+cu126)\n",
            "Requirement already satisfied: basicsr==1.3.4.9 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.3.4.9)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (4.12.0.88)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.32.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.16.2)\n",
            "Requirement already satisfied: tb-nightly in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.21.0a20250923)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.43.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.7->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.7->-r requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr==1.3.4.9->-r requirements.txt (line 3)) (2025.8.3)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr==1.3.4.9->-r requirements.txt (line 3)) (2025.9.9)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr==1.3.4.9->-r requirements.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.75.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.9)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (5.29.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.1.3)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.12/dist-packages (from yapf->basicsr==1.3.4.9->-r requirements.txt (line 3)) (4.4.0)\n",
            "/usr/local/lib/python3.12/dist-packages/setuptools/__init__.py:94: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Requirements should be satisfied by a PEP 517 installer.\n",
            "        If you are using pip, you can try `pip install --use-pep517`.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  dist.fetch_build_eggs(dist.setup_requires)\n",
            "running develop\n",
            "/usr/local/lib/python3.12/dist-packages/setuptools/command/develop.py:41: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  easy_install.initialize_options(self)\n",
            "/usr/local/lib/python3.12/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running egg_info\n",
            "creating hat.egg-info\n",
            "writing hat.egg-info/PKG-INFO\n",
            "writing dependency_links to hat.egg-info/dependency_links.txt\n",
            "writing requirements to hat.egg-info/requires.txt\n",
            "writing top-level names to hat.egg-info/top_level.txt\n",
            "writing manifest file 'hat.egg-info/SOURCES.txt'\n",
            "reading manifest file 'hat.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'hat.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.12/dist-packages/hat.egg-link (link to .)\n",
            "Adding hat 0.1.0 to easy-install.pth file\n",
            "\n",
            "Installed /content/HAT\n",
            "Processing dependencies for hat==0.1.0\n",
            "Searching for basicsr==1.3.4.9\n",
            "Best match: basicsr 1.3.4.9\n",
            "Adding basicsr 1.3.4.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for torch==2.8.0+cu126\n",
            "Best match: torch 2.8.0+cu126\n",
            "Adding torch 2.8.0+cu126 to easy-install.pth file\n",
            "Installing torchfrtrace script to /usr/local/bin\n",
            "Installing torchrun script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for einops==0.8.1\n",
            "Best match: einops 0.8.1\n",
            "Adding einops 0.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for yapf==0.43.0\n",
            "Best match: yapf 0.43.0\n",
            "Adding yapf 0.43.0 to easy-install.pth file\n",
            "Installing yapf script to /usr/local/bin\n",
            "Installing yapf-diff script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for tqdm==4.67.1\n",
            "Best match: tqdm 4.67.1\n",
            "Adding tqdm 4.67.1 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for torchvision==0.23.0+cu126\n",
            "Best match: torchvision 0.23.0+cu126\n",
            "Adding torchvision 0.23.0+cu126 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for tb-nightly==2.21.0a20250923\n",
            "Best match: tb-nightly 2.21.0a20250923\n",
            "Adding tb-nightly 2.21.0a20250923 to easy-install.pth file\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for scipy==1.16.2\n",
            "Best match: scipy 1.16.2\n",
            "Adding scipy 1.16.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for scikit-image==0.25.2\n",
            "Best match: scikit-image 0.25.2\n",
            "Adding scikit-image 0.25.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for requests==2.32.4\n",
            "Best match: requests 2.32.4\n",
            "Adding requests 2.32.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for PyYAML==6.0.2\n",
            "Best match: PyYAML 6.0.2\n",
            "Adding PyYAML 6.0.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for pillow==11.3.0\n",
            "Best match: pillow 11.3.0\n",
            "Adding pillow 11.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for opencv-python==4.12.0.88\n",
            "Best match: opencv-python 4.12.0.88\n",
            "Adding opencv-python 4.12.0.88 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for numpy==2.0.2\n",
            "Best match: numpy 2.0.2\n",
            "Adding numpy 2.0.2 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing numpy-config script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for lmdb==1.7.3\n",
            "Best match: lmdb 1.7.3\n",
            "Adding lmdb 1.7.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for future==1.0.0\n",
            "Best match: future 1.0.0\n",
            "Adding future 1.0.0 to easy-install.pth file\n",
            "Installing futurize script to /usr/local/bin\n",
            "Installing pasteurize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for addict==2.4.0\n",
            "Best match: addict 2.4.0\n",
            "Adding addict 2.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for triton==3.4.0\n",
            "Best match: triton 3.4.0\n",
            "Adding triton 3.4.0 to easy-install.pth file\n",
            "Installing proton script to /usr/local/bin\n",
            "Installing proton-viewer script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cufile-cu12==1.11.1.6\n",
            "Best match: nvidia-cufile-cu12 1.11.1.6\n",
            "Adding nvidia-cufile-cu12 1.11.1.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-nvjitlink-cu12==12.6.85\n",
            "Best match: nvidia-nvjitlink-cu12 12.6.85\n",
            "Adding nvidia-nvjitlink-cu12 12.6.85 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-nvtx-cu12==12.6.77\n",
            "Best match: nvidia-nvtx-cu12 12.6.77\n",
            "Adding nvidia-nvtx-cu12 12.6.77 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-nccl-cu12==2.27.3\n",
            "Best match: nvidia-nccl-cu12 2.27.3\n",
            "Adding nvidia-nccl-cu12 2.27.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cusparselt-cu12==0.7.1\n",
            "Best match: nvidia-cusparselt-cu12 0.7.1\n",
            "Adding nvidia-cusparselt-cu12 0.7.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cusparse-cu12==12.5.4.2\n",
            "Best match: nvidia-cusparse-cu12 12.5.4.2\n",
            "Adding nvidia-cusparse-cu12 12.5.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cusolver-cu12==11.7.1.2\n",
            "Best match: nvidia-cusolver-cu12 11.7.1.2\n",
            "Adding nvidia-cusolver-cu12 11.7.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-curand-cu12==10.3.7.77\n",
            "Best match: nvidia-curand-cu12 10.3.7.77\n",
            "Adding nvidia-curand-cu12 10.3.7.77 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cufft-cu12==11.3.0.4\n",
            "Best match: nvidia-cufft-cu12 11.3.0.4\n",
            "Adding nvidia-cufft-cu12 11.3.0.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cublas-cu12==12.6.4.1\n",
            "Best match: nvidia-cublas-cu12 12.6.4.1\n",
            "Adding nvidia-cublas-cu12 12.6.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cudnn-cu12==9.10.2.21\n",
            "Best match: nvidia-cudnn-cu12 9.10.2.21\n",
            "Adding nvidia-cudnn-cu12 9.10.2.21 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cuda-cupti-cu12==12.6.80\n",
            "Best match: nvidia-cuda-cupti-cu12 12.6.80\n",
            "Adding nvidia-cuda-cupti-cu12 12.6.80 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cuda-runtime-cu12==12.6.77\n",
            "Best match: nvidia-cuda-runtime-cu12 12.6.77\n",
            "Adding nvidia-cuda-runtime-cu12 12.6.77 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cuda-nvrtc-cu12==12.6.77\n",
            "Best match: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "Adding nvidia-cuda-nvrtc-cu12 12.6.77 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for fsspec==2025.3.0\n",
            "Best match: fsspec 2025.3.0\n",
            "Adding fsspec 2025.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for jinja2==3.1.6\n",
            "Best match: jinja2 3.1.6\n",
            "Adding jinja2 3.1.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for networkx==3.5\n",
            "Best match: networkx 3.5\n",
            "Adding networkx 3.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for sympy==1.13.3\n",
            "Best match: sympy 1.13.3\n",
            "Adding sympy 1.13.3 to easy-install.pth file\n",
            "Installing isympy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for setuptools==75.2.0\n",
            "Best match: setuptools 75.2.0\n",
            "Adding setuptools 75.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for typing-extensions==4.15.0\n",
            "Best match: typing-extensions 4.15.0\n",
            "Adding typing-extensions 4.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for filelock==3.19.1\n",
            "Best match: filelock 3.19.1\n",
            "Adding filelock 3.19.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for platformdirs==4.4.0\n",
            "Best match: platformdirs 4.4.0\n",
            "Adding platformdirs 4.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for werkzeug==3.1.3\n",
            "Best match: werkzeug 3.1.3\n",
            "Adding werkzeug 3.1.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for tensorboard-data-server==0.7.2\n",
            "Best match: tensorboard-data-server 0.7.2\n",
            "Adding tensorboard-data-server 0.7.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for protobuf==5.29.5\n",
            "Best match: protobuf 5.29.5\n",
            "Adding protobuf 5.29.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for packaging==25.0\n",
            "Best match: packaging 25.0\n",
            "Adding packaging 25.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for markdown==3.9\n",
            "Best match: markdown 3.9\n",
            "Adding markdown 3.9 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for grpcio==1.75.0\n",
            "Best match: grpcio 1.75.0\n",
            "Adding grpcio 1.75.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for absl-py==1.4.0\n",
            "Best match: absl-py 1.4.0\n",
            "Adding absl-py 1.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for lazy-loader==0.4\n",
            "Best match: lazy-loader 0.4\n",
            "Adding lazy-loader 0.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for tifffile==2025.9.9\n",
            "Best match: tifffile 2025.9.9\n",
            "Adding tifffile 2025.9.9 to easy-install.pth file\n",
            "Installing lsm2bin script to /usr/local/bin\n",
            "Installing tiff2fsspec script to /usr/local/bin\n",
            "Installing tiffcomment script to /usr/local/bin\n",
            "Installing tifffile script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for imageio==2.37.0\n",
            "Best match: imageio 2.37.0\n",
            "Adding imageio 2.37.0 to easy-install.pth file\n",
            "Installing imageio_download_bin script to /usr/local/bin\n",
            "Installing imageio_remove_bin script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for certifi==2025.8.3\n",
            "Best match: certifi 2025.8.3\n",
            "Adding certifi 2025.8.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for urllib3==2.5.0\n",
            "Best match: urllib3 2.5.0\n",
            "Adding urllib3 2.5.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for idna==3.10\n",
            "Best match: idna 3.10\n",
            "Adding idna 3.10 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for charset-normalizer==3.4.3\n",
            "Best match: charset-normalizer 3.4.3\n",
            "Adding charset-normalizer 3.4.3 to easy-install.pth file\n",
            "Installing normalizer script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for MarkupSafe==3.0.2\n",
            "Best match: MarkupSafe 3.0.2\n",
            "Adding MarkupSafe 3.0.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for mpmath==1.3.0\n",
            "Best match: mpmath 1.3.0\n",
            "Adding mpmath 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Finished processing dependencies for hat==0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 Get dataset from drive"
      ],
      "metadata": {
        "id": "pbeiQJEyP5NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_zip_path = '/content/drive/MyDrive/Final_project/2000dataset.zip'\n",
        "!unzip -q $dataset_zip_path -d $unzip_path"
      ],
      "metadata": {
        "id": "ND5jt3APZhc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rm \"/content/dataset/train/lr/desktop.ini\""
      ],
      "metadata": {
        "id": "wer2wOAZ8j2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4 Get pre-trained model from drive"
      ],
      "metadata": {
        "id": "0D00Gc9oP8fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/Final_project/HAT/Real_HAT_GAN_SRx4.pth\" \\\n",
        "   \"/content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth\""
      ],
      "metadata": {
        "id": "L4F7RhHqZ34e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth\"\n",
        "\n",
        "# Check file exists and size\n",
        "if os.path.exists(model_path):\n",
        "    size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
        "    print(f\"Model found! Size: {size_mb:.1f} MB\")\n",
        "    # Expected size: 81.2 MB for medium and 158 MB for Large\n",
        "else:\n",
        "    print(\"Error: File not copied correctly!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_vLm3eGZ-eN",
        "outputId": "c038097c-5314-4f02-9c22-d5b38da5cb84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model found! Size: 162.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5 Fine-tune pre-trained model on satellite images"
      ],
      "metadata": {
        "id": "-uWT_a5b8R12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Create configuration file for training"
      ],
      "metadata": {
        "id": "4VnDx2P3XIXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the YAML content\n",
        "yaml_content = \"\"\"# general settings\n",
        "name: train_Real_HAT_GAN_SRx4_finetune_from_mse_model\n",
        "model_type: RealHATGANModel\n",
        "scale: 4\n",
        "num_gpu: auto\n",
        "manual_seed: 100\n",
        "\n",
        "# USM the ground-truth\n",
        "l1_gt_usm: True\n",
        "percep_gt_usm: True\n",
        "gan_gt_usm: False\n",
        "high_order_degradation: False\n",
        "\n",
        "# dataset and data loader settings\n",
        "datasets:\n",
        "  train:\n",
        "    name: sat_images\n",
        "    type: PairedImageDataset\n",
        "    dataroot_gt: /content/dataset/train/hr\n",
        "    dataroot_lq: /content/dataset/train/lr\n",
        "    meta_info: /content/dataset/meta_info/meta_info_satelliteimages_train.txt\n",
        "    io_backend:\n",
        "      type: disk\n",
        "\n",
        "    gt_size: 256\n",
        "    use_hflip: true\n",
        "    use_rot: true\n",
        "\n",
        "    # data loader\n",
        "    use_shuffle: true\n",
        "    num_worker_per_gpu: 1\n",
        "    batch_size_per_gpu: 2\n",
        "    dataset_enlarge_ratio: 2\n",
        "    prefetch_mode: ~\n",
        "\n",
        "  val_1:\n",
        "    name: val_sat_images\n",
        "    type: PairedImageDataset\n",
        "    dataroot_gt: /content/dataset/val/hr\n",
        "    dataroot_lq: /content/dataset/val/lr\n",
        "    meta_info: /content/dataset/meta_info/meta_info_satelliteimages_val.txt\n",
        "    io_backend:\n",
        "      type: disk\n",
        "\n",
        "# network structures\n",
        "network_g:\n",
        "  type: HAT\n",
        "  upscale: 4\n",
        "  in_chans: 3\n",
        "  img_size: 64\n",
        "  window_size: 16\n",
        "  compress_ratio: 3\n",
        "  squeeze_factor: 30\n",
        "  conv_scale: 0.01\n",
        "  overlap_ratio: 0.5\n",
        "  img_range: 1.\n",
        "  depths: [6, 6, 6, 6, 6, 6]\n",
        "  embed_dim: 180\n",
        "  num_heads: [6, 6, 6, 6, 6, 6]\n",
        "  mlp_ratio: 2\n",
        "  upsampler: 'pixelshuffle'\n",
        "  resi_connection: '1conv'\n",
        "\n",
        "network_d:\n",
        "  type: UNetDiscriminatorSN\n",
        "  num_in_ch: 3\n",
        "  num_feat: 64\n",
        "  skip_connection: True\n",
        "\n",
        "# path\n",
        "path:\n",
        "  pretrain_network_g: /content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth\n",
        "  param_key_g: 'params_ema'\n",
        "  strict_load_g: true\n",
        "  resume_state: ~\n",
        "\n",
        "# training settings\n",
        "train:\n",
        "  ema_decay: 0.999\n",
        "  optim_g:\n",
        "    type: Adam\n",
        "    lr: !!float 2e-5\n",
        "    weight_decay: 0\n",
        "    betas: [0.9, 0.99]\n",
        "  optim_d:\n",
        "    type: Adam\n",
        "    lr: !!float 2e-5\n",
        "    weight_decay: 0\n",
        "    betas: [0.9, 0.99]\n",
        "\n",
        "  scheduler:\n",
        "    type: MultiStepLR\n",
        "    milestones: [2000, 3000, 4000]\n",
        "    gamma: 0.5\n",
        "\n",
        "  total_iter: 5000\n",
        "  warmup_iter: -1  # no warm up\n",
        "\n",
        "  # losses\n",
        "  pixel_opt:\n",
        "    type: L1Loss\n",
        "    loss_weight: 1.0\n",
        "    reduction: mean\n",
        "  perceptual_opt:\n",
        "    type: PerceptualLoss\n",
        "    layer_weights:\n",
        "      # before relu\n",
        "      'conv1_2': 0.1\n",
        "      'conv2_2': 0.1\n",
        "      'conv3_4': 1\n",
        "      'conv4_4': 1\n",
        "      'conv5_4': 1\n",
        "    vgg_type: vgg19\n",
        "    use_input_norm: true\n",
        "    perceptual_weight: !!float 1.0\n",
        "    style_weight: 0\n",
        "    range_norm: false\n",
        "    criterion: l1\n",
        "  # gan loss\n",
        "  gan_opt:\n",
        "    type: GANLoss\n",
        "    gan_type: vanilla\n",
        "    real_label_val: 1.0\n",
        "    fake_label_val: 0.0\n",
        "    loss_weight: !!float 1e-1\n",
        "\n",
        "  net_d_iters: 1\n",
        "  net_d_init_iters: 0\n",
        "\n",
        "\n",
        "# validation settings\n",
        "val:\n",
        "  val_freq: 100\n",
        "  save_img: true\n",
        "  pbar: False\n",
        "\n",
        "  metrics:\n",
        "    psnr:\n",
        "      type: calculate_psnr\n",
        "      crop_border: 4\n",
        "      test_y_channel: true\n",
        "      better: higher\n",
        "    ssim:\n",
        "      type: calculate_ssim\n",
        "      crop_border: 4\n",
        "      test_y_channel: true\n",
        "      better: higher\n",
        "    niqe:\n",
        "      type: calculate_niqe\n",
        "      crop_border: 4\n",
        "      input_order: 'HWC'\n",
        "      convert_to: 'y'\n",
        "\n",
        "# logging settings\n",
        "logger:\n",
        "  print_freq: 100\n",
        "  save_checkpoint_freq: 500\n",
        "  use_tb_logger: true\n",
        "  wandb:\n",
        "    project: HAT-satellite-SR\n",
        "    resume_id: ~\n",
        "\n",
        "# dist training settings\n",
        "dist_params:\n",
        "  backend: nccl\n",
        "  port: 29500\n",
        "\"\"\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "!mkdir -p /content/HAT/options/train/\n",
        "\n",
        "# Write to the YAML file\n",
        "with open('/content/HAT/options/train/train_HAT-GAN_SRx4_finetune.yml', 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "# Verify the file was created\n",
        "!ls -l /content/HAT/options/train/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ918WjeXO8I",
        "outputId": "9ab3761e-9ea9-4d5f-cbf8-3613482e00a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 3189 Sep 18 15:20 train_HAT-GAN_SRx4_finetune.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Fine tune model"
      ],
      "metadata": {
        "id": "gKNnhIQb8cc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 --master_port=4321 hat/train.py -opt /content/HAT/options/train/train_HAT-GAN_SRx4_finetune.yml --launcher none"
      ],
      "metadata": {
        "id": "Zr4TDlYoavJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f7cade4-e339-4f5e-9a59-fd7795527da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disable distributed.\n",
            "2025-09-18 15:21:04,749 INFO: \n",
            "                ____                _       _____  ____\n",
            "               / __ ) ____ _ _____ (_)_____/ ___/ / __ \\\n",
            "              / __  |/ __ `// ___// // ___/\\__ \\ / /_/ /\n",
            "             / /_/ // /_/ /(__  )/ // /__ ___/ // _, _/\n",
            "            /_____/ \\__,_//____//_/ \\___//____//_/ |_|\n",
            "     ______                   __   __                 __      __\n",
            "    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /\n",
            "   / / __ / __ \\ / __ \\ / __  /  / /   / / / // ___// //_/  / /\n",
            "  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/\n",
            "  \\____/ \\____/ \\____/ \\____/  /_____/\\____/ \\___//_/|_|  (_)\n",
            "    \n",
            "Version Information: \n",
            "\tBasicSR: 1.3.4.9\n",
            "\tPyTorch: 2.8.0+cu126\n",
            "\tTorchVision: 0.23.0+cu126\n",
            "2025-09-18 15:21:04,749 INFO: \n",
            "  name: train_Real_HAT_GAN_SRx4_finetune_from_mse_model\n",
            "  model_type: RealHATGANModel\n",
            "  scale: 4\n",
            "  num_gpu: 1\n",
            "  manual_seed: 100\n",
            "  l1_gt_usm: True\n",
            "  percep_gt_usm: True\n",
            "  gan_gt_usm: False\n",
            "  high_order_degradation: False\n",
            "  datasets:[\n",
            "    train:[\n",
            "      name: sat_images\n",
            "      type: PairedImageDataset\n",
            "      dataroot_gt: /content/dataset/train/hr\n",
            "      dataroot_lq: /content/dataset/train/lr\n",
            "      meta_info: /content/dataset/meta_info/meta_info_satelliteimages_train.txt\n",
            "      io_backend:[\n",
            "        type: disk\n",
            "      ]\n",
            "      gt_size: 256\n",
            "      use_hflip: True\n",
            "      use_rot: True\n",
            "      use_shuffle: True\n",
            "      num_worker_per_gpu: 1\n",
            "      batch_size_per_gpu: 2\n",
            "      dataset_enlarge_ratio: 2\n",
            "      prefetch_mode: None\n",
            "      phase: train\n",
            "      scale: 4\n",
            "    ]\n",
            "    val_1:[\n",
            "      name: val_sat_images\n",
            "      type: PairedImageDataset\n",
            "      dataroot_gt: /content/dataset/val/hr\n",
            "      dataroot_lq: /content/dataset/val/lr\n",
            "      meta_info: /content/dataset/meta_info/meta_info_satelliteimages_val.txt\n",
            "      io_backend:[\n",
            "        type: disk\n",
            "      ]\n",
            "      phase: val\n",
            "      scale: 4\n",
            "    ]\n",
            "  ]\n",
            "  network_g:[\n",
            "    type: HAT\n",
            "    upscale: 4\n",
            "    in_chans: 3\n",
            "    img_size: 64\n",
            "    window_size: 16\n",
            "    compress_ratio: 3\n",
            "    squeeze_factor: 30\n",
            "    conv_scale: 0.01\n",
            "    overlap_ratio: 0.5\n",
            "    img_range: 1.0\n",
            "    depths: [6, 6, 6, 6, 6, 6]\n",
            "    embed_dim: 180\n",
            "    num_heads: [6, 6, 6, 6, 6, 6]\n",
            "    mlp_ratio: 2\n",
            "    upsampler: pixelshuffle\n",
            "    resi_connection: 1conv\n",
            "  ]\n",
            "  network_d:[\n",
            "    type: UNetDiscriminatorSN\n",
            "    num_in_ch: 3\n",
            "    num_feat: 64\n",
            "    skip_connection: True\n",
            "  ]\n",
            "  path:[\n",
            "    pretrain_network_g: /content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth\n",
            "    param_key_g: params_ema\n",
            "    strict_load_g: True\n",
            "    resume_state: None\n",
            "    experiments_root: /content/HAT/experiments/train_Real_HAT_GAN_SRx4_finetune_from_mse_model\n",
            "    models: /content/HAT/experiments/train_Real_HAT_GAN_SRx4_finetune_from_mse_model/models\n",
            "    training_states: /content/HAT/experiments/train_Real_HAT_GAN_SRx4_finetune_from_mse_model/training_states\n",
            "    log: /content/HAT/experiments/train_Real_HAT_GAN_SRx4_finetune_from_mse_model\n",
            "    visualization: /content/HAT/experiments/train_Real_HAT_GAN_SRx4_finetune_from_mse_model/visualization\n",
            "  ]\n",
            "  train:[\n",
            "    ema_decay: 0.999\n",
            "    optim_g:[\n",
            "      type: Adam\n",
            "      lr: 2e-05\n",
            "      weight_decay: 0\n",
            "      betas: [0.9, 0.99]\n",
            "    ]\n",
            "    optim_d:[\n",
            "      type: Adam\n",
            "      lr: 2e-05\n",
            "      weight_decay: 0\n",
            "      betas: [0.9, 0.99]\n",
            "    ]\n",
            "    scheduler:[\n",
            "      type: MultiStepLR\n",
            "      milestones: [1200, 2500, 3800]\n",
            "      gamma: 0.5\n",
            "    ]\n",
            "    total_iter: 5000\n",
            "    warmup_iter: -1\n",
            "    pixel_opt:[\n",
            "      type: L1Loss\n",
            "      loss_weight: 1.0\n",
            "      reduction: mean\n",
            "    ]\n",
            "    perceptual_opt:[\n",
            "      type: PerceptualLoss\n",
            "      layer_weights:[\n",
            "        conv1_2: 0.1\n",
            "        conv2_2: 0.1\n",
            "        conv3_4: 1\n",
            "        conv4_4: 1\n",
            "        conv5_4: 1\n",
            "      ]\n",
            "      vgg_type: vgg19\n",
            "      use_input_norm: True\n",
            "      perceptual_weight: 1.0\n",
            "      style_weight: 0\n",
            "      range_norm: False\n",
            "      criterion: l1\n",
            "    ]\n",
            "    gan_opt:[\n",
            "      type: GANLoss\n",
            "      gan_type: vanilla\n",
            "      real_label_val: 1.0\n",
            "      fake_label_val: 0.0\n",
            "      loss_weight: 0.1\n",
            "    ]\n",
            "    net_d_iters: 1\n",
            "    net_d_init_iters: 0\n",
            "  ]\n",
            "  val:[\n",
            "    val_freq: 5\n",
            "    save_img: True\n",
            "    pbar: False\n",
            "    metrics:[\n",
            "      psnr:[\n",
            "        type: calculate_psnr\n",
            "        crop_border: 4\n",
            "        test_y_channel: True\n",
            "        better: higher\n",
            "      ]\n",
            "      ssim:[\n",
            "        type: calculate_ssim\n",
            "        crop_border: 4\n",
            "        test_y_channel: True\n",
            "        better: higher\n",
            "      ]\n",
            "      niqe:[\n",
            "        type: calculate_niqe\n",
            "        crop_border: 4\n",
            "        input_order: HWC\n",
            "        convert_to: y\n",
            "      ]\n",
            "    ]\n",
            "  ]\n",
            "  logger:[\n",
            "    print_freq: 5\n",
            "    save_checkpoint_freq: 5\n",
            "    use_tb_logger: True\n",
            "    wandb:[\n",
            "      project: HAT-satellite-SR\n",
            "      resume_id: None\n",
            "    ]\n",
            "  ]\n",
            "  dist_params:[\n",
            "    backend: nccl\n",
            "    port: 29500\n",
            "  ]\n",
            "  dist: False\n",
            "  rank: 0\n",
            "  world_size: 1\n",
            "  auto_resume: False\n",
            "  is_train: True\n",
            "  root_path: /content/HAT\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdsha43925\u001b[0m (\u001b[33mdsha43925-middlesex-university-mauritius\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "2025-09-18 15:21:06.386985: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758208866.407051    7459 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758208866.413010    7459 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758208866.429259    7459 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758208866.429292    7459 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758208866.429301    7459 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758208866.429307    7459 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-18 15:21:06.436170: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/HAT/wandb/run-20250918_152106-lrbfbxt4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrain_Real_HAT_GAN_SRx4_finetune_from_mse_model\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsha43925-middlesex-university-mauritius/HAT-satellite-SR\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsha43925-middlesex-university-mauritius/HAT-satellite-SR/runs/lrbfbxt4\u001b[0m\n",
            "2025-09-18 15:21:11,449 INFO: Use wandb logger with id=lrbfbxt4; project=HAT-satellite-SR.\n",
            "2025-09-18 15:21:11,535 INFO: Dataset [PairedImageDataset] - sat_images is built.\n",
            "2025-09-18 15:21:11,536 INFO: Training statistics:\n",
            "\tNumber of train images: 2000\n",
            "\tDataset enlarge ratio: 2\n",
            "\tBatch size per gpu: 2\n",
            "\tWorld size (gpu number): 1\n",
            "\tRequire iter number per epoch: 2000\n",
            "\tTotal epochs: 3; iters: 5000.\n",
            "2025-09-18 15:21:11,550 INFO: Dataset [PairedImageDataset] - val_sat_images is built.\n",
            "2025-09-18 15:21:11,551 INFO: Number of val images/folders in val_sat_images: 500\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "2025-09-18 15:21:12,476 INFO: Network [HAT] is created.\n",
            "2025-09-18 15:21:12,877 INFO: Network: HAT, with parameters: 20,772,507\n",
            "2025-09-18 15:21:12,878 INFO: HAT(\n",
            "  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (patch_unembed): PatchUnEmbed()\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (layers): ModuleList(\n",
            "    (0): RHAG(\n",
            "      (residual_group): AttenBlocks(\n",
            "        (blocks): ModuleList(\n",
            "          (0): HAB(\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(180, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(60, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(180, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 180, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): Identity()\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1-5): 5 x HAB(\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(180, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(60, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(180, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 180, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (overlap_attn): OCAB(\n",
            "          (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "          (unfold): Unfold(kernel_size=(24, 24), dilation=1, padding=4, stride=16)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "          (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "    (1-5): 5 x RHAG(\n",
            "      (residual_group): AttenBlocks(\n",
            "        (blocks): ModuleList(\n",
            "          (0-5): 6 x HAB(\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(180, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(60, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(180, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 180, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (overlap_attn): OCAB(\n",
            "          (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "          (unfold): Unfold(kernel_size=(24, 24), dilation=1, padding=4, stride=16)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "          (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_before_upsample): Sequential(\n",
            "    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "  )\n",
            "  (upsample): Upsample(\n",
            "    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): PixelShuffle(upscale_factor=2)\n",
            "    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): PixelShuffle(upscale_factor=2)\n",
            "  )\n",
            "  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n",
            "2025-09-18 15:21:13,294 INFO: Loading HAT model from /content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth, with param key: [params_ema].\n",
            "2025-09-18 15:21:13,436 INFO: Use Exponential Moving Average with decay: 0.999\n",
            "2025-09-18 15:21:13,973 INFO: Network [HAT] is created.\n",
            "2025-09-18 15:21:14,432 INFO: Loading HAT model from /content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth, with param key: [params_ema].\n",
            "2025-09-18 15:21:14,655 INFO: Network [UNetDiscriminatorSN] is created.\n",
            "2025-09-18 15:21:14,663 INFO: Network: UNetDiscriminatorSN, with parameters: 4,376,897\n",
            "2025-09-18 15:21:14,663 INFO: UNetDiscriminatorSN(\n",
            "  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n",
            "2025-09-18 15:21:14,672 INFO: Loss [L1Loss] is created.\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:03<00:00, 178MB/s]\n",
            "2025-09-18 15:21:19,569 INFO: Loss [PerceptualLoss] is created.\n",
            "2025-09-18 15:21:19,594 INFO: Loss [GANLoss] is created.\n",
            "2025-09-18 15:21:19,643 INFO: Model [RealHATGANModel] is created.\n",
            "2025-09-18 15:21:19,663 INFO: Start training from epoch: 0, iter: 0\n",
            "2025-09-18 15:21:33,931 INFO: [train..][epoch:  0, iter:       5, lr:(2.000e-05,)] [eta: 0:57:05, time (data): 2.853 (0.019)] l_g_pix: 8.2151e-02 l_g_percep: 1.2843e+01 l_g_gan: 6.6416e-02 l_d_real: 6.6412e-01 out_d_real: 5.9094e-02 l_d_fake: 7.2227e-01 out_d_fake: 5.7262e-02 \n",
            "2025-09-18 15:21:33,932 INFO: Saving models and training states.\n",
            "2025-09-18 15:23:50,085 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.1970\tBest: 19.1970 @ 5 iter\n",
            "\t # ssim: 0.4656\tBest: 0.4656 @ 5 iter\n",
            "\t # niqe: 6.5398\tBest: 6.5398 @ 5 iter\n",
            "\n",
            "2025-09-18 15:23:55,865 INFO: [train..][epoch:  0, iter:      10, lr:(2.000e-05,)] [eta: 18:24:00, time (data): 2.004 (0.011)] l_g_pix: 8.6930e-02 l_g_percep: 1.1722e+01 l_g_gan: 6.7346e-02 l_d_real: 6.7225e-01 out_d_real: 4.2315e-02 l_d_fake: 7.1314e-01 out_d_fake: 3.9548e-02 \n",
            "2025-09-18 15:23:55,866 INFO: Saving models and training states.\n",
            "2025-09-18 15:26:13,489 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.1970\tBest: 19.1970 @ 10 iter\n",
            "\t # ssim: 0.4656\tBest: 0.4656 @ 10 iter\n",
            "\t # niqe: 6.5394\tBest: 6.5398 @ 5 iter\n",
            "\n",
            "2025-09-18 15:26:19,221 INFO: [train..][epoch:  0, iter:      15, lr:(2.000e-05,)] [eta: 1 day, 1:02:29, time (data): 1.718 (0.009)] l_g_pix: 1.4267e-01 l_g_percep: 1.2815e+01 l_g_gan: 6.8327e-02 l_d_real: 6.7794e-01 out_d_real: 3.0715e-02 l_d_fake: 7.0312e-01 out_d_fake: 1.9822e-02 \n",
            "2025-09-18 15:26:19,223 INFO: Saving models and training states.\n",
            "W0918 15:26:50.030000 7446 torch/distributed/elastic/agent/server/api.py:723] Received 2 death signal, shutting down workers\n",
            "W0918 15:26:50.032000 7446 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 7459 closing signal SIGINT\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 715, in run\n",
            "    result = self._invoke_run(role)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 879, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 7446 got signal: 2\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1163, in emit\n",
            "    stream.write(msg + self.terminator)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 7446 got signal: 2\n",
            "Call stack:\n",
            "  File \"/usr/local/bin/torchrun\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('torch', 'console_scripts', 'torchrun')())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 901, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py\", line 138, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 724, in run\n",
            "    self._shutdown(e.sigval)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py\", line 368, in _shutdown\n",
            "    self._pcontext.close(death_sig)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 577, in close\n",
            "    self._close(death_sig=death_sig, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 900, in _close\n",
            "    logger.warning(\n",
            "Message: 'Sending process %s closing signal %s'\n",
            "Arguments: (7459, 'SIGINT')\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/HAT/hat/train.py\", line 11, in <module>\n",
            "    train_pipeline(root_path)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/train.py\", line 193, in train_pipeline\n",
            "    model.validation(val_loader, current_iter, tb_logger, opt['val']['save_img'])\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/models/base_model.py\", line 48, in validation\n",
            "    self.nondist_validation(dataloader, current_iter, tb_logger, save_img)\n",
            "  File \"/content/HAT/hat/models/testrealhatgan_model.py\", line 188, in nondist_validation\n",
            "    super(RealHATGANModel, self).nondist_validation(dataloader, current_iter, tb_logger, save_img)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/models/sr_model.py\", line 188, in nondist_validation\n",
            "    self.metric_results[name] += calculate_metric(metric_data, opt_)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/metrics/__init__.py\", line 19, in calculate_metric\n",
            "    metric = METRIC_REGISTRY.get(metric_type)(**data, **opt)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/metrics/niqe.py\", line 195, in calculate_niqe\n",
            "    niqe_result = niqe(img, mu_pris_param, cov_pris_param, gaussian_window)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/metrics/niqe.py\", line 117, in niqe\n",
            "    feat.append(compute_feature(block))\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/metrics/niqe.py\", line 61, in compute_feature\n",
            "    alpha, beta_l, beta_r = estimate_aggd_param(block * shifted_block)\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/metrics/niqe.py\", line 26, in estimate_aggd_param\n",
            "    r_gam = np.square(gamma(gam_reciprocal * 2)) / (gamma(gam_reciprocal) * gamma(gam_reciprocal * 3))\n",
            "                                                    ^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "W0918 15:27:15.769000 7446 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 7459 closing signal SIGTERM\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 715, in run\n",
            "    result = self._invoke_run(role)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 879, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 7446 got signal: 2\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 724, in run\n",
            "    self._shutdown(e.sigval)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py\", line 368, in _shutdown\n",
            "    self._pcontext.close(death_sig)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 577, in close\n",
            "    self._close(death_sig=death_sig, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 912, in _close\n",
            "    handler.proc.wait(time_to_wait)\n",
            "  File \"/usr/lib/python3.12/subprocess.py\", line 1264, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/subprocess.py\", line 2047, in _wait\n",
            "    time.sleep(delay)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 7446 got signal: 2\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/torchrun\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('torch', 'console_scripts', 'torchrun')())\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 901, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
            "    result = agent.run()\n",
            "             ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py\", line 138, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 729, in run\n",
            "    self._shutdown()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py\", line 368, in _shutdown\n",
            "    self._pcontext.close(death_sig)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 577, in close\n",
            "    self._close(death_sig=death_sig, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 912, in _close\n",
            "    handler.proc.wait(time_to_wait)\n",
            "  File \"/usr/lib/python3.12/subprocess.py\", line 1264, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/subprocess.py\", line 2047, in _wait\n",
            "    time.sleep(delay)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 7446 got signal: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6 Inference"
      ],
      "metadata": {
        "id": "sj-EIfVc4PXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Get zipfile containing low resolution image to test"
      ],
      "metadata": {
        "id": "epclaj6T4U6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload the zip file containing low resolution images for inference.\n",
        "#code_zip_path = '/content/drive/MyDrive/Final_project/testImages.zip'\n",
        "code_zip_path = '/content/drive/MyDrive/Final_project/LRInferenceImages.zip'\n",
        "unzip_path = '/content'\n",
        "!unzip -q $code_zip_path -d $unzip_path\n",
        "!ls -l $unzip_path"
      ],
      "metadata": {
        "id": "De7C5za-dHuj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d43b393-edb4-4a70-8f11-3df9cab3edf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwx------ 5 root root 4096 Sep 23 19:40 drive\n",
            "drwxrwxrwx 8 root root 4096 Sep 23 19:41 HAT\n",
            "drwxrwxrwx 2 root root 4096 Sep 23 18:42 LRInferenceImages\n",
            "drwxr-xr-x 1 root root 4096 Sep 19 13:40 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Create configuration file for inference"
      ],
      "metadata": {
        "id": "WPLsDv8E4crK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "replace with correct path in config:\n",
        "pretrain_network_g: '/content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth'"
      ],
      "metadata": {
        "id": "aa_dFF8bFtxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the YAML content as a string\n",
        "test_config_yml = \"\"\"\n",
        "name: HAT-L_SRx4_ImageNet-pretrain\n",
        "model_type: HATModel\n",
        "scale: 4\n",
        "num_gpu: 1\n",
        "manual_seed: 100\n",
        "\n",
        "tile:\n",
        "  tile_size: 64\n",
        "  tile_pad: 32\n",
        "\n",
        "datasets:\n",
        "  test_1:\n",
        "    name: customSatelliteIMages\n",
        "    type: SingleImageDataset\n",
        "    dataroot_lq: /content/LRInferenceImages\n",
        "    io_backend:\n",
        "      type: disk\n",
        "\n",
        "# network structures\n",
        "network_g:\n",
        "  type: HAT\n",
        "  upscale: 4\n",
        "  in_chans: 3\n",
        "  img_size: 64\n",
        "  window_size: 16\n",
        "  compress_ratio: 3\n",
        "  squeeze_factor: 30\n",
        "  conv_scale: 0.01\n",
        "  overlap_ratio: 0.5\n",
        "  img_range: 1.\n",
        "  depths: [6, 6, 6, 6, 6, 6]\n",
        "  embed_dim: 180\n",
        "  num_heads: [6, 6, 6, 6, 6, 6]\n",
        "  mlp_ratio: 2\n",
        "  upsampler: 'pixelshuffle'\n",
        "  resi_connection: '1conv'\n",
        "\n",
        "# path\n",
        "path:\n",
        "  pretrain_network_g: '/content/drive/MyDrive/Final_project/HAT/net_g_2000.pth'\n",
        "  strict_load_g: true\n",
        "  param_key_g: 'params_ema'\n",
        "\n",
        "# validation settings\n",
        "val:\n",
        "  save_img: true\n",
        "  suffix: ~  # add suffix to saved images, if None, use exp name\n",
        "\"\"\"\n",
        "\n",
        "output_path = \"/content/HAT/options/test/HAT-L_inference.yml\"\n",
        "\n",
        "# Create directories not exist\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "# Save to yml\n",
        "with open(output_path, \"w\") as f:\n",
        "    f.write(test_config_yml)\n",
        "\n",
        "print(f\"YAML config saved to {output_path}\")\n"
      ],
      "metadata": {
        "id": "0IKAW87Ebjl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e3cb78e-5653-4422-837e-c2d95b94bcf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YAML config saved to /content/HAT/options/test/HAT-L_inference.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Run for inference"
      ],
      "metadata": {
        "id": "gjRpZkWu8JUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python hat/test.py -opt options/test/HAT-L_inference.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOtgnQaCaw0H",
        "outputId": "d1c60aa6-bd64-4c2e-ea75-bfa272f1a989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disable distributed.\n",
            "2025-09-23 19:43:29,869 INFO: \n",
            "                ____                _       _____  ____\n",
            "               / __ ) ____ _ _____ (_)_____/ ___/ / __ \\\n",
            "              / __  |/ __ `// ___// // ___/\\__ \\ / /_/ /\n",
            "             / /_/ // /_/ /(__  )/ // /__ ___/ // _, _/\n",
            "            /_____/ \\__,_//____//_/ \\___//____//_/ |_|\n",
            "     ______                   __   __                 __      __\n",
            "    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /\n",
            "   / / __ / __ \\ / __ \\ / __  /  / /   / / / // ___// //_/  / /\n",
            "  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/\n",
            "  \\____/ \\____/ \\____/ \\____/  /_____/\\____/ \\___//_/|_|  (_)\n",
            "    \n",
            "Version Information: \n",
            "\tBasicSR: 1.3.4.9\n",
            "\tPyTorch: 2.8.0+cu126\n",
            "\tTorchVision: 0.23.0+cu126\n",
            "2025-09-23 19:43:29,870 INFO: \n",
            "  name: HAT-L_SRx4_ImageNet-pretrain\n",
            "  model_type: HATModel\n",
            "  scale: 4\n",
            "  num_gpu: 1\n",
            "  manual_seed: 100\n",
            "  tile:[\n",
            "    tile_size: 64\n",
            "    tile_pad: 32\n",
            "  ]\n",
            "  datasets:[\n",
            "    test_1:[\n",
            "      name: customSatelliteIMages\n",
            "      type: SingleImageDataset\n",
            "      dataroot_lq: /content/LRInferenceImages\n",
            "      io_backend:[\n",
            "        type: disk\n",
            "      ]\n",
            "      phase: test\n",
            "      scale: 4\n",
            "    ]\n",
            "  ]\n",
            "  network_g:[\n",
            "    type: HAT\n",
            "    upscale: 4\n",
            "    in_chans: 3\n",
            "    img_size: 64\n",
            "    window_size: 16\n",
            "    compress_ratio: 3\n",
            "    squeeze_factor: 30\n",
            "    conv_scale: 0.01\n",
            "    overlap_ratio: 0.5\n",
            "    img_range: 1.0\n",
            "    depths: [6, 6, 6, 6, 6, 6]\n",
            "    embed_dim: 180\n",
            "    num_heads: [6, 6, 6, 6, 6, 6]\n",
            "    mlp_ratio: 2\n",
            "    upsampler: pixelshuffle\n",
            "    resi_connection: 1conv\n",
            "  ]\n",
            "  path:[\n",
            "    pretrain_network_g: /content/drive/MyDrive/Final_project/HAT/net_g_2000.pth\n",
            "    strict_load_g: True\n",
            "    param_key_g: params_ema\n",
            "    results_root: /content/HAT/results/HAT-L_SRx4_ImageNet-pretrain\n",
            "    log: /content/HAT/results/HAT-L_SRx4_ImageNet-pretrain\n",
            "    visualization: /content/HAT/results/HAT-L_SRx4_ImageNet-pretrain/visualization\n",
            "  ]\n",
            "  val:[\n",
            "    save_img: True\n",
            "    suffix: None\n",
            "  ]\n",
            "  dist: False\n",
            "  rank: 0\n",
            "  world_size: 1\n",
            "  auto_resume: False\n",
            "  is_train: False\n",
            "\n",
            "2025-09-23 19:43:29,870 INFO: Dataset [SingleImageDataset] - customSatelliteIMages is built.\n",
            "2025-09-23 19:43:29,870 INFO: Number of test images in customSatelliteIMages: 9\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "2025-09-23 19:43:30,516 INFO: Network [HAT] is created.\n",
            "2025-09-23 19:43:30,901 INFO: Network: HAT, with parameters: 20,772,507\n",
            "2025-09-23 19:43:30,901 INFO: HAT(\n",
            "  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (patch_unembed): PatchUnEmbed()\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (layers): ModuleList(\n",
            "    (0): RHAG(\n",
            "      (residual_group): AttenBlocks(\n",
            "        (blocks): ModuleList(\n",
            "          (0): HAB(\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(180, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(60, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(180, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 180, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): Identity()\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1-5): 5 x HAB(\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(180, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(60, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(180, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 180, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (overlap_attn): OCAB(\n",
            "          (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "          (unfold): Unfold(kernel_size=(24, 24), dilation=1, padding=4, stride=16)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "          (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "    (1-5): 5 x RHAG(\n",
            "      (residual_group): AttenBlocks(\n",
            "        (blocks): ModuleList(\n",
            "          (0-5): 6 x HAB(\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(180, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(60, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(180, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 180, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (overlap_attn): OCAB(\n",
            "          (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "          (unfold): Unfold(kernel_size=(24, 24), dilation=1, padding=4, stride=16)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "          (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_before_upsample): Sequential(\n",
            "    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "  )\n",
            "  (upsample): Upsample(\n",
            "    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): PixelShuffle(upscale_factor=2)\n",
            "    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): PixelShuffle(upscale_factor=2)\n",
            "  )\n",
            "  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n",
            "2025-09-23 19:43:39,872 INFO: Loading HAT model from /content/drive/MyDrive/Final_project/HAT/net_g_2000.pth, with param key: [params_ema].\n",
            "2025-09-23 19:43:39,985 INFO: Model [HATModel] is created.\n",
            "2025-09-23 19:43:39,985 INFO: Testing customSatelliteIMages...\n",
            "\tTile 1/18\n",
            "\tTile 2/18\n",
            "\tTile 3/18\n",
            "\tTile 4/18\n",
            "\tTile 5/18\n",
            "\tTile 6/18\n",
            "\tTile 7/18\n",
            "\tTile 8/18\n",
            "\tTile 9/18\n",
            "\tTile 10/18\n",
            "\tTile 11/18\n",
            "\tTile 12/18\n",
            "\tTile 13/18\n",
            "\tTile 14/18\n",
            "\tTile 15/18\n",
            "\tTile 16/18\n",
            "\tTile 17/18\n",
            "\tTile 18/18\n",
            "\tTile 1/64\n",
            "\tTile 2/64\n",
            "\tTile 3/64\n",
            "\tTile 4/64\n",
            "\tTile 5/64\n",
            "\tTile 6/64\n",
            "\tTile 7/64\n",
            "\tTile 8/64\n",
            "\tTile 9/64\n",
            "\tTile 10/64\n",
            "\tTile 11/64\n",
            "\tTile 12/64\n",
            "\tTile 13/64\n",
            "\tTile 14/64\n",
            "\tTile 15/64\n",
            "\tTile 16/64\n",
            "\tTile 17/64\n",
            "\tTile 18/64\n",
            "\tTile 19/64\n",
            "\tTile 20/64\n",
            "\tTile 21/64\n",
            "\tTile 22/64\n",
            "\tTile 23/64\n",
            "\tTile 24/64\n",
            "\tTile 25/64\n",
            "\tTile 26/64\n",
            "\tTile 27/64\n",
            "\tTile 28/64\n",
            "\tTile 29/64\n",
            "\tTile 30/64\n",
            "\tTile 31/64\n",
            "\tTile 32/64\n",
            "\tTile 33/64\n",
            "\tTile 34/64\n",
            "\tTile 35/64\n",
            "\tTile 36/64\n",
            "\tTile 37/64\n",
            "\tTile 38/64\n",
            "\tTile 39/64\n",
            "\tTile 40/64\n",
            "\tTile 41/64\n",
            "\tTile 42/64\n",
            "\tTile 43/64\n",
            "\tTile 44/64\n",
            "\tTile 45/64\n",
            "\tTile 46/64\n",
            "\tTile 47/64\n",
            "\tTile 48/64\n",
            "\tTile 49/64\n",
            "\tTile 50/64\n",
            "\tTile 51/64\n",
            "\tTile 52/64\n",
            "\tTile 53/64\n",
            "\tTile 54/64\n",
            "\tTile 55/64\n",
            "\tTile 56/64\n",
            "\tTile 57/64\n",
            "\tTile 58/64\n",
            "\tTile 59/64\n",
            "\tTile 60/64\n",
            "\tTile 61/64\n",
            "\tTile 62/64\n",
            "\tTile 63/64\n",
            "\tTile 64/64\n",
            "\tTile 1/36\n",
            "\tTile 2/36\n",
            "\tTile 3/36\n",
            "\tTile 4/36\n",
            "\tTile 5/36\n",
            "\tTile 6/36\n",
            "\tTile 7/36\n",
            "\tTile 8/36\n",
            "\tTile 9/36\n",
            "\tTile 10/36\n",
            "\tTile 11/36\n",
            "\tTile 12/36\n",
            "\tTile 13/36\n",
            "\tTile 14/36\n",
            "\tTile 15/36\n",
            "\tTile 16/36\n",
            "\tTile 17/36\n",
            "\tTile 18/36\n",
            "\tTile 19/36\n",
            "\tTile 20/36\n",
            "\tTile 21/36\n",
            "\tTile 22/36\n",
            "\tTile 23/36\n",
            "\tTile 24/36\n",
            "\tTile 25/36\n",
            "\tTile 26/36\n",
            "\tTile 27/36\n",
            "\tTile 28/36\n",
            "\tTile 29/36\n",
            "\tTile 30/36\n",
            "\tTile 31/36\n",
            "\tTile 32/36\n",
            "\tTile 33/36\n",
            "\tTile 34/36\n",
            "\tTile 35/36\n",
            "\tTile 36/36\n",
            "\tTile 1/49\n",
            "\tTile 2/49\n",
            "\tTile 3/49\n",
            "\tTile 4/49\n",
            "\tTile 5/49\n",
            "\tTile 6/49\n",
            "\tTile 7/49\n",
            "\tTile 8/49\n",
            "\tTile 9/49\n",
            "\tTile 10/49\n",
            "\tTile 11/49\n",
            "\tTile 12/49\n",
            "\tTile 13/49\n",
            "\tTile 14/49\n",
            "\tTile 15/49\n",
            "\tTile 16/49\n",
            "\tTile 17/49\n",
            "\tTile 18/49\n",
            "\tTile 19/49\n",
            "\tTile 20/49\n",
            "\tTile 21/49\n",
            "\tTile 22/49\n",
            "\tTile 23/49\n",
            "\tTile 24/49\n",
            "\tTile 25/49\n",
            "\tTile 26/49\n",
            "\tTile 27/49\n",
            "\tTile 28/49\n",
            "\tTile 29/49\n",
            "\tTile 30/49\n",
            "\tTile 31/49\n",
            "\tTile 32/49\n",
            "\tTile 33/49\n",
            "\tTile 34/49\n",
            "\tTile 35/49\n",
            "\tTile 36/49\n",
            "\tTile 37/49\n",
            "\tTile 38/49\n",
            "\tTile 39/49\n",
            "\tTile 40/49\n",
            "\tTile 41/49\n",
            "\tTile 42/49\n",
            "\tTile 43/49\n",
            "\tTile 44/49\n",
            "\tTile 45/49\n",
            "\tTile 46/49\n",
            "\tTile 47/49\n",
            "\tTile 48/49\n",
            "\tTile 49/49\n",
            "\tTile 1/84\n",
            "\tTile 2/84\n",
            "\tTile 3/84\n",
            "\tTile 4/84\n",
            "\tTile 5/84\n",
            "\tTile 6/84\n",
            "\tTile 7/84\n",
            "\tTile 8/84\n",
            "\tTile 9/84\n",
            "\tTile 10/84\n",
            "\tTile 11/84\n",
            "\tTile 12/84\n",
            "\tTile 13/84\n",
            "\tTile 14/84\n",
            "\tTile 15/84\n",
            "\tTile 16/84\n",
            "\tTile 17/84\n",
            "\tTile 18/84\n",
            "\tTile 19/84\n",
            "\tTile 20/84\n",
            "\tTile 21/84\n",
            "\tTile 22/84\n",
            "\tTile 23/84\n",
            "\tTile 24/84\n",
            "\tTile 25/84\n",
            "\tTile 26/84\n",
            "\tTile 27/84\n",
            "\tTile 28/84\n",
            "\tTile 29/84\n",
            "\tTile 30/84\n",
            "\tTile 31/84\n",
            "\tTile 32/84\n",
            "\tTile 33/84\n",
            "\tTile 34/84\n",
            "\tTile 35/84\n",
            "\tTile 36/84\n",
            "\tTile 37/84\n",
            "\tTile 38/84\n",
            "\tTile 39/84\n",
            "\tTile 40/84\n",
            "\tTile 41/84\n",
            "\tTile 42/84\n",
            "\tTile 43/84\n",
            "\tTile 44/84\n",
            "\tTile 45/84\n",
            "\tTile 46/84\n",
            "\tTile 47/84\n",
            "\tTile 48/84\n",
            "\tTile 49/84\n",
            "\tTile 50/84\n",
            "\tTile 51/84\n",
            "\tTile 52/84\n",
            "\tTile 53/84\n",
            "\tTile 54/84\n",
            "\tTile 55/84\n",
            "\tTile 56/84\n",
            "\tTile 57/84\n",
            "\tTile 58/84\n",
            "\tTile 59/84\n",
            "\tTile 60/84\n",
            "\tTile 61/84\n",
            "\tTile 62/84\n",
            "\tTile 63/84\n",
            "\tTile 64/84\n",
            "\tTile 65/84\n",
            "\tTile 66/84\n",
            "\tTile 67/84\n",
            "\tTile 68/84\n",
            "\tTile 69/84\n",
            "\tTile 70/84\n",
            "\tTile 71/84\n",
            "\tTile 72/84\n",
            "\tTile 73/84\n",
            "\tTile 74/84\n",
            "\tTile 75/84\n",
            "\tTile 76/84\n",
            "\tTile 77/84\n",
            "\tTile 78/84\n",
            "\tTile 79/84\n",
            "\tTile 80/84\n",
            "\tTile 81/84\n",
            "\tTile 82/84\n",
            "\tTile 83/84\n",
            "\tTile 84/84\n",
            "\tTile 1/36\n",
            "\tTile 2/36\n",
            "\tTile 3/36\n",
            "\tTile 4/36\n",
            "\tTile 5/36\n",
            "\tTile 6/36\n",
            "\tTile 7/36\n",
            "\tTile 8/36\n",
            "\tTile 9/36\n",
            "\tTile 10/36\n",
            "\tTile 11/36\n",
            "\tTile 12/36\n",
            "\tTile 13/36\n",
            "\tTile 14/36\n",
            "\tTile 15/36\n",
            "\tTile 16/36\n",
            "\tTile 17/36\n",
            "\tTile 18/36\n",
            "\tTile 19/36\n",
            "\tTile 20/36\n",
            "\tTile 21/36\n",
            "\tTile 22/36\n",
            "\tTile 23/36\n",
            "\tTile 24/36\n",
            "\tTile 25/36\n",
            "\tTile 26/36\n",
            "\tTile 27/36\n",
            "\tTile 28/36\n",
            "\tTile 29/36\n",
            "\tTile 30/36\n",
            "\tTile 31/36\n",
            "\tTile 32/36\n",
            "\tTile 33/36\n",
            "\tTile 34/36\n",
            "\tTile 35/36\n",
            "\tTile 36/36\n",
            "\tTile 1/48\n",
            "\tTile 2/48\n",
            "\tTile 3/48\n",
            "\tTile 4/48\n",
            "\tTile 5/48\n",
            "\tTile 6/48\n",
            "\tTile 7/48\n",
            "\tTile 8/48\n",
            "\tTile 9/48\n",
            "\tTile 10/48\n",
            "\tTile 11/48\n",
            "\tTile 12/48\n",
            "\tTile 13/48\n",
            "\tTile 14/48\n",
            "\tTile 15/48\n",
            "\tTile 16/48\n",
            "\tTile 17/48\n",
            "\tTile 18/48\n",
            "\tTile 19/48\n",
            "\tTile 20/48\n",
            "\tTile 21/48\n",
            "\tTile 22/48\n",
            "\tTile 23/48\n",
            "\tTile 24/48\n",
            "\tTile 25/48\n",
            "\tTile 26/48\n",
            "\tTile 27/48\n",
            "\tTile 28/48\n",
            "\tTile 29/48\n",
            "\tTile 30/48\n",
            "\tTile 31/48\n",
            "\tTile 32/48\n",
            "\tTile 33/48\n",
            "\tTile 34/48\n",
            "\tTile 35/48\n",
            "\tTile 36/48\n",
            "\tTile 37/48\n",
            "\tTile 38/48\n",
            "\tTile 39/48\n",
            "\tTile 40/48\n",
            "\tTile 41/48\n",
            "\tTile 42/48\n",
            "\tTile 43/48\n",
            "\tTile 44/48\n",
            "\tTile 45/48\n",
            "\tTile 46/48\n",
            "\tTile 47/48\n",
            "\tTile 48/48\n",
            "\tTile 1/36\n",
            "\tTile 2/36\n",
            "\tTile 3/36\n",
            "\tTile 4/36\n",
            "\tTile 5/36\n",
            "\tTile 6/36\n",
            "\tTile 7/36\n",
            "\tTile 8/36\n",
            "\tTile 9/36\n",
            "\tTile 10/36\n",
            "\tTile 11/36\n",
            "\tTile 12/36\n",
            "\tTile 13/36\n",
            "\tTile 14/36\n",
            "\tTile 15/36\n",
            "\tTile 16/36\n",
            "\tTile 17/36\n",
            "\tTile 18/36\n",
            "\tTile 19/36\n",
            "\tTile 20/36\n",
            "\tTile 21/36\n",
            "\tTile 22/36\n",
            "\tTile 23/36\n",
            "\tTile 24/36\n",
            "\tTile 25/36\n",
            "\tTile 26/36\n",
            "\tTile 27/36\n",
            "\tTile 28/36\n",
            "\tTile 29/36\n",
            "\tTile 30/36\n",
            "\tTile 31/36\n",
            "\tTile 32/36\n",
            "\tTile 33/36\n",
            "\tTile 34/36\n",
            "\tTile 35/36\n",
            "\tTile 36/36\n",
            "\tTile 1/28\n",
            "\tTile 2/28\n",
            "\tTile 3/28\n",
            "\tTile 4/28\n",
            "\tTile 5/28\n",
            "\tTile 6/28\n",
            "\tTile 7/28\n",
            "\tTile 8/28\n",
            "\tTile 9/28\n",
            "\tTile 10/28\n",
            "\tTile 11/28\n",
            "\tTile 12/28\n",
            "\tTile 13/28\n",
            "\tTile 14/28\n",
            "\tTile 15/28\n",
            "\tTile 16/28\n",
            "\tTile 17/28\n",
            "\tTile 18/28\n",
            "\tTile 19/28\n",
            "\tTile 20/28\n",
            "\tTile 21/28\n",
            "\tTile 22/28\n",
            "\tTile 23/28\n",
            "\tTile 24/28\n",
            "\tTile 25/28\n",
            "\tTile 26/28\n",
            "\tTile 27/28\n",
            "\tTile 28/28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download the results\n",
        "zip_path = \"/content/HAT200G.zip\"\n",
        "folder_to_zip = \"/content/HAT/results/HAT-L_SRx4_ImageNet-pretrain/visualization/customSatelliteIMages\"\n",
        "shutil.make_archive(zip_path.replace(\".zip\",\"\"), 'zip', folder_to_zip)\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "id": "Rwe6VVXHcL_a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7387f685-471f-4320-f4cf-859e56fada87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_78fe2cdc-9dbc-4c8c-81c8-6cfa5d89a9e0\", \"HAT200G.zip\", 42155701)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zdKNyAuyFXbz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}