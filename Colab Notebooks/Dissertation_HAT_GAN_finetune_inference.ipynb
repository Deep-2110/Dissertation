{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qb0ilw6QZUE6",
        "Qw0fb540Fcke",
        "pbeiQJEyP5NI",
        "gKNnhIQb8cc9"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Readme"
      ],
      "metadata": {
        "id": "qb0ilw6QZUE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-requisites\n",
        "*   Set up weight and bias account: [Wandb](https://wandb.ai/site/)\n",
        "*   Upload dataset on drive following the hierarchy below: <br>\n",
        "<pre>\n",
        "MyDrive\n",
        "|_ Final_project\n",
        "   |_ 2000dataset.zip\n",
        "</pre>\n",
        "\n",
        "*   Upload the generator and the zip containing the source code in drive following the herachy below:\n",
        "<pre>\n",
        "MyDrive\n",
        "|_ Final_project\n",
        "   |_ HAT\n",
        "      |_ HAT.zip\n",
        "      |_Real_HAT_GAN_SRx4.pth\n",
        "</pre>\n",
        "\n",
        "*   for **inference** using generator obtained after fine tuning on the dataset, ensure net_g_5000.pth is at the hierarchy below:\n",
        "\n",
        " <pre>\n",
        "MyDrive\n",
        "|_ Final_project\n",
        "   |_ HAT\n",
        "      |_ net_g_5000.pth\n",
        "</pre>\n",
        "\n",
        "\n",
        "**Fine-Tuning**: Follow steps 1 to 4 in order. <br> <br>\n",
        "**Inference**:\n",
        "*   The HAT code and the generator is required for inference.\n",
        "*   Run step 1 and Step 5. *Note: At step 5.2, replace the dataroot_lq with correct LR image path to run inference on and prertain_network_g with correct generator path.*"
      ],
      "metadata": {
        "id": "tynv1wg-9amm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 Install dependencies and login to wandb"
      ],
      "metadata": {
        "id": "_ye5C6A-Pt0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq\n",
        "!pip install basicsr==1.3.4.9 -qqq\n",
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MumR80xdX1g4",
        "outputId": "fa4633bf-3d36-4099-916e-d4004927f8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/161.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m153.6/161.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.2/161.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.0/303.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for basicsr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files, drive\n",
        "import os\n",
        "import wandb\n",
        "import shutil"
      ],
      "metadata": {
        "id": "N9RX-eq-QAIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XVrKozUZJ1o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a5a81e-354a-4f75-ffef-8f24aa0f770c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mount drive to colab\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and log in to Weights & Biases; not required if doing inference only\n",
        "#0f804b966fe713df71643f26ffb10339e1cd94ca\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "kJ1nyj8kZJpR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "d44dbd7c-6cce-41da-f704-56ce673d85a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdsha43925\u001b[0m (\u001b[33mdsha43925-middlesex-university-mauritius\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Corrects import line from degradations.py as this version is obsolete\n",
        "#https://github.com/xinntao/Real-ESRGAN/issues/801\n",
        "#Note: if python version was updated on colab, change the file_path to correct one.\n",
        "\n",
        "file_path = \"/usr/local/lib/python3.12/dist-packages/basicsr/data/degradations.py\"\n",
        "line_to_replace = \"from torchvision.transforms.functional_tensor import rgb_to_grayscale\"\n",
        "new_line = \"from torchvision.transforms.functional import rgb_to_grayscale\"\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "lines = [new_line if line.strip() == line_to_replace else line for line in lines]\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    file.writelines(lines)\n",
        "\n",
        "print(f\"Replacing line '{line_to_replace}' with '{new_line}' in file {file_path}\")"
      ],
      "metadata": {
        "id": "QlHzbRq7YvBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54ac9334-29d5-494c-d782-7ddba7306478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replacing line 'from torchvision.transforms.functional_tensor import rgb_to_grayscale' with 'from torchvision.transforms.functional import rgb_to_grayscale' in file /usr/local/lib/python3.12/dist-packages/basicsr/data/degradations.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Get code"
      ],
      "metadata": {
        "id": "Qw0fb540Fcke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get code from drive and set up environment for HAT\n",
        "code_zip_path = '/content/drive/MyDrive/Final_project/HAT/HAT.zip'\n",
        "unzip_path = '/content'\n",
        "!unzip -q $code_zip_path -d $unzip_path\n",
        "!ls -l $unzip_path\n",
        "%cd /content/HAT\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop"
      ],
      "metadata": {
        "id": "Ofxyulr1bJnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dcd38e1-b775-4203-9483-397d21a700d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12\n",
            "drwx------ 5 root root 4096 Sep 14 13:15 drive\n",
            "drwxrwxrwx 7 root root 4096 Sep  4 06:15 HAT\n",
            "drwxr-xr-x 1 root root 4096 Sep  9 13:46 sample_data\n",
            "/content/HAT\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.8.0+cu126)\n",
            "Requirement already satisfied: basicsr==1.3.4.9 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.3.4.9)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (4.12.0.88)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.32.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.16.1)\n",
            "Requirement already satisfied: tb-nightly in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.21.0a20250914)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.12/dist-packages (from basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.43.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.7->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.7->-r requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr==1.3.4.9->-r requirements.txt (line 3)) (2025.8.3)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr==1.3.4.9->-r requirements.txt (line 3)) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr==1.3.4.9->-r requirements.txt (line 3)) (2025.8.28)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr==1.3.4.9->-r requirements.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (1.74.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.9)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (5.29.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr==1.3.4.9->-r requirements.txt (line 3)) (3.1.3)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.12/dist-packages (from yapf->basicsr==1.3.4.9->-r requirements.txt (line 3)) (4.4.0)\n",
            "/usr/local/lib/python3.12/dist-packages/setuptools/__init__.py:94: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Requirements should be satisfied by a PEP 517 installer.\n",
            "        If you are using pip, you can try `pip install --use-pep517`.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  dist.fetch_build_eggs(dist.setup_requires)\n",
            "running develop\n",
            "/usr/local/lib/python3.12/dist-packages/setuptools/command/develop.py:41: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  easy_install.initialize_options(self)\n",
            "/usr/local/lib/python3.12/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running egg_info\n",
            "creating hat.egg-info\n",
            "writing hat.egg-info/PKG-INFO\n",
            "writing dependency_links to hat.egg-info/dependency_links.txt\n",
            "writing requirements to hat.egg-info/requires.txt\n",
            "writing top-level names to hat.egg-info/top_level.txt\n",
            "writing manifest file 'hat.egg-info/SOURCES.txt'\n",
            "reading manifest file 'hat.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'hat.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.12/dist-packages/hat.egg-link (link to .)\n",
            "Adding hat 0.1.0 to easy-install.pth file\n",
            "\n",
            "Installed /content/HAT\n",
            "Processing dependencies for hat==0.1.0\n",
            "Searching for basicsr==1.3.4.9\n",
            "Best match: basicsr 1.3.4.9\n",
            "Adding basicsr 1.3.4.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for torch==2.8.0+cu126\n",
            "Best match: torch 2.8.0+cu126\n",
            "Adding torch 2.8.0+cu126 to easy-install.pth file\n",
            "Installing torchfrtrace script to /usr/local/bin\n",
            "Installing torchrun script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for einops==0.8.1\n",
            "Best match: einops 0.8.1\n",
            "Adding einops 0.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for yapf==0.43.0\n",
            "Best match: yapf 0.43.0\n",
            "Adding yapf 0.43.0 to easy-install.pth file\n",
            "Installing yapf script to /usr/local/bin\n",
            "Installing yapf-diff script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for tqdm==4.67.1\n",
            "Best match: tqdm 4.67.1\n",
            "Adding tqdm 4.67.1 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for torchvision==0.23.0+cu126\n",
            "Best match: torchvision 0.23.0+cu126\n",
            "Adding torchvision 0.23.0+cu126 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for tb-nightly==2.21.0a20250914\n",
            "Best match: tb-nightly 2.21.0a20250914\n",
            "Adding tb-nightly 2.21.0a20250914 to easy-install.pth file\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for scipy==1.16.1\n",
            "Best match: scipy 1.16.1\n",
            "Adding scipy 1.16.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for scikit-image==0.25.2\n",
            "Best match: scikit-image 0.25.2\n",
            "Adding scikit-image 0.25.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for requests==2.32.4\n",
            "Best match: requests 2.32.4\n",
            "Adding requests 2.32.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for PyYAML==6.0.2\n",
            "Best match: PyYAML 6.0.2\n",
            "Adding PyYAML 6.0.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for pillow==11.3.0\n",
            "Best match: pillow 11.3.0\n",
            "Adding pillow 11.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for opencv-python==4.12.0.88\n",
            "Best match: opencv-python 4.12.0.88\n",
            "Adding opencv-python 4.12.0.88 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for numpy==2.0.2\n",
            "Best match: numpy 2.0.2\n",
            "Adding numpy 2.0.2 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing numpy-config script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for lmdb==1.7.3\n",
            "Best match: lmdb 1.7.3\n",
            "Adding lmdb 1.7.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for future==1.0.0\n",
            "Best match: future 1.0.0\n",
            "Adding future 1.0.0 to easy-install.pth file\n",
            "Installing futurize script to /usr/local/bin\n",
            "Installing pasteurize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for addict==2.4.0\n",
            "Best match: addict 2.4.0\n",
            "Adding addict 2.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for triton==3.4.0\n",
            "Best match: triton 3.4.0\n",
            "Adding triton 3.4.0 to easy-install.pth file\n",
            "Installing proton script to /usr/local/bin\n",
            "Installing proton-viewer script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cufile-cu12==1.11.1.6\n",
            "Best match: nvidia-cufile-cu12 1.11.1.6\n",
            "Adding nvidia-cufile-cu12 1.11.1.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-nvjitlink-cu12==12.6.85\n",
            "Best match: nvidia-nvjitlink-cu12 12.6.85\n",
            "Adding nvidia-nvjitlink-cu12 12.6.85 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-nvtx-cu12==12.6.77\n",
            "Best match: nvidia-nvtx-cu12 12.6.77\n",
            "Adding nvidia-nvtx-cu12 12.6.77 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-nccl-cu12==2.27.3\n",
            "Best match: nvidia-nccl-cu12 2.27.3\n",
            "Adding nvidia-nccl-cu12 2.27.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cusparselt-cu12==0.7.1\n",
            "Best match: nvidia-cusparselt-cu12 0.7.1\n",
            "Adding nvidia-cusparselt-cu12 0.7.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cusparse-cu12==12.5.4.2\n",
            "Best match: nvidia-cusparse-cu12 12.5.4.2\n",
            "Adding nvidia-cusparse-cu12 12.5.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cusolver-cu12==11.7.1.2\n",
            "Best match: nvidia-cusolver-cu12 11.7.1.2\n",
            "Adding nvidia-cusolver-cu12 11.7.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-curand-cu12==10.3.7.77\n",
            "Best match: nvidia-curand-cu12 10.3.7.77\n",
            "Adding nvidia-curand-cu12 10.3.7.77 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cufft-cu12==11.3.0.4\n",
            "Best match: nvidia-cufft-cu12 11.3.0.4\n",
            "Adding nvidia-cufft-cu12 11.3.0.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cublas-cu12==12.6.4.1\n",
            "Best match: nvidia-cublas-cu12 12.6.4.1\n",
            "Adding nvidia-cublas-cu12 12.6.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cudnn-cu12==9.10.2.21\n",
            "Best match: nvidia-cudnn-cu12 9.10.2.21\n",
            "Adding nvidia-cudnn-cu12 9.10.2.21 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cuda-cupti-cu12==12.6.80\n",
            "Best match: nvidia-cuda-cupti-cu12 12.6.80\n",
            "Adding nvidia-cuda-cupti-cu12 12.6.80 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cuda-runtime-cu12==12.6.77\n",
            "Best match: nvidia-cuda-runtime-cu12 12.6.77\n",
            "Adding nvidia-cuda-runtime-cu12 12.6.77 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cuda-nvrtc-cu12==12.6.77\n",
            "Best match: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "Adding nvidia-cuda-nvrtc-cu12 12.6.77 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for fsspec==2025.3.0\n",
            "Best match: fsspec 2025.3.0\n",
            "Adding fsspec 2025.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for jinja2==3.1.6\n",
            "Best match: jinja2 3.1.6\n",
            "Adding jinja2 3.1.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for networkx==3.5\n",
            "Best match: networkx 3.5\n",
            "Adding networkx 3.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for sympy==1.13.3\n",
            "Best match: sympy 1.13.3\n",
            "Adding sympy 1.13.3 to easy-install.pth file\n",
            "Installing isympy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for setuptools==75.2.0\n",
            "Best match: setuptools 75.2.0\n",
            "Adding setuptools 75.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for typing-extensions==4.15.0\n",
            "Best match: typing-extensions 4.15.0\n",
            "Adding typing-extensions 4.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for filelock==3.19.1\n",
            "Best match: filelock 3.19.1\n",
            "Adding filelock 3.19.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for platformdirs==4.4.0\n",
            "Best match: platformdirs 4.4.0\n",
            "Adding platformdirs 4.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for werkzeug==3.1.3\n",
            "Best match: werkzeug 3.1.3\n",
            "Adding werkzeug 3.1.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for tensorboard-data-server==0.7.2\n",
            "Best match: tensorboard-data-server 0.7.2\n",
            "Adding tensorboard-data-server 0.7.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for protobuf==5.29.5\n",
            "Best match: protobuf 5.29.5\n",
            "Adding protobuf 5.29.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for packaging==25.0\n",
            "Best match: packaging 25.0\n",
            "Adding packaging 25.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for markdown==3.9\n",
            "Best match: markdown 3.9\n",
            "Adding markdown 3.9 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for grpcio==1.74.0\n",
            "Best match: grpcio 1.74.0\n",
            "Adding grpcio 1.74.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for absl-py==1.4.0\n",
            "Best match: absl-py 1.4.0\n",
            "Adding absl-py 1.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for lazy-loader==0.4\n",
            "Best match: lazy-loader 0.4\n",
            "Adding lazy-loader 0.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for tifffile==2025.8.28\n",
            "Best match: tifffile 2025.8.28\n",
            "Adding tifffile 2025.8.28 to easy-install.pth file\n",
            "Installing lsm2bin script to /usr/local/bin\n",
            "Installing tiff2fsspec script to /usr/local/bin\n",
            "Installing tiffcomment script to /usr/local/bin\n",
            "Installing tifffile script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for imageio==2.37.0\n",
            "Best match: imageio 2.37.0\n",
            "Adding imageio 2.37.0 to easy-install.pth file\n",
            "Installing imageio_download_bin script to /usr/local/bin\n",
            "Installing imageio_remove_bin script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for certifi==2025.8.3\n",
            "Best match: certifi 2025.8.3\n",
            "Adding certifi 2025.8.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for urllib3==2.5.0\n",
            "Best match: urllib3 2.5.0\n",
            "Adding urllib3 2.5.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for idna==3.10\n",
            "Best match: idna 3.10\n",
            "Adding idna 3.10 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for charset-normalizer==3.4.3\n",
            "Best match: charset-normalizer 3.4.3\n",
            "Adding charset-normalizer 3.4.3 to easy-install.pth file\n",
            "Installing normalizer script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for MarkupSafe==3.0.2\n",
            "Best match: MarkupSafe 3.0.2\n",
            "Adding MarkupSafe 3.0.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for mpmath==1.3.0\n",
            "Best match: mpmath 1.3.0\n",
            "Adding mpmath 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Finished processing dependencies for hat==0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 Get dataset from drive"
      ],
      "metadata": {
        "id": "pbeiQJEyP5NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_zip_path = '/content/drive/MyDrive/Final_project/2000dataset.zip'\n",
        "!unzip -q $dataset_zip_path -d $unzip_path"
      ],
      "metadata": {
        "id": "ND5jt3APZhc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rm \"/content/dataset/train/lr/desktop.ini\""
      ],
      "metadata": {
        "id": "wer2wOAZ8j2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4 Get pre-trained model from drive"
      ],
      "metadata": {
        "id": "0D00Gc9oP8fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/Final_project/HAT/Real_HAT_GAN_SRx4.pth\" \\\n",
        "   \"/content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth\""
      ],
      "metadata": {
        "id": "L4F7RhHqZ34e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth\"\n",
        "\n",
        "# Check file exists and size\n",
        "if os.path.exists(model_path):\n",
        "    size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
        "    print(f\"Model found! Size: {size_mb:.1f} MB\")\n",
        "    # Expected size: 81.2 MB for medium and 158 MB for Large\n",
        "else:\n",
        "    print(\"Error: File not copied correctly!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_vLm3eGZ-eN",
        "outputId": "dc0c1ca9-cf6c-4698-922f-6bdf979d59b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model found! Size: 162.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5 Fine-tune pre-trained model on satellite images"
      ],
      "metadata": {
        "id": "-uWT_a5b8R12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Create configuration file for training"
      ],
      "metadata": {
        "id": "4VnDx2P3XIXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the YAML content\n",
        "yaml_content = \"\"\"# general settings\n",
        "name: train_Real_HAT_GAN_SRx4_finetune_from_mse_model\n",
        "model_type: RealHATGANModel\n",
        "scale: 4\n",
        "num_gpu: auto\n",
        "manual_seed: 100\n",
        "\n",
        "# USM the ground-truth\n",
        "l1_gt_usm: True\n",
        "percep_gt_usm: True\n",
        "gan_gt_usm: False\n",
        "high_order_degradation: False\n",
        "\n",
        "# dataset and data loader settings\n",
        "datasets:\n",
        "  train:\n",
        "    name: sat_images\n",
        "    type: PairedImageDataset\n",
        "    dataroot_gt: /content/dataset/train/hr\n",
        "    dataroot_lq: /content/dataset/train/lr\n",
        "    meta_info: /content/dataset/meta_info/meta_info_satelliteimages_train.txt\n",
        "    io_backend:\n",
        "      type: disk\n",
        "\n",
        "    gt_size: 256\n",
        "    use_hflip: true\n",
        "    use_rot: true\n",
        "\n",
        "    # data loader\n",
        "    use_shuffle: true\n",
        "    num_worker_per_gpu: 1\n",
        "    batch_size_per_gpu: 2\n",
        "    dataset_enlarge_ratio: 2\n",
        "    prefetch_mode: ~\n",
        "\n",
        "  val_1:\n",
        "    name: val_sat_images\n",
        "    type: PairedImageDataset\n",
        "    dataroot_gt: /content/dataset/val/hr\n",
        "    dataroot_lq: /content/dataset/val/lr\n",
        "    meta_info: /content/dataset/meta_info/meta_info_satelliteimages_val.txt\n",
        "    io_backend:\n",
        "      type: disk\n",
        "\n",
        "# network structures\n",
        "network_g:\n",
        "  type: HAT\n",
        "  upscale: 4\n",
        "  in_chans: 3\n",
        "  img_size: 64\n",
        "  window_size: 16\n",
        "  compress_ratio: 3\n",
        "  squeeze_factor: 30\n",
        "  conv_scale: 0.01\n",
        "  overlap_ratio: 0.5\n",
        "  img_range: 1.\n",
        "  depths: [6, 6, 6, 6, 6, 6]\n",
        "  embed_dim: 180\n",
        "  num_heads: [6, 6, 6, 6, 6, 6]\n",
        "  mlp_ratio: 2\n",
        "  upsampler: 'pixelshuffle'\n",
        "  resi_connection: '1conv'\n",
        "\n",
        "network_d:\n",
        "  type: UNetDiscriminatorSN\n",
        "  num_in_ch: 3\n",
        "  num_feat: 64\n",
        "  skip_connection: True\n",
        "\n",
        "# path\n",
        "path:\n",
        "  pretrain_network_g: /content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth\n",
        "  param_key_g: 'params_ema'\n",
        "  strict_load_g: true\n",
        "  resume_state: ~\n",
        "\n",
        "# training settings\n",
        "train:\n",
        "  ema_decay: 0.999\n",
        "  optim_g:\n",
        "    type: Adam\n",
        "    lr: !!float 2e-5\n",
        "    weight_decay: 0\n",
        "    betas: [0.9, 0.99]\n",
        "  optim_d:\n",
        "    type: Adam\n",
        "    lr: !!float 2e-5\n",
        "    weight_decay: 0\n",
        "    betas: [0.9, 0.99]\n",
        "\n",
        "  scheduler:\n",
        "    type: MultiStepLR\n",
        "    milestones: [1200, 2500, 3800]\n",
        "    gamma: 0.5\n",
        "\n",
        "  total_iter: 5000\n",
        "  warmup_iter: -1  # no warm up\n",
        "\n",
        "  # losses\n",
        "  pixel_opt:\n",
        "    type: L1Loss\n",
        "    loss_weight: 1.0\n",
        "    reduction: mean\n",
        "  perceptual_opt:\n",
        "    type: PerceptualLoss\n",
        "    layer_weights:\n",
        "      # before relu\n",
        "      'conv1_2': 0.1\n",
        "      'conv2_2': 0.1\n",
        "      'conv3_4': 1\n",
        "      'conv4_4': 1\n",
        "      'conv5_4': 1\n",
        "    vgg_type: vgg19\n",
        "    use_input_norm: true\n",
        "    perceptual_weight: !!float 1.0\n",
        "    style_weight: 0\n",
        "    range_norm: false\n",
        "    criterion: l1\n",
        "  # gan loss\n",
        "  gan_opt:\n",
        "    type: GANLoss\n",
        "    gan_type: vanilla\n",
        "    real_label_val: 1.0\n",
        "    fake_label_val: 0.0\n",
        "    loss_weight: !!float 1e-1\n",
        "\n",
        "  net_d_iters: 1\n",
        "  net_d_init_iters: 0\n",
        "\n",
        "\n",
        "# validation settings\n",
        "val:\n",
        "  val_freq: 100\n",
        "  save_img: true\n",
        "  pbar: False\n",
        "\n",
        "  metrics:\n",
        "    psnr:\n",
        "      type: calculate_psnr\n",
        "      crop_border: 4\n",
        "      test_y_channel: true\n",
        "      better: higher\n",
        "    ssim:\n",
        "      type: calculate_ssim\n",
        "      crop_border: 4\n",
        "      test_y_channel: true\n",
        "      better: higher\n",
        "    niqe:\n",
        "      type: calculate_niqe\n",
        "      crop_border: 4\n",
        "      input_order: 'HWC'\n",
        "      convert_to: 'y'\n",
        "\n",
        "# logging settings\n",
        "logger:\n",
        "  print_freq: 100\n",
        "  save_checkpoint_freq: 500\n",
        "  use_tb_logger: true\n",
        "  wandb:\n",
        "    project: HAT-satellite-SR\n",
        "    resume_id: ~\n",
        "\n",
        "# dist training settings\n",
        "dist_params:\n",
        "  backend: nccl\n",
        "  port: 29500\n",
        "\"\"\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "!mkdir -p /content/HAT/options/train/\n",
        "\n",
        "# Write to the YAML file\n",
        "with open('/content/HAT/options/train/train_HAT-GAN_SRx4_finetune.yml', 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "# Verify the file was created\n",
        "!ls -l /content/HAT/options/train/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ918WjeXO8I",
        "outputId": "57a33ab8-d9bc-40b9-ce8b-a2159af7b43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 3195 Sep 13 08:02 train_HAT-GAN_SRx4_finetune.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Fine tune model"
      ],
      "metadata": {
        "id": "gKNnhIQb8cc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 --master_port=4321 hat/train.py -opt /content/HAT/options/train/train_HAT-GAN_SRx4_finetune.yml --launcher none"
      ],
      "metadata": {
        "id": "Zr4TDlYoavJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc8d5cc-91f3-4d96-d6ba-83c82567278a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disable distributed.\n",
            "2025-09-13 08:02:37,487 INFO: \n",
            "                ____                _       _____  ____\n",
            "               / __ ) ____ _ _____ (_)_____/ ___/ / __ \\\n",
            "              / __  |/ __ `// ___// // ___/\\__ \\ / /_/ /\n",
            "             / /_/ // /_/ /(__  )/ // /__ ___/ // _, _/\n",
            "            /_____/ \\__,_//____//_/ \\___//____//_/ |_|\n",
            "     ______                   __   __                 __      __\n",
            "    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /\n",
            "   / / __ / __ \\ / __ \\ / __  /  / /   / / / // ___// //_/  / /\n",
            "  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/\n",
            "  \\____/ \\____/ \\____/ \\____/  /_____/\\____/ \\___//_/|_|  (_)\n",
            "    \n",
            "Version Information: \n",
            "\tBasicSR: 1.3.4.9\n",
            "\tPyTorch: 2.8.0+cu126\n",
            "\tTorchVision: 0.23.0+cu126\n",
            "2025-09-13 08:02:37,488 INFO: \n",
            "  name: train_Real_HAT_GAN_SRx4_finetune_from_mse_model\n",
            "  model_type: RealHATGANModel\n",
            "  scale: 4\n",
            "  num_gpu: 1\n",
            "  manual_seed: 100\n",
            "  l1_gt_usm: True\n",
            "  percep_gt_usm: True\n",
            "  gan_gt_usm: False\n",
            "  high_order_degradation: False\n",
            "  datasets:[\n",
            "    train:[\n",
            "      name: sat_images\n",
            "      type: PairedImageDataset\n",
            "      dataroot_gt: /content/dataset/train/hr\n",
            "      dataroot_lq: /content/dataset/train/lr\n",
            "      meta_info: /content/dataset/meta_info/meta_info_satelliteimages_train.txt\n",
            "      io_backend:[\n",
            "        type: disk\n",
            "      ]\n",
            "      gt_size: 256\n",
            "      use_hflip: True\n",
            "      use_rot: True\n",
            "      use_shuffle: True\n",
            "      num_worker_per_gpu: 1\n",
            "      batch_size_per_gpu: 2\n",
            "      dataset_enlarge_ratio: 2\n",
            "      prefetch_mode: None\n",
            "      phase: train\n",
            "      scale: 4\n",
            "    ]\n",
            "    val_1:[\n",
            "      name: val_sat_images\n",
            "      type: PairedImageDataset\n",
            "      dataroot_gt: /content/dataset/val/hr\n",
            "      dataroot_lq: /content/dataset/val/lr\n",
            "      meta_info: /content/dataset/meta_info/meta_info_satelliteimages_val.txt\n",
            "      io_backend:[\n",
            "        type: disk\n",
            "      ]\n",
            "      phase: val\n",
            "      scale: 4\n",
            "    ]\n",
            "  ]\n",
            "  network_g:[\n",
            "    type: HAT\n",
            "    upscale: 4\n",
            "    in_chans: 3\n",
            "    img_size: 64\n",
            "    window_size: 16\n",
            "    compress_ratio: 3\n",
            "    squeeze_factor: 30\n",
            "    conv_scale: 0.01\n",
            "    overlap_ratio: 0.5\n",
            "    img_range: 1.0\n",
            "    depths: [6, 6, 6, 6, 6, 6]\n",
            "    embed_dim: 180\n",
            "    num_heads: [6, 6, 6, 6, 6, 6]\n",
            "    mlp_ratio: 2\n",
            "    upsampler: pixelshuffle\n",
            "    resi_connection: 1conv\n",
            "  ]\n",
            "  network_d:[\n",
            "    type: UNetDiscriminatorSN\n",
            "    num_in_ch: 3\n",
            "    num_feat: 64\n",
            "    skip_connection: True\n",
            "  ]\n",
            "  path:[\n",
            "    pretrain_network_g: /content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth\n",
            "    param_key_g: params_ema\n",
            "    strict_load_g: True\n",
            "    resume_state: None\n",
            "    experiments_root: /content/HAT/experiments/train_Real_HAT_GAN_SRx4_finetune_from_mse_model\n",
            "    models: /content/HAT/experiments/train_Real_HAT_GAN_SRx4_finetune_from_mse_model/models\n",
            "    training_states: /content/HAT/experiments/train_Real_HAT_GAN_SRx4_finetune_from_mse_model/training_states\n",
            "    log: /content/HAT/experiments/train_Real_HAT_GAN_SRx4_finetune_from_mse_model\n",
            "    visualization: /content/HAT/experiments/train_Real_HAT_GAN_SRx4_finetune_from_mse_model/visualization\n",
            "  ]\n",
            "  train:[\n",
            "    ema_decay: 0.999\n",
            "    optim_g:[\n",
            "      type: Adam\n",
            "      lr: 2e-05\n",
            "      weight_decay: 0\n",
            "      betas: [0.9, 0.99]\n",
            "    ]\n",
            "    optim_d:[\n",
            "      type: Adam\n",
            "      lr: 2e-05\n",
            "      weight_decay: 0\n",
            "      betas: [0.9, 0.99]\n",
            "    ]\n",
            "    scheduler:[\n",
            "      type: MultiStepLR\n",
            "      milestones: [1200, 2500, 3800]\n",
            "      gamma: 0.5\n",
            "    ]\n",
            "    total_iter: 5000\n",
            "    warmup_iter: -1\n",
            "    pixel_opt:[\n",
            "      type: L1Loss\n",
            "      loss_weight: 1.0\n",
            "      reduction: mean\n",
            "    ]\n",
            "    perceptual_opt:[\n",
            "      type: PerceptualLoss\n",
            "      layer_weights:[\n",
            "        conv1_2: 0.1\n",
            "        conv2_2: 0.1\n",
            "        conv3_4: 1\n",
            "        conv4_4: 1\n",
            "        conv5_4: 1\n",
            "      ]\n",
            "      vgg_type: vgg19\n",
            "      use_input_norm: True\n",
            "      perceptual_weight: 1.0\n",
            "      style_weight: 0\n",
            "      range_norm: False\n",
            "      criterion: l1\n",
            "    ]\n",
            "    gan_opt:[\n",
            "      type: GANLoss\n",
            "      gan_type: vanilla\n",
            "      real_label_val: 1.0\n",
            "      fake_label_val: 0.0\n",
            "      loss_weight: 0.1\n",
            "    ]\n",
            "    net_d_iters: 1\n",
            "    net_d_init_iters: 0\n",
            "  ]\n",
            "  val:[\n",
            "    val_freq: 100\n",
            "    save_img: True\n",
            "    pbar: False\n",
            "    metrics:[\n",
            "      psnr:[\n",
            "        type: calculate_psnr\n",
            "        crop_border: 4\n",
            "        test_y_channel: True\n",
            "        better: higher\n",
            "      ]\n",
            "      ssim:[\n",
            "        type: calculate_ssim\n",
            "        crop_border: 4\n",
            "        test_y_channel: True\n",
            "        better: higher\n",
            "      ]\n",
            "      niqe:[\n",
            "        type: calculate_niqe\n",
            "        crop_border: 4\n",
            "        input_order: HWC\n",
            "        convert_to: y\n",
            "      ]\n",
            "    ]\n",
            "  ]\n",
            "  logger:[\n",
            "    print_freq: 100\n",
            "    save_checkpoint_freq: 500\n",
            "    use_tb_logger: True\n",
            "    wandb:[\n",
            "      project: HAT-satellite-SR\n",
            "      resume_id: None\n",
            "    ]\n",
            "  ]\n",
            "  dist_params:[\n",
            "    backend: nccl\n",
            "    port: 29500\n",
            "  ]\n",
            "  dist: False\n",
            "  rank: 0\n",
            "  world_size: 1\n",
            "  auto_resume: False\n",
            "  is_train: True\n",
            "  root_path: /content/HAT\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdsha43925\u001b[0m (\u001b[33mdsha43925-middlesex-university-mauritius\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "2025-09-13 08:02:39.384011: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1757750559.403948    1368 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1757750559.409954    1368 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1757750559.425235    1368 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757750559.425262    1368 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757750559.425266    1368 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757750559.425271    1368 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-13 08:02:39.430053: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m creating run (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m creating run (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m creating run (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/HAT/wandb/run-20250913_080239-etoh0lka\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrain_Real_HAT_GAN_SRx4_finetune_from_mse_model\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsha43925-middlesex-university-mauritius/HAT-satellite-SR\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsha43925-middlesex-university-mauritius/HAT-satellite-SR/runs/etoh0lka\u001b[0m\n",
            "2025-09-13 08:02:44,484 INFO: Use wandb logger with id=etoh0lka; project=HAT-satellite-SR.\n",
            "2025-09-13 08:02:44,532 INFO: Dataset [PairedImageDataset] - sat_images is built.\n",
            "2025-09-13 08:02:44,533 INFO: Training statistics:\n",
            "\tNumber of train images: 2000\n",
            "\tDataset enlarge ratio: 2\n",
            "\tBatch size per gpu: 2\n",
            "\tWorld size (gpu number): 1\n",
            "\tRequire iter number per epoch: 2000\n",
            "\tTotal epochs: 3; iters: 5000.\n",
            "2025-09-13 08:02:44,540 INFO: Dataset [PairedImageDataset] - val_sat_images is built.\n",
            "2025-09-13 08:02:44,541 INFO: Number of val images/folders in val_sat_images: 500\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "2025-09-13 08:02:45,159 INFO: Network [HAT] is created.\n",
            "2025-09-13 08:02:45,399 INFO: Network: HAT, with parameters: 20,772,507\n",
            "2025-09-13 08:02:45,400 INFO: HAT(\n",
            "  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (patch_unembed): PatchUnEmbed()\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (layers): ModuleList(\n",
            "    (0): RHAG(\n",
            "      (residual_group): AttenBlocks(\n",
            "        (blocks): ModuleList(\n",
            "          (0): HAB(\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(180, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(60, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(180, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 180, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): Identity()\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1-5): 5 x HAB(\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(180, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(60, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(180, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 180, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (overlap_attn): OCAB(\n",
            "          (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "          (unfold): Unfold(kernel_size=(24, 24), dilation=1, padding=4, stride=16)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "          (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "    (1-5): 5 x RHAG(\n",
            "      (residual_group): AttenBlocks(\n",
            "        (blocks): ModuleList(\n",
            "          (0-5): 6 x HAB(\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(180, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(60, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(180, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 180, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (overlap_attn): OCAB(\n",
            "          (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "          (unfold): Unfold(kernel_size=(24, 24), dilation=1, padding=4, stride=16)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "          (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_before_upsample): Sequential(\n",
            "    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "  )\n",
            "  (upsample): Upsample(\n",
            "    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): PixelShuffle(upscale_factor=2)\n",
            "    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): PixelShuffle(upscale_factor=2)\n",
            "  )\n",
            "  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n",
            "2025-09-13 08:02:45,656 INFO: Loading HAT model from /content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth, with param key: [params_ema].\n",
            "2025-09-13 08:02:45,765 INFO: Use Exponential Moving Average with decay: 0.999\n",
            "2025-09-13 08:02:46,038 INFO: Network [HAT] is created.\n",
            "2025-09-13 08:02:46,368 INFO: Loading HAT model from /content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth, with param key: [params_ema].\n",
            "2025-09-13 08:02:46,514 INFO: Network [UNetDiscriminatorSN] is created.\n",
            "2025-09-13 08:02:46,520 INFO: Network: UNetDiscriminatorSN, with parameters: 4,376,897\n",
            "2025-09-13 08:02:46,520 INFO: UNetDiscriminatorSN(\n",
            "  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n",
            "2025-09-13 08:02:46,524 INFO: Loss [L1Loss] is created.\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:07<00:00, 81.3MB/s]\n",
            "2025-09-13 08:02:55,588 INFO: Loss [PerceptualLoss] is created.\n",
            "2025-09-13 08:02:55,613 INFO: Loss [GANLoss] is created.\n",
            "2025-09-13 08:02:55,656 INFO: Model [RealHATGANModel] is created.\n",
            "2025-09-13 08:02:55,676 INFO: Start training from epoch: 0, iter: 0\n",
            "2025-09-13 08:04:48,988 INFO: [train..][epoch:  0, iter:     100, lr:(2.000e-05,)] [eta: 1:23:28, time (data): 1.133 (0.005)] l_g_pix: 9.8960e-02 l_g_percep: 1.2843e+01 l_g_gan: 7.1950e-02 l_d_real: 6.8709e-01 out_d_real: 2.5404e-02 l_d_fake: 6.7085e-01 out_d_fake: -4.8659e-02 \n",
            "2025-09-13 08:07:02,751 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2074\tBest: 19.2074 @ 100 iter\n",
            "\t # ssim: 0.4664\tBest: 0.4664 @ 100 iter\n",
            "\t # niqe: 6.5079\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 08:08:50,078 INFO: [train..][epoch:  0, iter:     200, lr:(2.000e-05,)] [eta: 2:17:01, time (data): 1.103 (0.004)] l_g_pix: 1.1940e-01 l_g_percep: 1.0875e+01 l_g_gan: 8.4132e-02 l_d_real: 4.0005e-01 out_d_real: 8.4933e-01 l_d_fake: 6.1943e-01 out_d_fake: -2.2171e-01 \n",
            "2025-09-13 08:11:02,959 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2255\tBest: 19.2255 @ 200 iter\n",
            "\t # ssim: 0.4679\tBest: 0.4679 @ 200 iter\n",
            "\t # niqe: 6.4503\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 08:12:50,773 INFO: [train..][epoch:  0, iter:     300, lr:(2.000e-05,)] [eta: 2:32:13, time (data): 1.078 (0.004)] l_g_pix: 8.3212e-02 l_g_percep: 1.0394e+01 l_g_gan: 8.6798e-02 l_d_real: 7.2719e-01 out_d_real: -2.3636e-02 l_d_fake: 5.6479e-01 out_d_fake: -3.0323e-01 \n",
            "2025-09-13 08:15:04,110 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2452\tBest: 19.2452 @ 300 iter\n",
            "\t # ssim: 0.4696\tBest: 0.4696 @ 300 iter\n",
            "\t # niqe: 6.4101\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 08:16:52,192 INFO: [train..][epoch:  0, iter:     400, lr:(2.000e-05,)] [eta: 2:37:58, time (data): 1.080 (0.004)] l_g_pix: 8.4690e-02 l_g_percep: 1.1026e+01 l_g_gan: 9.9735e-02 l_d_real: 6.9722e-01 out_d_real: 1.3101e-01 l_d_fake: 5.1440e-01 out_d_fake: -4.8293e-01 \n",
            "2025-09-13 08:19:03,889 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2628\tBest: 19.2628 @ 400 iter\n",
            "\t # ssim: 0.4711\tBest: 0.4711 @ 400 iter\n",
            "\t # niqe: 6.3939\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 08:20:51,524 INFO: [train..][epoch:  0, iter:     500, lr:(2.000e-05,)] [eta: 2:39:30, time (data): 1.076 (0.004)] l_g_pix: 5.0256e-02 l_g_percep: 8.9854e+00 l_g_gan: 1.0072e-01 l_d_real: 9.8521e-01 out_d_real: -5.1218e-01 l_d_fake: 4.5948e-01 out_d_fake: -5.4771e-01 \n",
            "2025-09-13 08:20:51,525 INFO: Saving models and training states.\n",
            "2025-09-13 08:23:08,141 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2753\tBest: 19.2753 @ 500 iter\n",
            "\t # ssim: 0.4724\tBest: 0.4724 @ 500 iter\n",
            "\t # niqe: 6.3610\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 08:24:55,787 INFO: [train..][epoch:  0, iter:     600, lr:(2.000e-05,)] [eta: 2:39:48, time (data): 1.076 (0.004)] l_g_pix: 7.0661e-02 l_g_percep: 1.0265e+01 l_g_gan: 5.8019e-02 l_d_real: 4.6230e-01 out_d_real: 6.2671e-01 l_d_fake: 8.7264e-01 out_d_fake: 2.9242e-01 \n",
            "2025-09-13 08:27:08,463 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2817\tBest: 19.2817 @ 600 iter\n",
            "\t # ssim: 0.4735\tBest: 0.4735 @ 600 iter\n",
            "\t # niqe: 6.3272\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 08:28:56,211 INFO: [train..][epoch:  0, iter:     700, lr:(2.000e-05,)] [eta: 2:38:28, time (data): 1.078 (0.004)] l_g_pix: 8.5497e-02 l_g_percep: 1.0529e+01 l_g_gan: 1.1488e-01 l_d_real: 8.2900e-01 out_d_real: -1.7246e-01 l_d_fake: 4.0048e-01 out_d_fake: -7.4800e-01 \n",
            "2025-09-13 08:31:07,546 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2872\tBest: 19.2872 @ 700 iter\n",
            "\t # ssim: 0.4744\tBest: 0.4744 @ 700 iter\n",
            "\t # niqe: 6.3124\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 08:32:55,166 INFO: [train..][epoch:  0, iter:     800, lr:(2.000e-05,)] [eta: 2:36:20, time (data): 1.077 (0.004)] l_g_pix: 8.1033e-02 l_g_percep: 1.2546e+01 l_g_gan: 7.4883e-02 l_d_real: 5.5984e-01 out_d_real: 4.7788e-01 l_d_fake: 7.2476e-01 out_d_fake: -2.4198e-02 \n",
            "2025-09-13 08:35:08,102 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2902\tBest: 19.2902 @ 800 iter\n",
            "\t # ssim: 0.4752\tBest: 0.4752 @ 800 iter\n",
            "\t # niqe: 6.3136\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 08:36:55,783 INFO: [train..][epoch:  0, iter:     900, lr:(2.000e-05,)] [eta: 2:33:55, time (data): 1.077 (0.003)] l_g_pix: 9.7530e-02 l_g_percep: 1.0805e+01 l_g_gan: 7.7968e-02 l_d_real: 4.4631e-01 out_d_real: 8.9460e-01 l_d_fake: 6.8420e-01 out_d_fake: -9.5499e-02 \n",
            "2025-09-13 08:39:07,002 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2925\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4758\tBest: 0.4758 @ 900 iter\n",
            "\t # niqe: 6.2953\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 08:40:54,814 INFO: [train..][epoch:  0, iter:   1,000, lr:(2.000e-05,)] [eta: 2:31:04, time (data): 1.078 (0.003)] l_g_pix: 1.0978e-01 l_g_percep: 1.2386e+01 l_g_gan: 1.3905e-01 l_d_real: 7.1943e-01 out_d_real: 2.5522e-01 l_d_fake: 3.1171e-01 out_d_fake: -1.0786e+00 \n",
            "2025-09-13 08:40:54,815 INFO: Saving models and training states.\n",
            "2025-09-13 08:43:11,003 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2892\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4762\tBest: 0.4762 @ 1000 iter\n",
            "\t # niqe: 6.2807\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 08:44:59,051 INFO: [train..][epoch:  0, iter:   1,100, lr:(2.000e-05,)] [eta: 2:28:20, time (data): 1.081 (0.003)] l_g_pix: 5.9632e-02 l_g_percep: 9.4344e+00 l_g_gan: 9.7224e-02 l_d_real: 6.6141e-01 out_d_real: 1.7941e-01 l_d_fake: 4.9107e-01 out_d_fake: -4.8115e-01 \n",
            "2025-09-13 08:47:10,576 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2790\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4762\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.2516\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 08:48:59,912 INFO: [train..][epoch:  0, iter:   1,200, lr:(2.000e-05,)] [eta: 2:25:11, time (data): 1.087 (0.003)] l_g_pix: 9.6821e-02 l_g_percep: 9.6467e+00 l_g_gan: 8.1647e-02 l_d_real: 7.1501e-01 out_d_real: 1.7382e-01 l_d_fake: 9.2918e-01 out_d_fake: 1.1271e-01 \n",
            "2025-09-13 08:51:12,388 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2665\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4760\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.2222\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 08:53:01,512 INFO: [train..][epoch:  0, iter:   1,300, lr:(1.000e-05,)] [eta: 2:21:57, time (data): 1.091 (0.003)] l_g_pix: 8.3721e-02 l_g_percep: 1.1018e+01 l_g_gan: 1.3285e-01 l_d_real: 8.2376e-01 out_d_real: 2.2868e-02 l_d_fake: 3.6873e-01 out_d_fake: -9.5972e-01 \n",
            "2025-09-13 08:55:13,558 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2536\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4758\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.2036\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 08:57:02,849 INFO: [train..][epoch:  0, iter:   1,400, lr:(1.000e-05,)] [eta: 2:18:35, time (data): 1.092 (0.003)] l_g_pix: 1.0166e-01 l_g_percep: 1.3354e+01 l_g_gan: 1.3383e-01 l_d_real: 8.2820e-01 out_d_real: 7.2456e-02 l_d_fake: 3.5349e-01 out_d_fake: -9.8460e-01 \n",
            "2025-09-13 08:59:15,055 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2358\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4753\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1898\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 09:01:04,129 INFO: [train..][epoch:  0, iter:   1,500, lr:(1.000e-05,)] [eta: 2:15:08, time (data): 1.091 (0.003)] l_g_pix: 1.3713e-01 l_g_percep: 1.3189e+01 l_g_gan: 1.1729e-01 l_d_real: 3.9780e-01 out_d_real: 1.0459e+00 l_d_fake: 4.2151e-01 out_d_fake: -7.5139e-01 \n",
            "2025-09-13 09:01:04,130 INFO: Saving models and training states.\n",
            "2025-09-13 09:03:19,936 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2203\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4749\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1776\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 09:05:09,080 INFO: [train..][epoch:  0, iter:   1,600, lr:(1.000e-05,)] [eta: 2:11:44, time (data): 1.091 (0.003)] l_g_pix: 1.1605e-01 l_g_percep: 1.4640e+01 l_g_gan: 1.2894e-01 l_d_real: 6.4637e-01 out_d_real: 2.8517e-01 l_d_fake: 3.8946e-01 out_d_fake: -8.9995e-01 \n",
            "2025-09-13 09:07:22,001 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.2030\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4744\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1743\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 09:09:11,219 INFO: [train..][epoch:  0, iter:   1,700, lr:(1.000e-05,)] [eta: 2:08:10, time (data): 1.092 (0.003)] l_g_pix: 1.7180e-01 l_g_percep: 1.7477e+01 l_g_gan: 1.1508e-01 l_d_real: 3.3083e-01 out_d_real: 1.6308e+00 l_d_fake: 5.8865e-01 out_d_fake: -5.6205e-01 \n",
            "2025-09-13 09:11:24,086 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.1844\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4737\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1634\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 09:13:13,286 INFO: [train..][epoch:  0, iter:   1,800, lr:(1.000e-05,)] [eta: 2:04:33, time (data): 1.092 (0.003)] l_g_pix: 1.1725e-01 l_g_percep: 1.2670e+01 l_g_gan: 1.1243e-01 l_d_real: 4.3259e-01 out_d_real: 1.0828e+00 l_d_fake: 5.4800e-01 out_d_fake: -5.7632e-01 \n",
            "2025-09-13 09:15:26,000 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.1652\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4732\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1669\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 09:17:15,239 INFO: [train..][epoch:  0, iter:   1,900, lr:(1.000e-05,)] [eta: 2:00:53, time (data): 1.092 (0.004)] l_g_pix: 9.4221e-02 l_g_percep: 1.0741e+01 l_g_gan: 1.2700e-01 l_d_real: 8.1705e-01 out_d_real: -4.1353e-02 l_d_fake: 3.5612e-01 out_d_fake: -9.1386e-01 \n",
            "2025-09-13 09:19:26,492 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.1466\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4726\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1638\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 09:21:15,679 INFO: [train..][epoch:  0, iter:   2,000, lr:(1.000e-05,)] [eta: 1:57:09, time (data): 1.092 (0.004)] l_g_pix: 5.0311e-02 l_g_percep: 7.9803e+00 l_g_gan: 1.0962e-01 l_d_real: 7.0897e-01 out_d_real: 1.1227e-01 l_d_fake: 4.4519e-01 out_d_fake: -6.5105e-01 \n",
            "2025-09-13 09:21:15,680 INFO: Saving models and training states.\n",
            "2025-09-13 09:23:28,183 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.1297\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4721\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1644\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 09:25:17,426 INFO: [train..][epoch:  1, iter:   2,100, lr:(1.000e-05,)] [eta: 1:53:24, time (data): 1.091 (0.004)] l_g_pix: 8.2263e-02 l_g_percep: 1.1946e+01 l_g_gan: 1.1051e-01 l_d_real: 5.3953e-01 out_d_real: 7.8943e-01 l_d_fake: 5.4531e-01 out_d_fake: -5.5975e-01 \n",
            "2025-09-13 09:27:28,240 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.1116\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4715\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1661\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 09:29:17,682 INFO: [train..][epoch:  1, iter:   2,200, lr:(1.000e-05,)] [eta: 1:49:37, time (data): 1.093 (0.004)] l_g_pix: 8.9473e-02 l_g_percep: 9.7048e+00 l_g_gan: 7.4105e-02 l_d_real: 4.2099e-01 out_d_real: 9.7758e-01 l_d_fake: 8.4022e-01 out_d_fake: 9.9170e-02 \n",
            "2025-09-13 09:31:29,907 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.0955\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4707\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1562\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 09:33:18,979 INFO: [train..][epoch:  1, iter:   2,300, lr:(1.000e-05,)] [eta: 1:45:49, time (data): 1.091 (0.004)] l_g_pix: 1.2099e-01 l_g_percep: 1.6346e+01 l_g_gan: 1.2487e-01 l_d_real: 6.0098e-01 out_d_real: 8.3541e-01 l_d_fake: 4.7814e-01 out_d_fake: -7.7059e-01 \n",
            "2025-09-13 09:35:31,147 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.0831\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4700\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1524\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 09:37:20,371 INFO: [train..][epoch:  1, iter:   2,400, lr:(1.000e-05,)] [eta: 1:42:00, time (data): 1.091 (0.004)] l_g_pix: 1.3866e-01 l_g_percep: 1.6337e+01 l_g_gan: 1.0086e-01 l_d_real: 5.4819e-01 out_d_real: 1.0471e+00 l_d_fake: 6.4643e-01 out_d_fake: -3.6221e-01 \n",
            "2025-09-13 09:39:31,756 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.0633\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4690\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1445\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 09:41:20,860 INFO: [train..][epoch:  1, iter:   2,500, lr:(1.000e-05,)] [eta: 1:38:10, time (data): 1.091 (0.004)] l_g_pix: 1.2175e-01 l_g_percep: 1.5154e+01 l_g_gan: 1.0501e-01 l_d_real: 4.1801e-01 out_d_real: 1.1949e+00 l_d_fake: 5.8492e-01 out_d_fake: -4.6518e-01 \n",
            "2025-09-13 09:41:20,861 INFO: Saving models and training states.\n",
            "2025-09-13 09:43:33,735 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.0484\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4683\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1523\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 09:45:22,900 INFO: [train..][epoch:  1, iter:   2,600, lr:(5.000e-06,)] [eta: 1:34:20, time (data): 1.091 (0.004)] l_g_pix: 1.1367e-01 l_g_percep: 1.2253e+01 l_g_gan: 1.0622e-01 l_d_real: 4.2552e-01 out_d_real: 9.1188e-01 l_d_fake: 5.1242e-01 out_d_fake: -5.4976e-01 \n",
            "2025-09-13 09:47:34,611 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.0367\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4677\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1567\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 09:49:23,755 INFO: [train..][epoch:  1, iter:   2,700, lr:(5.000e-06,)] [eta: 1:30:28, time (data): 1.091 (0.004)] l_g_pix: 1.0577e-01 l_g_percep: 1.0676e+01 l_g_gan: 8.7731e-02 l_d_real: 3.5840e-01 out_d_real: 1.0108e+00 l_d_fake: 5.8178e-01 out_d_fake: -2.9553e-01 \n",
            "2025-09-13 09:51:36,245 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.0219\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4670\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1566\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 09:53:25,571 INFO: [train..][epoch:  1, iter:   2,800, lr:(5.000e-06,)] [eta: 1:26:37, time (data): 1.092 (0.004)] l_g_pix: 5.0425e-02 l_g_percep: 8.0552e+00 l_g_gan: 1.3855e-01 l_d_real: 8.2469e-01 out_d_real: -1.7511e-01 l_d_fake: 2.9855e-01 out_d_fake: -1.0869e+00 \n",
            "2025-09-13 09:55:38,284 INFO: Validation val_sat_images\n",
            "\t # psnr: 19.0077\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4664\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1574\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 09:57:27,517 INFO: [train..][epoch:  1, iter:   2,900, lr:(5.000e-06,)] [eta: 1:22:44, time (data): 1.092 (0.004)] l_g_pix: 8.9567e-02 l_g_percep: 1.1405e+01 l_g_gan: 1.5032e-01 l_d_real: 8.5406e-01 out_d_real: 4.8946e-02 l_d_fake: 3.1170e-01 out_d_fake: -1.1914e+00 \n",
            "2025-09-13 09:59:38,912 INFO: Validation val_sat_images\n",
            "\t # psnr: 18.9929\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4657\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1628\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 10:01:28,018 INFO: [train..][epoch:  1, iter:   3,000, lr:(5.000e-06,)] [eta: 1:18:50, time (data): 1.091 (0.004)] l_g_pix: 9.3547e-02 l_g_percep: 9.7194e+00 l_g_gan: 9.1439e-02 l_d_real: 2.1744e-01 out_d_real: 1.6457e+00 l_d_fake: 5.9096e-01 out_d_fake: -3.2343e-01 \n",
            "2025-09-13 10:01:28,019 INFO: Saving models and training states.\n",
            "2025-09-13 10:03:41,415 INFO: Validation val_sat_images\n",
            "\t # psnr: 18.9839\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4653\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1676\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 10:05:30,605 INFO: [train..][epoch:  1, iter:   3,100, lr:(5.000e-06,)] [eta: 1:14:57, time (data): 1.092 (0.004)] l_g_pix: 1.4992e-01 l_g_percep: 1.3978e+01 l_g_gan: 1.1096e-01 l_d_real: 2.1767e-01 out_d_real: 1.9109e+00 l_d_fake: 6.1083e-01 out_d_fake: -4.9879e-01 \n",
            "2025-09-13 10:07:41,852 INFO: Validation val_sat_images\n",
            "\t # psnr: 18.9752\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4649\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1792\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 10:09:31,038 INFO: [train..][epoch:  1, iter:   3,200, lr:(5.000e-06,)] [eta: 1:11:03, time (data): 1.092 (0.004)] l_g_pix: 6.8007e-02 l_g_percep: 1.0939e+01 l_g_gan: 1.3275e-01 l_d_real: 6.5342e-01 out_d_real: 3.7734e-01 l_d_fake: 3.3430e-01 out_d_fake: -9.9316e-01 \n",
            "2025-09-13 10:11:42,637 INFO: Validation val_sat_images\n",
            "\t # psnr: 18.9645\tBest: 19.2925 @ 900 iter\n",
            "\t # ssim: 0.4643\tBest: 0.4762 @ 1100 iter\n",
            "\t # niqe: 6.1910\tBest: 6.5079 @ 100 iter\n",
            "\n",
            "2025-09-13 10:13:31,777 INFO: [train..][epoch:  1, iter:   3,300, lr:(5.000e-06,)] [eta: 1:07:08, time (data): 1.091 (0.004)] l_g_pix: 1.0429e-01 l_g_percep: 9.8492e+00 l_g_gan: 9.8128e-02 l_d_real: 4.4582e-01 out_d_real: 1.1542e+00 l_d_fake: 1.0407e+00 out_d_fake: 5.9415e-02 \n",
            "W0913 10:15:26.414000 1357 torch/distributed/elastic/agent/server/api.py:723] Received 2 death signal, shutting down workers\n",
            "W0913 10:15:26.417000 1357 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1368 closing signal SIGINT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/HAT/hat/train.py\", line 11, in <module>\n",
            "    train_pipeline(root_path)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/train.py\", line 193, in train_pipeline\n",
            "    model.validation(val_loader, current_iter, tb_logger, opt['val']['save_img'])\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/models/base_model.py\", line 48, in validation\n",
            "    self.nondist_validation(dataloader, current_iter, tb_logger, save_img)\n",
            "  File \"/content/HAT/hat/models/testrealhatgan_model.py\", line 188, in nondist_validation\n",
            "    super(RealHATGANModel, self).nondist_validation(dataloader, current_iter, tb_logger, save_img)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/models/sr_model.py\", line 188, in nondist_validation\n",
            "    self.metric_results[name] += calculate_metric(metric_data, opt_)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/metrics/__init__.py\", line 19, in calculate_metric\n",
            "    metric = METRIC_REGISTRY.get(metric_type)(**data, **opt)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/metrics/niqe.py\", line 195, in calculate_niqe\n",
            "    niqe_result = niqe(img, mu_pris_param, cov_pris_param, gaussian_window)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/metrics/niqe.py\", line 117, in niqe\n",
            "    feat.append(compute_feature(block))\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/metrics/niqe.py\", line 60, in compute_feature\n",
            "    shifted_block = np.roll(block, shifts[i], axis=(0, 1))\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/numpy/_core/numeric.py\", line 1274, in roll\n",
            "    result[res_index] = a[arr_index]\n",
            "    ~~~~~~^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mtrain_Real_HAT_GAN_SRx4_finetune_from_mse_model\u001b[0m at: \u001b[34mhttps://wandb.ai/dsha43925-middlesex-university-mauritius/HAT-satellite-SR/runs/etoh0lka\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250913_080239-etoh0lka/logs\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/torchrun\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('torch', 'console_scripts', 'torchrun')())\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 901, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
            "    result = agent.run()\n",
            "             ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py\", line 138, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 715, in run\n",
            "    result = self._invoke_run(role)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 879, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 1357 got signal: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where models are stored\n",
        "models_dir = \"/content/HAT/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/models\"\n",
        "\n",
        "# Model checkpoints you want\n",
        "checkpoints = [2500, 3000, 3500, 4000, 5000]\n",
        "\n",
        "# Path for the output zip\n",
        "zip_path = \"/content/selected_models.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"w\") as zipf:\n",
        "    for ckpt in checkpoints:\n",
        "        file_path = os.path.join(models_dir, f\"net_g_{ckpt}.pth\")\n",
        "        if os.path.exists(file_path):\n",
        "            zipf.write(file_path, arcname=f\"net_g_{ckpt}.pth\")\n",
        "        else:\n",
        "            print(f\"⚠️ Not found: {file_path}\")\n",
        "\n",
        "print(\"✅ Models zipped at:\", zip_path)\n",
        "\n",
        "# Download the zip\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "cKKnrGwqJU2-",
        "outputId": "6309445e-139b-43f5-a609-847c7161be6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Models zipped at: /content/selected_models.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5a3449b6-99cf-4c5a-b4c8-cf6179f40dfa\", \"selected_models.zip\", 1657810947)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Root directory where images are stored\n",
        "root_dir = \"/content/HAT/experiments/train_HAT-L_SRx4_finetune_from_ImageNet_pretrain/visualization\"\n",
        "\n",
        "# Images and checkpoints you want\n",
        "image_names = [\n",
        "    \"Jakarta_patch_359\",\n",
        "    \"pataya_patch_443\",\n",
        "    \"gujarat_patch_79\",\n",
        "    \"AthensGreece_patch_116\"\n",
        "]\n",
        "\n",
        "checkpoints = [100, 1000, 2000, 3000, 4000, 5000]\n",
        "\n",
        "# Path for the output zip\n",
        "zip_path = \"/content/selected_images.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"w\") as zipf:\n",
        "    for img_name in image_names:\n",
        "        for ckpt in checkpoints:\n",
        "            file_path = os.path.join(root_dir, img_name, f\"{img_name}_{ckpt}.png\")\n",
        "            if os.path.exists(file_path):\n",
        "                zipf.write(file_path, arcname=f\"{img_name}_{ckpt}.png\")\n",
        "            else:\n",
        "                print(f\"⚠️ Not found: {file_path}\")\n",
        "\n",
        "print(\"✅ Images zipped at:\", zip_path)\n",
        "\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "4UB_eak13fUg",
        "outputId": "c533b715-4d6d-4c1c-9093-113028812ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Images zipped at: /content/selected_images.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_10248cf2-e7fc-412c-b2f2-a1307ee5d3ac\", \"selected_images.zip\", 2623163)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6 Inference"
      ],
      "metadata": {
        "id": "sj-EIfVc4PXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Get zipfile containing low resolution image to test"
      ],
      "metadata": {
        "id": "epclaj6T4U6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload the zip file containing low resolution images for inference.\n",
        "code_zip_path = '/content/drive/MyDrive/Final_project/testImages.zip'\n",
        "unzip_path = '/content'\n",
        "!unzip -q $code_zip_path -d $unzip_path\n",
        "!ls -l $unzip_path"
      ],
      "metadata": {
        "id": "De7C5za-dHuj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ed3f34-3762-41c7-e175-5509877c850b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwx------ 5 root root 4096 Sep 14 13:15 drive\n",
            "drwxrwxrwx 8 root root 4096 Sep 14 13:16 HAT\n",
            "drwxr-xr-x 1 root root 4096 Sep  9 13:46 sample_data\n",
            "drwxrwxrwx 7 root root 4096 Sep 11 19:39 testImages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Create configuration file for inference"
      ],
      "metadata": {
        "id": "WPLsDv8E4crK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "replace with correct path in config:\n",
        "pretrain_network_g: '/content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth'"
      ],
      "metadata": {
        "id": "aa_dFF8bFtxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the YAML content as a string\n",
        "test_config_yml = \"\"\"\n",
        "name: HAT-L_SRx4_ImageNet-pretrain\n",
        "model_type: HATModel\n",
        "scale: 4\n",
        "num_gpu: 1\n",
        "manual_seed: 100\n",
        "\n",
        "tile:\n",
        "  tile_size: 64\n",
        "  tile_pad: 32\n",
        "\n",
        "datasets:\n",
        "  test_1:\n",
        "    name: customSatelliteIMages\n",
        "    type: SingleImageDataset\n",
        "    dataroot_lq: /content/testImages/water\n",
        "    io_backend:\n",
        "      type: disk\n",
        "\n",
        "# network structures\n",
        "network_g:\n",
        "  type: HAT\n",
        "  upscale: 4\n",
        "  in_chans: 3\n",
        "  img_size: 64\n",
        "  window_size: 16\n",
        "  compress_ratio: 3\n",
        "  squeeze_factor: 30\n",
        "  conv_scale: 0.01\n",
        "  overlap_ratio: 0.5\n",
        "  img_range: 1.\n",
        "  depths: [6, 6, 6, 6, 6, 6]\n",
        "  embed_dim: 180\n",
        "  num_heads: [6, 6, 6, 6, 6, 6]\n",
        "  mlp_ratio: 2\n",
        "  upsampler: 'pixelshuffle'\n",
        "  resi_connection: '1conv'\n",
        "\n",
        "# path\n",
        "path:\n",
        "  pretrain_network_g: '/content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth'\n",
        "  strict_load_g: true\n",
        "  param_key_g: 'params_ema'\n",
        "\n",
        "# validation settings\n",
        "val:\n",
        "  save_img: true\n",
        "  suffix: ~  # add suffix to saved images, if None, use exp name\n",
        "\"\"\"\n",
        "\n",
        "output_path = \"/content/HAT/options/test/HAT-L_inference.yml\"\n",
        "\n",
        "# Create directories not exist\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "# Save to yml\n",
        "with open(output_path, \"w\") as f:\n",
        "    f.write(test_config_yml)\n",
        "\n",
        "print(f\"YAML config saved to {output_path}\")\n"
      ],
      "metadata": {
        "id": "0IKAW87Ebjl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b622c15-fd86-49f2-d687-b8a2c43fc017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YAML config saved to /content/HAT/options/test/HAT-L_inference.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Run for inference"
      ],
      "metadata": {
        "id": "gjRpZkWu8JUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python hat/test.py -opt options/test/HAT-L_inference.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOtgnQaCaw0H",
        "outputId": "a56046ed-0874-4613-9569-be4808bfe778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disable distributed.\n",
            "Path already exists. Rename it to /content/HAT/results/HAT-L_SRx4_ImageNet-pretrain_archived_20250914_135101\n",
            "2025-09-14 13:51:01,096 INFO: \n",
            "                ____                _       _____  ____\n",
            "               / __ ) ____ _ _____ (_)_____/ ___/ / __ \\\n",
            "              / __  |/ __ `// ___// // ___/\\__ \\ / /_/ /\n",
            "             / /_/ // /_/ /(__  )/ // /__ ___/ // _, _/\n",
            "            /_____/ \\__,_//____//_/ \\___//____//_/ |_|\n",
            "     ______                   __   __                 __      __\n",
            "    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /\n",
            "   / / __ / __ \\ / __ \\ / __  /  / /   / / / // ___// //_/  / /\n",
            "  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/\n",
            "  \\____/ \\____/ \\____/ \\____/  /_____/\\____/ \\___//_/|_|  (_)\n",
            "    \n",
            "Version Information: \n",
            "\tBasicSR: 1.3.4.9\n",
            "\tPyTorch: 2.8.0+cu126\n",
            "\tTorchVision: 0.23.0+cu126\n",
            "2025-09-14 13:51:01,097 INFO: \n",
            "  name: HAT-L_SRx4_ImageNet-pretrain\n",
            "  model_type: HATModel\n",
            "  scale: 4\n",
            "  num_gpu: 1\n",
            "  manual_seed: 100\n",
            "  tile:[\n",
            "    tile_size: 64\n",
            "    tile_pad: 32\n",
            "  ]\n",
            "  datasets:[\n",
            "    test_1:[\n",
            "      name: customSatelliteIMages\n",
            "      type: SingleImageDataset\n",
            "      dataroot_lq: /content/testImages/water\n",
            "      io_backend:[\n",
            "        type: disk\n",
            "      ]\n",
            "      phase: test\n",
            "      scale: 4\n",
            "    ]\n",
            "  ]\n",
            "  network_g:[\n",
            "    type: HAT\n",
            "    upscale: 4\n",
            "    in_chans: 3\n",
            "    img_size: 64\n",
            "    window_size: 16\n",
            "    compress_ratio: 3\n",
            "    squeeze_factor: 30\n",
            "    conv_scale: 0.01\n",
            "    overlap_ratio: 0.5\n",
            "    img_range: 1.0\n",
            "    depths: [6, 6, 6, 6, 6, 6]\n",
            "    embed_dim: 180\n",
            "    num_heads: [6, 6, 6, 6, 6, 6]\n",
            "    mlp_ratio: 2\n",
            "    upsampler: pixelshuffle\n",
            "    resi_connection: 1conv\n",
            "  ]\n",
            "  path:[\n",
            "    pretrain_network_g: /content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth\n",
            "    strict_load_g: True\n",
            "    param_key_g: params_ema\n",
            "    results_root: /content/HAT/results/HAT-L_SRx4_ImageNet-pretrain\n",
            "    log: /content/HAT/results/HAT-L_SRx4_ImageNet-pretrain\n",
            "    visualization: /content/HAT/results/HAT-L_SRx4_ImageNet-pretrain/visualization\n",
            "  ]\n",
            "  val:[\n",
            "    save_img: True\n",
            "    suffix: None\n",
            "  ]\n",
            "  dist: False\n",
            "  rank: 0\n",
            "  world_size: 1\n",
            "  auto_resume: False\n",
            "  is_train: False\n",
            "\n",
            "2025-09-14 13:51:01,097 INFO: Dataset [SingleImageDataset] - customSatelliteIMages is built.\n",
            "2025-09-14 13:51:01,097 INFO: Number of test images in customSatelliteIMages: 51\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "2025-09-14 13:51:01,661 INFO: Network [HAT] is created.\n",
            "2025-09-14 13:51:01,915 INFO: Network: HAT, with parameters: 20,772,507\n",
            "2025-09-14 13:51:01,916 INFO: HAT(\n",
            "  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (patch_unembed): PatchUnEmbed()\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (layers): ModuleList(\n",
            "    (0): RHAG(\n",
            "      (residual_group): AttenBlocks(\n",
            "        (blocks): ModuleList(\n",
            "          (0): HAB(\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(180, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(60, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(180, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 180, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): Identity()\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1-5): 5 x HAB(\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(180, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(60, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(180, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 180, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (overlap_attn): OCAB(\n",
            "          (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "          (unfold): Unfold(kernel_size=(24, 24), dilation=1, padding=4, stride=16)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "          (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "    (1-5): 5 x RHAG(\n",
            "      (residual_group): AttenBlocks(\n",
            "        (blocks): ModuleList(\n",
            "          (0-5): 6 x HAB(\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(180, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(60, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(180, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 180, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (overlap_attn): OCAB(\n",
            "          (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "          (unfold): Unfold(kernel_size=(24, 24), dilation=1, padding=4, stride=16)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "          (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_before_upsample): Sequential(\n",
            "    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "  )\n",
            "  (upsample): Upsample(\n",
            "    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): PixelShuffle(upscale_factor=2)\n",
            "    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): PixelShuffle(upscale_factor=2)\n",
            "  )\n",
            "  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n",
            "2025-09-14 13:51:02,303 INFO: Loading HAT model from /content/HAT/experiments/pretrained_models/Real_HAT_GAN_SRx4.pth, with param key: [params_ema].\n",
            "2025-09-14 13:51:02,475 INFO: Model [HATModel] is created.\n",
            "2025-09-14 13:51:02,476 INFO: Testing customSatelliteIMages...\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n",
            "\tTile 1/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download the results\n",
        "zip_path = \"/content/hat/NotFineTuned/results_water.zip\"\n",
        "folder_to_zip = \"/content/HAT/results/HAT-L_SRx4_ImageNet-pretrain/visualization/customSatelliteIMages\"\n",
        "shutil.make_archive(zip_path.replace(\".zip\",\"\"), 'zip', folder_to_zip)\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "id": "Rwe6VVXHcL_a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "0943389e-6c90-403b-8dd4-c78fa4d43d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b1fc0efa-a396-49f2-9eda-0da1bad98431\", \"results_water.zip\", 4729493)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qxKE7kz4AXvZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}