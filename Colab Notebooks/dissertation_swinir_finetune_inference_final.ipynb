{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lcyjG-dZr86Q"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Readme"
      ],
      "metadata": {
        "id": "FSNfPjKhUNVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-requisites\n",
        "*   Set up weight and bias account: [Wandb](https://wandb.ai/site/)\n",
        "*   Upload dataset on drive following the hierarchy below: <br>\n",
        "<pre>\n",
        "MyDrive\n",
        "|_ Final_project\n",
        "   |_ 2000dataset.zip\n",
        "</pre>\n",
        "\n",
        "*   Upload the zip containing the code in drive following the heirarchy below:\n",
        "<pre>\n",
        "MyDrive\n",
        "|_ Final_project\n",
        "   |_ SwinIR\n",
        "      |_ SwinIR.zip\n",
        "</pre>\n",
        "\n",
        "*   for **inference** using generator obtained after fine tunig on the dataset, ensure 5000_G is at the hierarchy below:\n",
        "\n",
        " <pre>\n",
        "MyDrive\n",
        "|_ Final_project\n",
        "   |_ SwinIR\n",
        "      |_ 5000_G.pth\n",
        "</pre>\n",
        "\n",
        "\n",
        "**Fine-Tuning**: Follow steps 1 to 4 in order. <br> <br>\n",
        "**Inference**:\n",
        "*   The SwinIR code from Kair repository and the generator is required for inference. Generator can be the one obtained after fine tuning with _G.pth or the original pretrained weight avilable at [JingyunLiang/SwinIR.git](http://github.com/JingyunLiang/SwinIR/releases/tag/v0.0)\n",
        "*   Run step 1 and Step 5. *Note: At step 5.2, replace the image path with correct image path or folder path.*"
      ],
      "metadata": {
        "id": "MBeeMks8UO_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Install dependencies and upload SwinIR code"
      ],
      "metadata": {
        "id": "Xk28WyGOrv1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install basicsr\n",
        "!pip install wandb -qqq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYJ4q8o8f2w_",
        "outputId": "f5f53203-16fe-480b-d114-4a4ee18c2ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting basicsr\n",
            "  Downloading basicsr-1.4.2.tar.gz (172 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/172.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting addict (from basicsr)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from basicsr) (1.0.0)\n",
            "Collecting lmdb (from basicsr)\n",
            "  Downloading lmdb-1.7.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from basicsr) (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from basicsr) (4.12.0.88)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from basicsr) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from basicsr) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from basicsr) (2.32.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from basicsr) (0.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from basicsr) (1.16.1)\n",
            "Collecting tb-nightly (from basicsr)\n",
            "  Downloading tb_nightly-2.21.0a20250914-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.12/dist-packages (from basicsr) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from basicsr) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from basicsr) (4.67.1)\n",
            "Collecting yapf (from basicsr)\n",
            "  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->basicsr) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr) (2025.8.3)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr) (2025.8.28)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr) (0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr) (1.74.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr) (3.9)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr) (5.29.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr) (3.1.3)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.12/dist-packages (from yapf->basicsr) (4.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.7->basicsr) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tb-nightly->basicsr) (3.0.2)\n",
            "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading lmdb-1.7.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.0/303.0 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tb_nightly-2.21.0a20250914-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yapf-0.43.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: basicsr\n",
            "  Building wheel for basicsr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for basicsr: filename=basicsr-1.4.2-py3-none-any.whl size=214817 sha256=35b3e3a4cd0129987966af250445382e8ff5211230e53f7f439b9fcecfdb7f25\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/e3/e4/58f29bfabb622dd40b6d9839318ce5bf092062b81ca3aa19ea\n",
            "Successfully built basicsr\n",
            "Installing collected packages: lmdb, addict, yapf, tb-nightly, basicsr\n",
            "Successfully installed addict-2.4.0 basicsr-1.4.2 lmdb-1.7.3 tb-nightly-2.21.0a20250914 yapf-0.43.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#basicssr contains a file degradation.py where the import library is obsolete\n",
        "#and has to be replaced\n",
        "#https://github.com/xinntao/Real-ESRGAN/issues/801\n",
        "\n",
        "file_path = \"/usr/local/lib/python3.12/dist-packages/basicsr/data/degradations.py\"\n",
        "line_to_replace = \"from torchvision.transforms.functional_tensor import rgb_to_grayscale\"\n",
        "new_line = \"from torchvision.transforms.functional import rgb_to_grayscale\"\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "lines = [new_line if line.strip() == line_to_replace else line for line in lines]\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    file.writelines(lines)\n",
        "\n",
        "print(f\"Replacing line '{line_to_replace}' with '{new_line}' in file {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWiNMUqS7UdL",
        "outputId": "efe5837d-487b-4d59-cd05-fa15025612e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replacing line 'from torchvision.transforms.functional_tensor import rgb_to_grayscale' with 'from torchvision.transforms.functional import rgb_to_grayscale' in file /usr/local/lib/python3.12/dist-packages/basicsr/data/degradations.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files, drive\n",
        "import zipfile\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import shutil\n",
        "import wandb\n",
        "import basicsr"
      ],
      "metadata": {
        "id": "IUs75bMkr6A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log in to Weights & Biases; not needed of doing inference only\n",
        "#\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "_de6TJ7qXoSF",
        "outputId": "98074a22-35d2-44c7-da28-fd4515b9221c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdsha43925\u001b[0m (\u001b[33mdsha43925-middlesex-university-mauritius\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get code from drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "code_zip_path = '/content/drive/MyDrive/Final_project/SwinIR/SwinIR.zip'\n",
        "unzip_path = '/content'\n",
        "!unzip -q $code_zip_path -d $unzip_path\n",
        "!ls -l $unzip_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvKBKrSZ_LJK",
        "outputId": "8eb33f6e-e9db-4ebb-844d-0247d677aa1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "total 12\n",
            "drwx------ 5 root root 4096 Sep 14 17:09 drive\n",
            "drwxrwxrwx 8 root root 4096 Sep 13 17:33 KAIR\n",
            "drwxr-xr-x 1 root root 4096 Sep  9 13:46 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/KAIR\n",
        "!pip install sewar\n",
        "!pip install -r /content/KAIR/requirement.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KqXTO-Wr_R_",
        "outputId": "15d9e933-23ae-42be-b4e2-00669c8a9e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KAIR\n",
            "Collecting sewar\n",
            "  Downloading sewar-0.4.6.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from sewar) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sewar) (1.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sewar) (11.3.0)\n",
            "Building wheels for collected packages: sewar\n",
            "  Building wheel for sewar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sewar: filename=sewar-0.4.6-py3-none-any.whl size=11418 sha256=5d14d2cd9ccd08e61232e3bd878098cc3169dd6eace4d39681e4858f11722892\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/4e/29/b15a3d425c5f0fe8f461cbfdaf4fa98ef203fed97ce1df6695\n",
            "Successfully built sewar\n",
            "Installing collected packages: sewar\n",
            "Successfully installed sewar-0.4.6\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from -r /content/KAIR/requirement.txt (line 1)) (4.12.0.88)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from -r /content/KAIR/requirement.txt (line 2)) (0.25.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from -r /content/KAIR/requirement.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r /content/KAIR/requirement.txt (line 4)) (0.23.0+cu126)\n",
            "Collecting hdf5storage (from -r /content/KAIR/requirement.txt (line 5))\n",
            "  Downloading hdf5storage-0.2.1-py3-none-any.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from -r /content/KAIR/requirement.txt (line 6))\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.12/dist-packages (from -r /content/KAIR/requirement.txt (line 7)) (1.7.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from -r /content/KAIR/requirement.txt (line 8)) (2.32.4)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from -r /content/KAIR/requirement.txt (line 9)) (1.0.19)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from -r /content/KAIR/requirement.txt (line 10)) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python->-r /content/KAIR/requirement.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->-r /content/KAIR/requirement.txt (line 2)) (1.16.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->-r /content/KAIR/requirement.txt (line 2)) (3.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->-r /content/KAIR/requirement.txt (line 2)) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->-r /content/KAIR/requirement.txt (line 2)) (2025.8.28)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->-r /content/KAIR/requirement.txt (line 2)) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->-r /content/KAIR/requirement.txt (line 2)) (0.4)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->-r /content/KAIR/requirement.txt (line 4)) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (3.4.0)\n",
            "Requirement already satisfied: h5py>=3.9 in /usr/local/lib/python3.12/dist-packages (from hdf5storage->-r /content/KAIR/requirement.txt (line 5)) (3.14.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->-r /content/KAIR/requirement.txt (line 8)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->-r /content/KAIR/requirement.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->-r /content/KAIR/requirement.txt (line 8)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->-r /content/KAIR/requirement.txt (line 8)) (2025.8.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm->-r /content/KAIR/requirement.txt (line 9)) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm->-r /content/KAIR/requirement.txt (line 9)) (0.34.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm->-r /content/KAIR/requirement.txt (line 9)) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm->-r /content/KAIR/requirement.txt (line 9)) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm->-r /content/KAIR/requirement.txt (line 9)) (1.1.9)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchvision->-r /content/KAIR/requirement.txt (line 4)) (3.0.2)\n",
            "Downloading hdf5storage-0.2.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.8/75.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja, hdf5storage\n",
            "Successfully installed hdf5storage-0.2.1 ninja-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Get pre-trained models"
      ],
      "metadata": {
        "id": "QEoN6RGMAJuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "following authors guide for fine tuning at https://github.com/cszn/KAIR/issues/94"
      ],
      "metadata": {
        "id": "xwwIMUKMRrKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN-with-dict-keys-params-and-params_ema.pth -O /content/KAIR/model_zoo/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN-with-dict-keys-params-and-params_ema.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0y7a-qjtBmw",
        "outputId": "44e90608-8ff9-474c-b771-90d87b410901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-14 17:10:05--  https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN-with-dict-keys-params-and-params_ema.pth\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/396770997/ad377089-20a9-4d77-87fd-1d85be80e1b4?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-14T18%3A05%3A03Z&rscd=attachment%3B+filename%3D003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN-with-dict-keys-params-and-params_ema.pth&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-14T17%3A04%3A56Z&ske=2025-09-14T18%3A05%3A03Z&sks=b&skv=2018-11-09&sig=MN504sTC3Lv8ZFF6Ewa5aAtTESllr6PGsjPMZGhhK08%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1Nzg3MDEwNSwibmJmIjoxNzU3ODY5ODA1LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.qgSlEtxzPfYrHFCiKtIZdtvD6Ru4J2HoMZZQZsS3dhk&response-content-disposition=attachment%3B%20filename%3D003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN-with-dict-keys-params-and-params_ema.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-09-14 17:10:06--  https://release-assets.githubusercontent.com/github-production-release-asset/396770997/ad377089-20a9-4d77-87fd-1d85be80e1b4?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-14T18%3A05%3A03Z&rscd=attachment%3B+filename%3D003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN-with-dict-keys-params-and-params_ema.pth&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-14T17%3A04%3A56Z&ske=2025-09-14T18%3A05%3A03Z&sks=b&skv=2018-11-09&sig=MN504sTC3Lv8ZFF6Ewa5aAtTESllr6PGsjPMZGhhK08%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1Nzg3MDEwNSwibmJmIjoxNzU3ODY5ODA1LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.qgSlEtxzPfYrHFCiKtIZdtvD6Ru4J2HoMZZQZsS3dhk&response-content-disposition=attachment%3B%20filename%3D003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN-with-dict-keys-params-and-params_ema.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 134262205 (128M) [application/octet-stream]\n",
            "Saving to: ‘/content/KAIR/model_zoo/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN-with-dict-keys-params-and-params_ema.pth’\n",
            "\n",
            "/content/KAIR/model 100%[===================>] 128.04M  25.2MB/s    in 5.1s    \n",
            "\n",
            "2025-09-14 17:10:11 (25.2 MB/s) - ‘/content/KAIR/model_zoo/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN-with-dict-keys-params-and-params_ema.pth’ saved [134262205/134262205]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTANT! Do not run for inference. use for training only!\n",
        "#This step is required as network will fine-tune from pre-trained models only if they are in the the models folder from superresolution\n",
        "#They also need to be renamed to 1_G and 1_E as the model understands it like a checkpoint saved from previous exec.\n",
        "\n",
        "# source and destination paths\n",
        "source_path = \"/content/KAIR/model_zoo/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN-with-dict-keys-params-and-params_ema.pth\"\n",
        "destination_dir = \"/content/KAIR/superresolution/swinir_sr_realworld_x4_gan/models/\"\n",
        "\n",
        "# Create destination directory if not exist\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Copy and rename to 1_G.pth\n",
        "destination_g = os.path.join(destination_dir, \"1_G.pth\")\n",
        "shutil.copy2(source_path, destination_g)\n",
        "print(f\"Copied to: {destination_g}\")\n",
        "\n",
        "# Copy and rename to 1_E.pth\n",
        "destination_e = os.path.join(destination_dir, \"1_E.pth\")\n",
        "shutil.copy2(source_path, destination_e)\n",
        "print(f\"Copied to: {destination_e}\")\n",
        "\n",
        "# Verify the files were created\n",
        "if os.path.exists(destination_g) and os.path.exists(destination_e):\n",
        "    print(\"Both files created successfully.\")\n",
        "    print(f\"1_G.pth size: {os.path.getsize(destination_g)} bytes\")\n",
        "    print(f\"1_E.pth size: {os.path.getsize(destination_e)} bytes\")\n",
        "else:\n",
        "    print(\"Files were not created properly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCZasZX_LP8C",
        "outputId": "55641ceb-bf54-4d5f-e0bd-22242122ffc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied to: /content/KAIR/superresolution/swinir_sr_realworld_x4_gan/models/1_G.pth\n",
            "Copied to: /content/KAIR/superresolution/swinir_sr_realworld_x4_gan/models/1_E.pth\n",
            "Both files created successfully.\n",
            "1_G.pth size: 134262205 bytes\n",
            "1_E.pth size: 134262205 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Upload dataset from Drive"
      ],
      "metadata": {
        "id": "IPmsyRikAGKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_zip_path = '/content/drive/MyDrive/Final_project/2000dataset.zip'\n",
        "!unzip -q $dataset_zip_path -d $unzip_path\n",
        "\n",
        "print(f\"Dataset extracted to: {unzip_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nll2HWYr3LFR",
        "outputId": "badf3c08-cd4c-40f1-8eed-b71cb6f616b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted to: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Fine tune swinIR GAN model"
      ],
      "metadata": {
        "id": "usvKtGBcXt5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the JSON content\n",
        "json_content = {\n",
        "    \"task\": \"swinir_sr_realworld_x4_gan\",\n",
        "    \"model\": \"gan\",\n",
        "    \"gpu_ids\": [0],\n",
        "    \"dist\": False,\n",
        "    \"scale\": 4,\n",
        "    \"n_channels\": 3,\n",
        "    \"path\": {\n",
        "        \"root\": \"superresolution\",\n",
        "        \"pretrained_netG\": \"/content/KAIR/superresolution/swinir_sr_realworld_x4_gan/models/1_G.pth\",\n",
        "        #\"pretrained_netD\": null,\n",
        "        \"pretrained_netE\": \"/content/KAIR/superresolution/swinir_sr_realworld_x4_gan/models/1_E.pth\"\n",
        "    },\n",
        "    \"datasets\": {\n",
        "        \"train\": {\n",
        "            \"name\": \"train_dataset\",\n",
        "            \"dataset_type\": \"sr\",\n",
        "            \"dataroot_H\": \"/content/dataset/train/hr\",\n",
        "            \"dataroot_L\": \"/content/dataset/train/lr\",\n",
        "            \"H_size\": 256,\n",
        "            \"shuffle_prob\": 0.1,\n",
        "            \"lq_patchsize\": 64,\n",
        "            \"use_sharp\": False,\n",
        "            \"dataloader_shuffle\": True,\n",
        "            \"dataloader_num_workers\": 2,\n",
        "            \"dataloader_batch_size\": 2\n",
        "        },\n",
        "        \"test\": {\n",
        "            \"name\": \"test_dataset\",\n",
        "            \"dataset_type\": \"sr\",\n",
        "            \"dataroot_H\": \"/content/dataset/val/hr\",\n",
        "            \"dataroot_L\": \"/content/dataset/val/lr\"\n",
        "        }\n",
        "    },\n",
        "    \"netG\": {\n",
        "        \"net_type\": \"swinir\",\n",
        "        \"upscale\": 4,\n",
        "        \"in_chans\": 3,\n",
        "        \"img_size\": 64,\n",
        "        \"window_size\": 8,\n",
        "        \"img_range\": 1.0,\n",
        "        \"depths\": [6, 6, 6, 6, 6, 6,],\n",
        "        \"embed_dim\": 180,\n",
        "        \"num_heads\": [6, 6, 6, 6, 6, 6],\n",
        "        \"mlp_ratio\": 2,\n",
        "        \"upsampler\": \"nearest+conv\",\n",
        "        \"resi_connection\": \"1conv\",\n",
        "        \"init_type\": \"default\"\n",
        "    },\n",
        "    \"netD\": {\n",
        "      \"net_type\": \"discriminator_unet\",\n",
        "      \"in_nc\": 3,\n",
        "      \"base_nc\": 64,\n",
        "      \"n_layers\": 3,\n",
        "      \"norm_type\": \"spectral\",\n",
        "      \"init_type\": \"orthogonal\",\n",
        "      \"init_bn_type\": \"uniform\",\n",
        "      \"init_gain\": 0.2,\n",
        "  }\n",
        "    , \"train\": {\n",
        "    \"G_lossfn_type\": \"l1\"\n",
        "    , \"G_lossfn_weight\": 1\n",
        "\n",
        "    , \"F_lossfn_type\": \"l1\"\n",
        "    , \"F_lossfn_weight\": 1\n",
        "    , \"F_feature_layer\": [2,7,16,25,34]\n",
        "    , \"F_weights\": [0.1,0.1,1.0,1.0,1.0]\n",
        "    , \"F_use_input_norm\": True\n",
        "    , \"F_use_range_norm\": False\n",
        "\n",
        "    , \"gan_type\": \"gan\"\n",
        "    , \"D_lossfn_weight\": 0.1\n",
        "\n",
        "    , \"E_decay\": 0.999\n",
        "\n",
        "    , \"D_init_iters\": 0\n",
        "\n",
        "    , \"G_optimizer_type\": \"adam\"\n",
        "    , \"G_optimizer_lr\": 1e-5\n",
        "    , \"G_optimizer_wd\": 0\n",
        "\n",
        "    , \"D_optimizer_type\": \"adam\"\n",
        "    , \"D_optimizer_lr\": 1e-4\n",
        "    , \"D_optimizer_wd\": 0\n",
        "\n",
        "    , \"G_scheduler_type\": \"MultiStepLR\"\n",
        "    , \"G_scheduler_milestones\": [3800]\n",
        "    , \"G_scheduler_gamma\": 0.5\n",
        "    , \"G_optimizer_reuse\": True\n",
        "\n",
        "    , \"D_scheduler_type\": \"MultiStepLR\"\n",
        "    , \"D_scheduler_milestones\": [3800]\n",
        "    , \"D_scheduler_gamma\": 0.5\n",
        "    , \"D_optimizer_reuse\": False\n",
        "\n",
        "    , \"G_param_strict\": True\n",
        "    , \"D_param_strict\": True\n",
        "    , \"E_param_strict\": True\n",
        "\n",
        "    , \"checkpoint_test\": 100\n",
        "    , \"checkpoint_save\": 500\n",
        "    , \"checkpoint_print\": 100\n",
        "    , \"manual_seed\": 134\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "# Ensure directory exists\n",
        "os.makedirs('/content/KAIR/options/swinir', exist_ok=True)\n",
        "\n",
        "# Write to new file\n",
        "new_json_path = '/content/KAIR/options/swinir/train_swinir_sr_realworld_x4_gan_satellite.json'\n",
        "with open(new_json_path, 'w') as f:\n",
        "    json.dump(json_content, f, indent=2)\n",
        "\n",
        "print(f\"New JSON created at: {new_json_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A9O4sxUB8XX",
        "outputId": "bb6921b1-b696-4177-ecea-32e19e886a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New JSON created at: /content/KAIR/options/swinir/train_swinir_sr_realworld_x4_gan_satellite.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/KAIR\n",
        "!python main_train_psnr.py --opt options/swinir/train_swinir_sr_realworld_x4_gan_satellite.json --wandb_project SwinIR --wandb_name SwinIR_finetune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA2fixqCuArz",
        "outputId": "1fc7c98d-6193-4328-9811-af4eb2208bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KAIR\n",
            "export CUDA_VISIBLE_DEVICES=0\n",
            "number of GPUs is: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdsha43925\u001b[0m (\u001b[33mdsha43925-middlesex-university-mauritius\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/KAIR/wandb/run-20250914_185648-h98lkbp4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mSwinIR_finetune\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsha43925-middlesex-university-mauritius/SwinIR\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsha43925-middlesex-university-mauritius/SwinIR/runs/h98lkbp4\u001b[0m\n",
            "LogHandlers setup!\n",
            "25-09-14 18:56:49.462 :   task: swinir_sr_realworld_x4_gan\n",
            "  model: gan\n",
            "  gpu_ids: [0]\n",
            "  dist: False\n",
            "  scale: 4\n",
            "  n_channels: 3\n",
            "  path:[\n",
            "    root: superresolution\n",
            "    pretrained_netG: superresolution/swinir_sr_realworld_x4_gan/models/2000_G.pth\n",
            "    pretrained_netE: superresolution/swinir_sr_realworld_x4_gan/models/2000_E.pth\n",
            "    task: superresolution/swinir_sr_realworld_x4_gan\n",
            "    log: superresolution/swinir_sr_realworld_x4_gan\n",
            "    options: superresolution/swinir_sr_realworld_x4_gan/options\n",
            "    models: superresolution/swinir_sr_realworld_x4_gan/models\n",
            "    images: superresolution/swinir_sr_realworld_x4_gan/images\n",
            "    pretrained_optimizerG: superresolution/swinir_sr_realworld_x4_gan/models/2000_optimizerG.pth\n",
            "  ]\n",
            "  datasets:[\n",
            "    train:[\n",
            "      name: train_dataset\n",
            "      dataset_type: sr\n",
            "      dataroot_H: /content/dataset/train/hr\n",
            "      dataroot_L: /content/dataset/train/lr\n",
            "      H_size: 256\n",
            "      shuffle_prob: 0.1\n",
            "      lq_patchsize: 64\n",
            "      use_sharp: False\n",
            "      dataloader_shuffle: True\n",
            "      dataloader_num_workers: 2\n",
            "      dataloader_batch_size: 2\n",
            "      phase: train\n",
            "      scale: 4\n",
            "      n_channels: 3\n",
            "    ]\n",
            "    test:[\n",
            "      name: test_dataset\n",
            "      dataset_type: sr\n",
            "      dataroot_H: /content/dataset/val/hr\n",
            "      dataroot_L: /content/dataset/val/lr\n",
            "      phase: test\n",
            "      scale: 4\n",
            "      n_channels: 3\n",
            "    ]\n",
            "  ]\n",
            "  netG:[\n",
            "    net_type: swinir\n",
            "    upscale: 4\n",
            "    in_chans: 3\n",
            "    img_size: 64\n",
            "    window_size: 8\n",
            "    img_range: 1.0\n",
            "    depths: [6, 6, 6, 6, 6, 6]\n",
            "    embed_dim: 180\n",
            "    num_heads: [6, 6, 6, 6, 6, 6]\n",
            "    mlp_ratio: 2\n",
            "    upsampler: nearest+conv\n",
            "    resi_connection: 1conv\n",
            "    init_type: default\n",
            "    scale: 4\n",
            "  ]\n",
            "  netD:[\n",
            "    net_type: discriminator_unet\n",
            "    in_nc: 3\n",
            "    base_nc: 64\n",
            "    n_layers: 3\n",
            "    norm_type: spectral\n",
            "    init_type: orthogonal\n",
            "    init_bn_type: uniform\n",
            "    init_gain: 0.2\n",
            "  ]\n",
            "  train:[\n",
            "    G_lossfn_type: l1\n",
            "    G_lossfn_weight: 1\n",
            "    F_lossfn_type: l1\n",
            "    F_lossfn_weight: 1\n",
            "    F_feature_layer: [2, 7, 16, 25, 34]\n",
            "    F_weights: [0.1, 0.1, 1.0, 1.0, 1.0]\n",
            "    F_use_input_norm: True\n",
            "    F_use_range_norm: False\n",
            "    gan_type: gan\n",
            "    D_lossfn_weight: 0.1\n",
            "    E_decay: 0.999\n",
            "    D_init_iters: 0\n",
            "    G_optimizer_type: adam\n",
            "    G_optimizer_lr: 1e-05\n",
            "    G_optimizer_wd: 0\n",
            "    D_optimizer_type: adam\n",
            "    D_optimizer_lr: 0.0001\n",
            "    D_optimizer_wd: 0\n",
            "    G_scheduler_type: MultiStepLR\n",
            "    G_scheduler_milestones: [3800]\n",
            "    G_scheduler_gamma: 0.5\n",
            "    G_optimizer_reuse: True\n",
            "    D_scheduler_type: MultiStepLR\n",
            "    D_scheduler_milestones: [3800]\n",
            "    D_scheduler_gamma: 0.5\n",
            "    D_optimizer_reuse: False\n",
            "    G_param_strict: True\n",
            "    D_param_strict: True\n",
            "    E_param_strict: True\n",
            "    checkpoint_test: 100\n",
            "    checkpoint_save: 500\n",
            "    checkpoint_print: 100\n",
            "    manual_seed: 134\n",
            "    G_optimizer_betas: [0.9, 0.999]\n",
            "    G_scheduler_restart_weights: 1\n",
            "  ]\n",
            "  opt_path: options/swinir/train_swinir_sr_realworld_x4_gan_satellite.json\n",
            "  is_train: True\n",
            "  merge_bn: False\n",
            "  merge_bn_startpoint: -1\n",
            "  find_unused_parameters: False\n",
            "  use_static_graph: False\n",
            "  num_gpu: 1\n",
            "  rank: 0\n",
            "  world_size: 1\n",
            "\n",
            "Random seed: 134\n",
            "Dataset [DatasetSR - train_dataset] is created.\n",
            "25-09-14 18:56:49.476 : Number of train images: 2,000, iters: 1,000\n",
            "Dataset [DatasetSR - test_dataset] is created.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Pass this initialization! Initialization was done during network definition!\n",
            "using the UNet discriminator\n",
            "Initialization method [orthogonal + uniform], gain is [0.20]\n",
            "Pass this initialization! Initialization was done during network definition!\n",
            "Training model [ModelGAN] is created.\n",
            "Loading model for G [superresolution/swinir_sr_realworld_x4_gan/models/2000_G.pth] ...\n",
            "Loading model for E [superresolution/swinir_sr_realworld_x4_gan/models/2000_E.pth] ...\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Sequential(\n",
            "  (child0): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (child1): Sequential(\n",
            "    (0): ReLU(inplace=True)\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (child2): Sequential(\n",
            "    (0): ReLU(inplace=True)\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (child3): Sequential(\n",
            "    (0): ReLU(inplace=True)\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (child4): Sequential(\n",
            "    (0): ReLU(inplace=True)\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "feature_layer: [2, 7, 16, 25, 34]  with weights: [0.1, 0.1, 1.0, 1.0, 1.0]\n",
            "Loading optimizerG [superresolution/swinir_sr_realworld_x4_gan/models/2000_optimizerG.pth] ...\n",
            "25-09-14 18:56:53.995 : \n",
            "Networks name: SwinIR\n",
            "Params number: 11715559\n",
            "Net structure:\n",
            "SwinIR(\n",
            "  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (patch_unembed): PatchUnEmbed()\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (layers): ModuleList(\n",
            "    (0): RSTB(\n",
            "      (residual_group): BasicLayer(\n",
            "        dim=180, input_resolution=(64, 64), depth=6\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): Identity()\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.003)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.006)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.009)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.011)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.014)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "    (1): RSTB(\n",
            "      (residual_group): BasicLayer(\n",
            "        dim=180, input_resolution=(64, 64), depth=6\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.017)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.020)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.023)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.026)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.029)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.031)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "    (2): RSTB(\n",
            "      (residual_group): BasicLayer(\n",
            "        dim=180, input_resolution=(64, 64), depth=6\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.034)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.037)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.040)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.043)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.046)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.049)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "    (3): RSTB(\n",
            "      (residual_group): BasicLayer(\n",
            "        dim=180, input_resolution=(64, 64), depth=6\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.051)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.054)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.057)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.060)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.063)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.066)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "    (4): RSTB(\n",
            "      (residual_group): BasicLayer(\n",
            "        dim=180, input_resolution=(64, 64), depth=6\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.069)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.071)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.074)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.077)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.080)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.083)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "    (5): RSTB(\n",
            "      (residual_group): BasicLayer(\n",
            "        dim=180, input_resolution=(64, 64), depth=6\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.086)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.089)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.091)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.094)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.097)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): SwinTransformerBlock(\n",
            "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
            "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              dim=180, window_size=(8, 8), num_heads=6\n",
            "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.100)\n",
            "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
            "  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_before_upsample): Sequential(\n",
            "    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "  )\n",
            "  (conv_up1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_up2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_hr): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            ")\n",
            "\n",
            "Networks name: Discriminator_UNet\n",
            "Params number: 4376897\n",
            "Net structure:\n",
            "Discriminator_UNet(\n",
            "  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n",
            "\n",
            "25-09-14 18:56:54.272 : \n",
            " |  mean  |  min   |  max   |  std   || shape               \n",
            " | -0.001 | -0.448 |  0.483 |  0.124 | torch.Size([180, 3, 3, 3]) || conv_first.weight\n",
            " |  0.007 | -0.848 |  0.462 |  0.161 | torch.Size([180]) || conv_first.bias\n",
            " |  1.164 |  0.511 |  2.421 |  0.259 | torch.Size([180]) || patch_embed.norm.weight\n",
            " | -0.009 | -1.290 |  0.402 |  0.136 | torch.Size([180]) || patch_embed.norm.bias\n",
            " |  0.393 | -0.004 |  1.065 |  0.177 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight\n",
            " | -0.005 | -0.585 |  0.756 |  0.219 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias\n",
            " | -0.269 | -2.108 |  1.984 |  0.675 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index\n",
            " |  0.001 | -1.613 |  1.702 |  0.164 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight\n",
            " | -0.042 | -1.245 |  1.276 |  0.411 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias\n",
            " |  0.000 | -1.121 |  0.880 |  0.071 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight\n",
            " | -0.007 | -1.226 |  0.233 |  0.121 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias\n",
            " |  0.528 |  0.001 |  1.075 |  0.183 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight\n",
            " | -0.021 | -0.544 |  0.494 |  0.205 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias\n",
            " |  0.000 | -1.064 |  0.857 |  0.101 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight\n",
            " | -0.034 | -0.270 |  0.245 |  0.055 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias\n",
            " | -0.001 | -1.028 |  1.394 |  0.101 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight\n",
            " |  0.006 | -1.341 |  0.523 |  0.132 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask\n",
            " |  0.535 | -0.001 |  1.180 |  0.192 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight\n",
            " | -0.014 | -0.698 |  0.905 |  0.202 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias\n",
            " | -0.157 | -2.942 |  1.591 |  0.478 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index\n",
            " | -0.000 | -1.151 |  0.982 |  0.166 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight\n",
            " |  0.016 | -0.914 |  0.911 |  0.287 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias\n",
            " | -0.000 | -0.728 |  0.709 |  0.094 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight\n",
            " |  0.003 | -0.930 |  0.246 |  0.117 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias\n",
            " |  0.599 |  0.319 |  1.265 |  0.166 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight\n",
            " | -0.018 | -0.369 |  0.543 |  0.150 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias\n",
            " |  0.001 | -0.934 |  0.976 |  0.114 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight\n",
            " | -0.044 | -0.198 |  0.110 |  0.047 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias\n",
            " |  0.000 | -0.801 |  1.700 |  0.087 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight\n",
            " |  0.005 | -0.762 |  0.224 |  0.084 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias\n",
            " |  0.674 |  0.109 |  1.108 |  0.171 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight\n",
            " | -0.026 | -0.913 |  1.223 |  0.229 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias\n",
            " | -0.136 | -1.768 |  1.900 |  0.515 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index\n",
            " |  0.000 | -2.021 |  2.048 |  0.173 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight\n",
            " |  0.014 | -0.849 |  0.830 |  0.220 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias\n",
            " |  0.001 | -0.674 |  0.528 |  0.112 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight\n",
            " | -0.003 | -0.486 |  0.290 |  0.111 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias\n",
            " |  0.695 |  0.173 |  1.140 |  0.150 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight\n",
            " | -0.025 | -0.402 |  0.774 |  0.142 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias\n",
            " |  0.001 | -0.967 |  0.856 |  0.128 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight\n",
            " | -0.041 | -0.234 |  0.084 |  0.046 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias\n",
            " | -0.000 | -1.031 |  0.592 |  0.083 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight\n",
            " | -0.002 | -0.404 |  0.166 |  0.077 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask\n",
            " |  0.719 |  0.127 |  1.050 |  0.172 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight\n",
            " | -0.036 | -0.843 |  1.434 |  0.210 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias\n",
            " | -0.139 | -1.860 |  1.763 |  0.549 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index\n",
            " |  0.000 | -2.897 |  3.058 |  0.184 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight\n",
            " |  0.002 | -1.095 |  1.171 |  0.234 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias\n",
            " |  0.000 | -0.605 |  0.570 |  0.117 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight\n",
            " | -0.009 | -0.543 |  0.371 |  0.130 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias\n",
            " |  0.729 |  0.434 |  1.103 |  0.123 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight\n",
            " | -0.035 | -0.458 |  0.574 |  0.119 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias\n",
            " |  0.000 | -0.992 |  1.402 |  0.147 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight\n",
            " | -0.037 | -0.212 |  0.082 |  0.043 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias\n",
            " | -0.000 | -0.614 |  0.521 |  0.088 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight\n",
            " | -0.003 | -0.323 |  0.156 |  0.064 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias\n",
            " |  0.782 |  0.096 |  1.094 |  0.188 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight\n",
            " | -0.051 | -0.747 |  1.143 |  0.174 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias\n",
            " | -0.209 | -3.029 |  1.746 |  0.600 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index\n",
            " |  0.000 | -1.881 |  1.626 |  0.190 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight\n",
            " |  0.007 | -0.886 |  0.854 |  0.168 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias\n",
            " |  0.000 | -0.613 |  0.635 |  0.118 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight\n",
            " | -0.010 | -0.586 |  0.760 |  0.132 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias\n",
            " |  0.729 |  0.407 |  1.086 |  0.111 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight\n",
            " | -0.051 | -0.423 |  0.356 |  0.096 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias\n",
            " |  0.000 | -0.999 |  0.933 |  0.142 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight\n",
            " | -0.029 | -0.229 |  0.066 |  0.040 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias\n",
            " | -0.000 | -0.689 |  0.659 |  0.086 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight\n",
            " | -0.009 | -0.294 |  0.549 |  0.067 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask\n",
            " |  0.713 |  0.095 |  1.009 |  0.169 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight\n",
            " | -0.056 | -0.526 |  1.070 |  0.150 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias\n",
            " | -0.226 | -2.281 |  2.401 |  0.762 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index\n",
            " |  0.001 | -0.973 |  0.921 |  0.186 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight\n",
            " | -0.004 | -0.718 |  0.773 |  0.136 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias\n",
            " | -0.001 | -1.075 |  1.038 |  0.115 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight\n",
            " | -0.014 | -0.282 |  1.208 |  0.133 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias\n",
            " |  0.507 |  0.257 |  0.984 |  0.084 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight\n",
            " | -0.047 | -0.329 |  0.465 |  0.073 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias\n",
            " |  0.001 | -0.955 |  0.926 |  0.130 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight\n",
            " | -0.045 | -0.164 |  0.093 |  0.042 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias\n",
            " | -0.000 | -1.424 |  1.109 |  0.090 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight\n",
            " | -0.010 | -0.132 |  0.992 |  0.086 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias\n",
            " |  0.000 | -0.670 |  0.734 |  0.099 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight\n",
            " | -0.000 | -0.867 |  0.842 |  0.218 | torch.Size([180]) || layers.0.conv.bias\n",
            " |  0.729 |  0.053 |  1.204 |  0.250 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight\n",
            " |  0.014 | -0.911 |  0.918 |  0.185 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias\n",
            " | -0.066 | -1.983 |  1.474 |  0.494 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index\n",
            " |  0.000 | -0.849 |  1.108 |  0.149 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight\n",
            " | -0.010 | -0.879 |  0.805 |  0.146 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias\n",
            " |  0.001 | -0.791 |  0.806 |  0.151 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight\n",
            " | -0.009 | -1.115 |  0.723 |  0.166 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias\n",
            " |  0.735 |  0.310 |  1.109 |  0.135 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight\n",
            " |  0.012 | -0.427 |  0.464 |  0.110 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias\n",
            " |  0.001 | -1.023 |  0.808 |  0.145 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight\n",
            " | -0.068 | -0.222 |  0.077 |  0.050 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias\n",
            " |  0.001 | -1.030 |  1.103 |  0.140 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight\n",
            " | -0.001 | -1.206 |  0.884 |  0.157 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask\n",
            " |  0.778 |  0.139 |  1.114 |  0.210 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight\n",
            " |  0.015 | -0.853 |  0.955 |  0.185 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias\n",
            " |  0.020 | -1.632 |  1.404 |  0.401 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index\n",
            " | -0.000 | -1.155 |  0.917 |  0.155 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight\n",
            " | -0.007 | -0.857 |  0.832 |  0.132 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias\n",
            " | -0.000 | -1.016 |  0.964 |  0.149 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight\n",
            " | -0.003 | -1.547 |  1.076 |  0.191 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias\n",
            " |  0.790 |  0.420 |  1.264 |  0.126 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight\n",
            " |  0.013 | -0.826 |  0.506 |  0.134 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias\n",
            " | -0.000 | -0.862 |  0.820 |  0.168 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight\n",
            " | -0.070 | -0.204 |  0.082 |  0.045 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias\n",
            " |  0.000 | -1.124 |  0.944 |  0.153 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight\n",
            " |  0.000 | -1.262 |  0.854 |  0.157 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias\n",
            " |  0.863 |  0.130 |  1.157 |  0.192 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight\n",
            " |  0.016 | -1.112 |  0.976 |  0.196 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias\n",
            " |  0.128 | -2.934 |  2.190 |  0.540 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index\n",
            " |  0.001 | -1.147 |  0.981 |  0.158 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight\n",
            " | -0.004 | -0.680 |  0.606 |  0.114 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias\n",
            " |  0.000 | -0.719 |  0.695 |  0.148 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight\n",
            " |  0.002 | -1.299 |  1.059 |  0.185 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias\n",
            " |  0.762 |  0.361 |  1.071 |  0.093 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight\n",
            " |  0.009 | -0.784 |  0.521 |  0.131 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias\n",
            " |  0.001 | -0.810 |  0.726 |  0.165 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight\n",
            " | -0.094 | -0.239 |  0.049 |  0.044 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias\n",
            " | -0.001 | -1.107 |  0.953 |  0.146 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight\n",
            " |  0.003 | -0.724 |  0.610 |  0.123 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask\n",
            " |  0.876 |  0.127 |  1.109 |  0.163 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight\n",
            " |  0.020 | -1.282 |  0.978 |  0.183 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias\n",
            " |  0.121 | -3.150 |  2.229 |  0.666 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index\n",
            " |  0.000 | -1.063 |  0.933 |  0.160 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight\n",
            " |  0.004 | -0.588 |  0.544 |  0.118 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias\n",
            " | -0.000 | -0.591 |  0.592 |  0.144 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight\n",
            " |  0.012 | -0.786 |  1.412 |  0.184 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias\n",
            " |  0.755 |  0.416 |  1.093 |  0.080 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight\n",
            " |  0.015 | -0.589 |  0.552 |  0.107 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias\n",
            " |  0.001 | -0.755 |  0.735 |  0.163 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight\n",
            " | -0.094 | -0.205 |  0.067 |  0.045 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias\n",
            " | -0.000 | -0.963 |  0.781 |  0.143 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight\n",
            " |  0.008 | -0.387 |  0.589 |  0.098 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias\n",
            " |  0.900 |  0.149 |  1.129 |  0.151 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight\n",
            " |  0.025 | -1.469 |  1.038 |  0.186 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias\n",
            " |  0.127 | -2.884 |  1.902 |  0.587 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index\n",
            " |  0.000 | -1.191 |  0.932 |  0.160 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight\n",
            " | -0.002 | -0.501 |  0.366 |  0.098 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias\n",
            " | -0.000 | -0.610 |  0.635 |  0.142 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight\n",
            " |  0.019 | -0.395 |  1.400 |  0.168 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias\n",
            " |  0.690 |  0.336 |  0.981 |  0.073 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight\n",
            " |  0.014 | -0.641 |  0.441 |  0.106 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias\n",
            " |  0.002 | -0.682 |  0.714 |  0.159 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight\n",
            " | -0.098 | -0.264 |  0.029 |  0.048 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias\n",
            " | -0.001 | -0.862 |  0.844 |  0.141 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight\n",
            " |  0.009 | -0.468 |  0.857 |  0.106 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask\n",
            " |  0.860 |  0.193 |  1.064 |  0.115 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight\n",
            " |  0.031 | -1.394 |  0.872 |  0.178 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias\n",
            " |  0.085 | -2.212 |  1.812 |  0.517 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index\n",
            " |  0.000 | -0.763 |  1.010 |  0.157 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight\n",
            " | -0.004 | -0.343 |  0.334 |  0.082 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias\n",
            " | -0.001 | -0.962 |  1.038 |  0.141 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight\n",
            " |  0.020 | -0.915 |  1.190 |  0.150 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias\n",
            " |  0.509 |  0.332 |  0.692 |  0.057 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight\n",
            " |  0.016 | -0.490 |  0.547 |  0.088 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias\n",
            " |  0.004 | -0.648 |  0.726 |  0.146 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight\n",
            " | -0.113 | -0.293 |  0.078 |  0.059 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias\n",
            " | -0.002 | -0.878 |  1.161 |  0.150 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight\n",
            " |  0.018 | -1.304 |  0.751 |  0.146 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias\n",
            " | -0.000 | -0.519 |  0.534 |  0.075 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight\n",
            " |  0.000 | -0.762 |  1.573 |  0.210 | torch.Size([180]) || layers.1.conv.bias\n",
            " |  0.744 |  0.117 |  1.040 |  0.183 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight\n",
            " |  0.016 | -1.135 |  0.637 |  0.162 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias\n",
            " |  0.015 | -3.104 |  1.207 |  0.455 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index\n",
            " |  0.000 | -0.739 |  0.758 |  0.147 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight\n",
            " |  0.004 | -0.538 |  0.713 |  0.112 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias\n",
            " | -0.000 | -0.714 |  0.722 |  0.169 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight\n",
            " |  0.000 | -0.660 |  0.781 |  0.154 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias\n",
            " |  0.983 |  0.362 |  1.603 |  0.148 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight\n",
            " |  0.026 | -0.627 |  0.395 |  0.135 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias\n",
            " |  0.000 | -1.060 |  1.149 |  0.166 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight\n",
            " | -0.110 | -0.284 |  0.069 |  0.061 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias\n",
            " | -0.000 | -0.945 |  1.052 |  0.175 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight\n",
            " |  0.004 | -0.408 |  0.899 |  0.113 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask\n",
            " |  0.801 |  0.164 |  1.089 |  0.169 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight\n",
            " |  0.021 | -1.363 |  0.652 |  0.177 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias\n",
            " | -0.020 | -2.918 |  1.470 |  0.635 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index\n",
            " |  0.000 | -0.796 |  0.745 |  0.151 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight\n",
            " | -0.001 | -0.472 |  0.861 |  0.115 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias\n",
            " | -0.001 | -0.892 |  0.741 |  0.174 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight\n",
            " |  0.005 | -0.828 |  0.998 |  0.167 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias\n",
            " |  0.951 |  0.337 |  1.326 |  0.126 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight\n",
            " |  0.024 | -0.669 |  0.630 |  0.158 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias\n",
            " |  0.002 | -0.977 |  1.312 |  0.165 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight\n",
            " | -0.090 | -0.245 |  0.068 |  0.050 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias\n",
            " |  0.000 | -1.036 |  1.066 |  0.175 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight\n",
            " |  0.004 | -0.500 |  1.152 |  0.134 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias\n",
            " |  0.860 |  0.149 |  1.099 |  0.164 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight\n",
            " |  0.027 | -1.448 |  0.727 |  0.186 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias\n",
            " |  0.078 | -2.035 |  1.449 |  0.465 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index\n",
            " |  0.000 | -0.820 |  0.967 |  0.154 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight\n",
            " | -0.008 | -0.701 |  0.626 |  0.110 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias\n",
            " |  0.000 | -0.877 |  1.049 |  0.176 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight\n",
            " |  0.010 | -0.746 |  1.327 |  0.186 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias\n",
            " |  0.927 |  0.348 |  1.185 |  0.130 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight\n",
            " |  0.039 | -0.795 |  0.694 |  0.164 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias\n",
            " |  0.001 | -0.955 |  0.664 |  0.164 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight\n",
            " | -0.095 | -0.288 |  0.019 |  0.050 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias\n",
            " | -0.000 | -0.816 |  0.967 |  0.173 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight\n",
            " |  0.006 | -0.698 |  1.273 |  0.147 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask\n",
            " |  0.865 |  0.123 |  1.104 |  0.160 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight\n",
            " |  0.036 | -1.123 |  0.659 |  0.167 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias\n",
            " |  0.101 | -3.790 |  1.787 |  0.645 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index\n",
            " | -0.000 | -0.817 |  0.760 |  0.155 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight\n",
            " | -0.003 | -0.486 |  0.690 |  0.122 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias\n",
            " |  0.001 | -0.852 |  1.126 |  0.171 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight\n",
            " |  0.007 | -1.084 |  1.441 |  0.197 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias\n",
            " |  0.882 |  0.366 |  1.121 |  0.117 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight\n",
            " |  0.044 | -0.801 |  0.717 |  0.147 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias\n",
            " |  0.002 | -0.790 |  0.687 |  0.161 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight\n",
            " | -0.105 | -0.278 |  0.044 |  0.052 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias\n",
            " |  0.000 | -0.895 |  1.066 |  0.170 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight\n",
            " |  0.000 | -1.192 |  1.164 |  0.162 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias\n",
            " |  0.864 |  0.110 |  1.151 |  0.146 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight\n",
            " |  0.044 | -1.072 |  0.715 |  0.162 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias\n",
            " |  0.110 | -3.404 |  1.827 |  0.628 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index\n",
            " |  0.000 | -1.062 |  0.719 |  0.155 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight\n",
            " | -0.002 | -0.530 |  0.401 |  0.113 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias\n",
            " |  0.000 | -0.952 |  1.023 |  0.171 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight\n",
            " |  0.006 | -1.643 |  1.096 |  0.198 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias\n",
            " |  0.815 |  0.389 |  1.037 |  0.100 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight\n",
            " |  0.040 | -0.822 |  0.728 |  0.150 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias\n",
            " |  0.002 | -0.722 |  0.836 |  0.158 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight\n",
            " | -0.109 | -0.337 |  0.023 |  0.057 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias\n",
            " | -0.000 | -1.157 |  1.025 |  0.168 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight\n",
            " |  0.000 | -1.759 |  0.633 |  0.181 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask\n",
            " |  0.787 |  0.171 |  0.962 |  0.111 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight\n",
            " |  0.045 | -0.961 |  0.657 |  0.155 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias\n",
            " | -0.038 | -2.672 |  1.570 |  0.756 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index\n",
            " | -0.000 | -0.767 |  0.881 |  0.153 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight\n",
            " | -0.003 | -0.418 |  0.436 |  0.095 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias\n",
            " | -0.001 | -1.635 |  1.846 |  0.169 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight\n",
            " |  0.004 | -2.072 |  0.617 |  0.198 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias\n",
            " |  0.659 |  0.381 |  0.851 |  0.074 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight\n",
            " |  0.018 | -1.467 |  0.559 |  0.162 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias\n",
            " |  0.000 | -0.614 |  0.663 |  0.148 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight\n",
            " | -0.135 | -0.352 |  0.046 |  0.066 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias\n",
            " | -0.001 | -2.060 |  0.902 |  0.161 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight\n",
            " |  0.001 | -1.863 |  0.196 |  0.175 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias\n",
            " | -0.000 | -0.529 |  0.579 |  0.067 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight\n",
            " |  0.006 | -1.265 |  1.267 |  0.214 | torch.Size([180]) || layers.2.conv.bias\n",
            " |  0.708 |  0.184 |  0.950 |  0.145 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight\n",
            " |  0.008 | -1.166 |  0.652 |  0.154 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias\n",
            " |  0.002 | -2.264 |  1.580 |  0.708 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index\n",
            " |  0.000 | -0.899 |  1.019 |  0.152 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight\n",
            " | -0.004 | -0.578 |  0.502 |  0.097 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias\n",
            " | -0.001 | -0.830 |  0.792 |  0.195 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight\n",
            " |  0.003 | -0.361 |  0.479 |  0.154 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias\n",
            " |  1.168 |  0.611 |  1.755 |  0.126 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight\n",
            " |  0.006 | -0.453 |  0.364 |  0.143 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias\n",
            " |  0.001 | -1.084 |  1.288 |  0.172 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight\n",
            " | -0.113 | -0.413 |  0.083 |  0.079 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias\n",
            " | -0.001 | -0.956 |  0.872 |  0.184 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight\n",
            " | -0.003 | -0.229 |  0.303 |  0.083 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask\n",
            " |  0.756 |  0.245 |  0.992 |  0.146 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight\n",
            " |  0.007 | -1.035 |  0.677 |  0.153 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias\n",
            " |  0.157 | -2.016 |  1.623 |  0.638 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index\n",
            " |  0.000 | -0.911 |  0.922 |  0.152 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight\n",
            " |  0.006 | -0.483 |  0.525 |  0.101 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias\n",
            " | -0.000 | -0.929 |  0.784 |  0.187 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight\n",
            " |  0.004 | -0.420 |  0.452 |  0.158 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias\n",
            " |  1.184 |  0.587 |  1.488 |  0.139 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight\n",
            " |  0.007 | -0.644 |  0.562 |  0.156 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias\n",
            " |  0.001 | -1.048 |  1.106 |  0.172 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight\n",
            " | -0.099 | -0.342 |  0.059 |  0.061 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias\n",
            " | -0.001 | -0.848 |  1.107 |  0.186 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight\n",
            " | -0.003 | -0.321 |  0.214 |  0.103 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias\n",
            " |  0.760 |  0.240 |  1.008 |  0.144 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight\n",
            " |  0.007 | -1.208 |  0.728 |  0.165 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias\n",
            " |  0.153 | -1.959 |  1.512 |  0.586 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index\n",
            " |  0.000 | -0.851 |  0.899 |  0.154 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight\n",
            " | -0.001 | -0.559 |  0.388 |  0.093 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias\n",
            " | -0.001 | -0.782 |  0.907 |  0.189 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight\n",
            " |  0.013 | -0.515 |  0.501 |  0.167 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias\n",
            " |  1.211 |  0.662 |  1.581 |  0.139 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight\n",
            " |  0.005 | -0.496 |  0.527 |  0.127 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias\n",
            " |  0.001 | -0.873 |  0.997 |  0.174 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight\n",
            " | -0.098 | -0.262 |  0.102 |  0.058 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias\n",
            " | -0.001 | -0.832 |  0.868 |  0.193 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight\n",
            " |  0.000 | -0.316 |  0.348 |  0.116 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask\n",
            " |  0.785 |  0.239 |  1.079 |  0.159 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight\n",
            " |  0.007 | -1.279 |  0.695 |  0.166 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias\n",
            " |  0.176 | -2.165 |  2.405 |  0.653 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index\n",
            " |  0.000 | -0.777 |  0.837 |  0.153 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight\n",
            " |  0.004 | -0.561 |  0.434 |  0.107 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias\n",
            " | -0.001 | -0.797 |  1.046 |  0.185 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight\n",
            " |  0.007 | -0.500 |  0.592 |  0.161 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias\n",
            " |  1.203 |  0.670 |  1.619 |  0.156 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight\n",
            " |  0.006 | -0.431 |  0.551 |  0.146 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias\n",
            " |  0.001 | -0.903 |  0.994 |  0.175 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight\n",
            " | -0.097 | -0.256 |  0.063 |  0.054 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias\n",
            " | -0.001 | -0.926 |  0.960 |  0.198 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight\n",
            " |  0.001 | -0.368 |  0.466 |  0.129 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias\n",
            " |  0.790 |  0.233 |  1.059 |  0.156 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight\n",
            " |  0.005 | -1.615 |  0.855 |  0.182 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias\n",
            " |  0.042 | -4.506 |  2.094 |  0.719 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index\n",
            " |  0.000 | -0.872 |  0.895 |  0.155 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight\n",
            " | -0.002 | -0.531 |  0.472 |  0.111 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias\n",
            " | -0.000 | -1.115 |  1.115 |  0.193 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight\n",
            " |  0.012 | -0.679 |  0.831 |  0.170 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias\n",
            " |  1.189 |  0.723 |  1.518 |  0.137 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight\n",
            " |  0.020 | -0.269 |  0.595 |  0.117 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias\n",
            " | -0.001 | -0.825 |  0.879 |  0.176 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight\n",
            " | -0.096 | -0.265 |  0.047 |  0.056 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias\n",
            " |  0.000 | -1.055 |  1.196 |  0.205 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight\n",
            " |  0.008 | -0.607 |  0.639 |  0.143 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask\n",
            " |  0.743 |  0.217 |  0.994 |  0.141 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight\n",
            " |  0.007 | -1.114 |  0.761 |  0.149 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias\n",
            " |  0.167 | -2.794 |  2.067 |  0.463 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index\n",
            " |  0.000 | -0.828 |  0.783 |  0.152 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight\n",
            " |  0.003 | -0.423 |  0.363 |  0.109 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias\n",
            " | -0.002 | -1.037 |  0.913 |  0.188 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight\n",
            " |  0.019 | -0.951 |  0.913 |  0.186 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias\n",
            " |  1.143 |  0.806 |  1.434 |  0.108 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight\n",
            " |  0.005 | -0.341 |  0.643 |  0.121 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias\n",
            " |  0.000 | -0.890 |  0.829 |  0.173 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight\n",
            " | -0.108 | -0.314 |  0.057 |  0.061 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias\n",
            " |  0.002 | -1.139 |  1.030 |  0.200 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight\n",
            " |  0.002 | -1.167 |  0.635 |  0.181 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias\n",
            " | -0.000 | -0.707 |  0.641 |  0.076 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight\n",
            " |  0.011 | -0.796 |  1.068 |  0.211 | torch.Size([180]) || layers.3.conv.bias\n",
            " |  0.838 |  0.241 |  1.234 |  0.187 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight\n",
            " | -0.010 | -0.650 |  0.449 |  0.124 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias\n",
            " |  0.148 | -4.418 |  1.610 |  0.586 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index\n",
            " | -0.000 | -1.321 |  1.384 |  0.177 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight\n",
            " |  0.002 | -0.597 |  0.551 |  0.112 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias\n",
            " |  0.000 | -1.220 |  1.125 |  0.249 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight\n",
            " |  0.008 | -0.473 |  0.531 |  0.193 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias\n",
            " |  1.668 |  1.017 |  1.973 |  0.157 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight\n",
            " | -0.006 | -0.587 |  0.614 |  0.246 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias\n",
            " | -0.001 | -1.106 |  0.909 |  0.204 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight\n",
            " | -0.190 | -0.680 |  0.178 |  0.116 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias\n",
            " |  0.000 | -0.914 |  1.018 |  0.212 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight\n",
            " |  0.006 | -0.176 |  0.224 |  0.072 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask\n",
            " |  0.894 |  0.249 |  1.280 |  0.213 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight\n",
            " | -0.010 | -0.673 |  0.615 |  0.108 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias\n",
            " |  0.112 | -4.827 |  2.679 |  0.805 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index\n",
            " | -0.000 | -1.131 |  1.259 |  0.173 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight\n",
            " | -0.006 | -0.560 |  0.562 |  0.125 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias\n",
            " |  0.001 | -1.206 |  0.993 |  0.253 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight\n",
            " |  0.002 | -0.442 |  0.555 |  0.219 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias\n",
            " |  1.695 |  1.098 |  1.992 |  0.165 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight\n",
            " |  0.019 | -0.582 |  0.594 |  0.221 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias\n",
            " | -0.005 | -0.882 |  1.261 |  0.203 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight\n",
            " | -0.158 | -0.529 |  0.126 |  0.105 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias\n",
            " | -0.001 | -1.018 |  1.011 |  0.222 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight\n",
            " |  0.006 | -0.224 |  0.262 |  0.097 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias\n",
            " |  0.864 |  0.276 |  1.235 |  0.208 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight\n",
            " | -0.008 | -0.503 |  0.627 |  0.091 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias\n",
            " |  0.175 | -4.069 |  1.891 |  0.584 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index\n",
            " |  0.000 | -1.301 |  1.076 |  0.173 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight\n",
            " |  0.003 | -0.599 |  0.495 |  0.121 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias\n",
            " | -0.001 | -1.186 |  1.109 |  0.269 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight\n",
            " |  0.004 | -0.602 |  0.560 |  0.200 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias\n",
            " |  1.770 |  1.194 |  2.085 |  0.160 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight\n",
            " | -0.005 | -0.468 |  0.442 |  0.179 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias\n",
            " | -0.003 | -0.906 |  1.241 |  0.204 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight\n",
            " | -0.141 | -0.427 |  0.146 |  0.089 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias\n",
            " | -0.000 | -1.551 |  1.284 |  0.237 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight\n",
            " |  0.008 | -0.242 |  0.266 |  0.100 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask\n",
            " |  0.852 |  0.283 |  1.353 |  0.212 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight\n",
            " | -0.010 | -0.482 |  0.724 |  0.101 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias\n",
            " |  0.150 | -3.361 |  1.337 |  0.429 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index\n",
            " | -0.000 | -1.212 |  1.144 |  0.167 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight\n",
            " |  0.005 | -0.502 |  0.582 |  0.107 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias\n",
            " |  0.000 | -1.202 |  1.167 |  0.260 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight\n",
            " |  0.001 | -0.465 |  0.414 |  0.191 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias\n",
            " |  1.750 |  1.173 |  2.062 |  0.163 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight\n",
            " | -0.002 | -0.449 |  0.608 |  0.201 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias\n",
            " | -0.004 | -1.006 |  0.828 |  0.203 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight\n",
            " | -0.139 | -0.404 |  0.081 |  0.082 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias\n",
            " | -0.000 | -1.235 |  1.291 |  0.242 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight\n",
            " |  0.002 | -0.293 |  0.343 |  0.107 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias\n",
            " |  0.857 |  0.274 |  1.392 |  0.217 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight\n",
            " | -0.011 | -0.582 |  0.780 |  0.112 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias\n",
            " |  0.027 | -5.395 |  2.122 |  0.620 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index\n",
            " | -0.000 | -1.611 |  1.115 |  0.165 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight\n",
            " | -0.002 | -0.570 |  0.406 |  0.099 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias\n",
            " |  0.001 | -1.442 |  1.224 |  0.269 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight\n",
            " | -0.008 | -0.507 |  0.488 |  0.188 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias\n",
            " |  1.707 |  1.038 |  2.133 |  0.163 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight\n",
            " |  0.004 | -0.682 |  0.587 |  0.221 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias\n",
            " | -0.004 | -0.911 |  0.844 |  0.202 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight\n",
            " | -0.140 | -0.479 |  0.066 |  0.080 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias\n",
            " |  0.000 | -1.439 |  1.389 |  0.257 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight\n",
            " |  0.004 | -0.267 |  0.338 |  0.093 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask\n",
            " |  0.843 |  0.318 |  1.242 |  0.196 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight\n",
            " | -0.017 | -0.598 |  0.628 |  0.102 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias\n",
            " |  0.014 | -2.923 |  1.449 |  0.527 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index\n",
            " |  0.000 | -1.037 |  1.018 |  0.165 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight\n",
            " | -0.004 | -0.720 |  0.454 |  0.101 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias\n",
            " |  0.001 | -1.405 |  1.232 |  0.262 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight\n",
            " | -0.007 | -0.522 |  0.586 |  0.199 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias\n",
            " |  1.735 |  1.127 |  2.071 |  0.152 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight\n",
            " |  0.006 | -0.690 |  0.661 |  0.241 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias\n",
            " | -0.005 | -0.973 |  0.982 |  0.205 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight\n",
            " | -0.137 | -0.414 |  0.065 |  0.079 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias\n",
            " | -0.000 | -1.695 |  1.680 |  0.274 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight\n",
            " | -0.002 | -0.312 |  0.351 |  0.113 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias\n",
            " | -0.000 | -0.707 |  0.714 |  0.105 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight\n",
            " |  0.027 | -1.339 |  0.706 |  0.285 | torch.Size([180]) || layers.4.conv.bias\n",
            " |  1.124 |  0.387 |  1.895 |  0.289 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight\n",
            " | -0.008 | -0.662 |  1.047 |  0.192 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias\n",
            " |  0.003 | -2.191 |  1.901 |  0.512 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index\n",
            " |  0.000 | -1.939 |  1.894 |  0.228 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight\n",
            " |  0.009 | -0.365 |  0.569 |  0.110 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias\n",
            " |  0.001 | -1.775 |  2.089 |  0.335 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight\n",
            " |  0.018 | -0.679 |  0.700 |  0.237 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias\n",
            " |  2.112 |  1.190 |  2.885 |  0.302 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight\n",
            " | -0.001 | -0.361 |  0.398 |  0.123 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias\n",
            " | -0.001 | -1.270 |  1.304 |  0.264 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight\n",
            " | -0.067 | -0.637 |  0.549 |  0.133 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias\n",
            " |  0.001 | -1.693 |  1.621 |  0.280 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight\n",
            " | -0.006 | -0.242 |  0.198 |  0.092 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask\n",
            " |  1.074 |  0.458 |  1.880 |  0.283 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight\n",
            " | -0.021 | -0.488 |  0.498 |  0.143 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias\n",
            " |  0.058 | -2.515 |  1.532 |  0.490 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index\n",
            " | -0.000 | -1.843 |  1.908 |  0.214 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight\n",
            " | -0.004 | -0.437 |  0.643 |  0.115 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias\n",
            " | -0.000 | -2.065 |  1.894 |  0.313 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight\n",
            " |  0.017 | -0.789 |  0.650 |  0.245 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias\n",
            " |  2.198 |  1.234 |  2.878 |  0.314 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight\n",
            " | -0.007 | -0.302 |  0.244 |  0.111 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias\n",
            " | -0.001 | -1.385 |  1.415 |  0.264 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight\n",
            " | -0.051 | -0.451 |  0.277 |  0.121 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias\n",
            " |  0.002 | -2.430 |  1.873 |  0.312 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight\n",
            " | -0.004 | -0.228 |  0.273 |  0.097 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias\n",
            " |  1.047 |  0.384 |  1.954 |  0.315 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight\n",
            " | -0.014 | -0.540 |  0.695 |  0.176 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias\n",
            " | -0.012 | -2.360 |  1.753 |  0.460 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index\n",
            " | -0.000 | -1.810 |  1.564 |  0.201 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight\n",
            " |  0.001 | -0.376 |  0.351 |  0.098 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias\n",
            " | -0.001 | -2.615 |  2.257 |  0.372 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight\n",
            " |  0.010 | -0.923 |  0.715 |  0.264 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias\n",
            " |  2.209 |  1.225 |  2.953 |  0.339 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight\n",
            " | -0.018 | -0.360 |  0.302 |  0.118 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias\n",
            " |  0.000 | -1.318 |  1.158 |  0.259 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight\n",
            " | -0.041 | -0.529 |  0.232 |  0.124 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias\n",
            " |  0.001 | -2.504 |  2.898 |  0.344 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight\n",
            " | -0.005 | -0.351 |  0.238 |  0.083 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask\n",
            " |  1.105 |  0.452 |  1.898 |  0.333 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight\n",
            " | -0.020 | -0.453 |  0.600 |  0.179 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias\n",
            " | -0.082 | -2.665 |  1.600 |  0.483 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index\n",
            " | -0.000 | -1.900 |  2.116 |  0.204 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight\n",
            " | -0.007 | -0.490 |  0.352 |  0.102 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias\n",
            " | -0.001 | -2.193 |  1.925 |  0.363 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight\n",
            " |  0.020 | -0.743 |  0.684 |  0.264 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias\n",
            " |  2.216 |  1.306 |  3.034 |  0.353 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight\n",
            " | -0.016 | -0.371 |  0.273 |  0.130 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias\n",
            " |  0.000 | -1.206 |  1.235 |  0.254 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight\n",
            " | -0.013 | -0.414 |  0.260 |  0.126 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias\n",
            " |  0.001 | -2.726 |  2.929 |  0.369 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight\n",
            " | -0.013 | -0.331 |  0.212 |  0.091 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias\n",
            " |  1.021 |  0.452 |  1.598 |  0.286 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight\n",
            " | -0.018 | -0.504 |  0.566 |  0.195 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias\n",
            " | -0.061 | -2.787 |  1.169 |  0.403 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index\n",
            " |  0.001 | -2.000 |  1.897 |  0.186 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight\n",
            " |  0.001 | -0.328 |  0.380 |  0.095 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias\n",
            " |  0.004 | -2.590 |  2.483 |  0.380 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight\n",
            " |  0.009 | -0.909 |  0.722 |  0.274 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias\n",
            " |  2.061 |  1.193 |  2.816 |  0.331 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight\n",
            " | -0.009 | -0.434 |  0.352 |  0.136 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias\n",
            " | -0.000 | -1.194 |  1.275 |  0.246 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight\n",
            " | -0.032 | -0.410 |  0.246 |  0.119 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias\n",
            " | -0.001 | -2.544 |  2.682 |  0.382 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight\n",
            " | -0.006 | -0.385 |  0.328 |  0.120 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias\n",
            " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask\n",
            " |  1.066 |  0.426 |  1.707 |  0.287 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight\n",
            " | -0.007 | -0.527 |  0.749 |  0.213 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias\n",
            " | -0.019 | -1.416 |  1.067 |  0.281 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table\n",
            " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index\n",
            " | -0.000 | -1.811 |  1.359 |  0.186 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight\n",
            " |  0.005 | -0.352 |  0.428 |  0.105 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias\n",
            " |  0.002 | -2.174 |  2.028 |  0.351 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight\n",
            " |  0.015 | -0.798 |  0.890 |  0.309 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias\n",
            " |  1.948 |  1.139 |  2.864 |  0.334 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight\n",
            " | -0.012 | -0.410 |  0.473 |  0.146 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias\n",
            " | -0.000 | -1.157 |  1.210 |  0.250 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight\n",
            " | -0.014 | -0.386 |  0.222 |  0.117 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias\n",
            " | -0.004 | -2.954 |  2.497 |  0.417 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight\n",
            " | -0.013 | -0.479 |  0.369 |  0.148 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias\n",
            " |  0.001 | -1.316 |  1.395 |  0.192 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight\n",
            " |  0.018 | -0.558 |  0.385 |  0.155 | torch.Size([180]) || layers.5.conv.bias\n",
            " |  0.072 |  0.019 |  0.359 |  0.051 | torch.Size([180]) || norm.weight\n",
            " | -0.004 | -0.122 |  0.549 |  0.063 | torch.Size([180]) || norm.bias\n",
            " | -0.000 | -0.751 |  0.712 |  0.137 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight\n",
            " | -0.009 | -0.367 |  0.642 |  0.130 | torch.Size([180]) || conv_after_body.bias\n",
            " |  0.000 | -0.471 |  1.010 |  0.085 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight\n",
            " | -0.079 | -0.633 |  0.090 |  0.173 | torch.Size([64]) || conv_before_upsample.0.bias\n",
            " | -0.004 | -0.648 |  0.552 |  0.080 | torch.Size([64, 64, 3, 3]) || conv_up1.weight\n",
            " | -0.033 | -0.141 |  0.111 |  0.052 | torch.Size([64]) || conv_up1.bias\n",
            " | -0.004 | -0.607 |  0.650 |  0.077 | torch.Size([64, 64, 3, 3]) || conv_up2.weight\n",
            " | -0.017 | -0.728 |  0.067 |  0.097 | torch.Size([64]) || conv_up2.bias\n",
            " | -0.007 | -0.784 |  0.802 |  0.091 | torch.Size([64, 64, 3, 3]) || conv_hr.weight\n",
            " |  0.036 | -0.106 |  0.167 |  0.050 | torch.Size([64]) || conv_hr.bias\n",
            " |  0.002 | -0.199 |  0.522 |  0.055 | torch.Size([3, 64, 3, 3]) || conv_last.weight\n",
            " | -0.098 | -0.113 | -0.071 |  0.023 | torch.Size([3]) || conv_last.bias\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:575: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  _warn_get_lr_called_within_step(self)\n",
            "25-09-14 18:58:17.449 : <epoch:  0, iter:   2,100, lr:1.000e-05, iter_time:2.462s> G_loss: 8.627e-02 F_loss: 1.266e+01 D_loss: 6.921e-02 D_real: 2.039e-03 D_fake: 2.094e-03 \n",
            "25-09-14 19:00:24.765 : <epoch:  0, iter:   2,100, Avg PSNR: 17.49dB, Avg SSIM: 0.4220, Avg NIQE: 5.98>\n",
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "25-09-14 19:01:46.103 : <epoch:  0, iter:   2,200, lr:1.000e-05, iter_time:1.762s> G_loss: 7.842e-02 F_loss: 9.615e+00 D_loss: 7.185e-02 D_real: -5.068e-03 D_fake: -4.950e-02 \n",
            "25-09-14 19:03:52.117 : <epoch:  0, iter:   2,200, Avg PSNR: 17.47dB, Avg SSIM: 0.4238, Avg NIQE: 6.21>\n",
            "25-09-14 19:05:13.773 : <epoch:  0, iter:   2,300, lr:1.000e-05, iter_time:1.940s> G_loss: 7.563e-02 F_loss: 8.545e+00 D_loss: 7.075e-02 D_real: 9.589e-02 D_fake: -2.717e-02 \n",
            "25-09-14 19:07:17.808 : <epoch:  0, iter:   2,300, Avg PSNR: 17.44dB, Avg SSIM: 0.4210, Avg NIQE: 6.23>\n",
            "25-09-14 19:08:39.453 : <epoch:  0, iter:   2,400, lr:1.000e-05, iter_time:1.985s> G_loss: 9.320e-02 F_loss: 1.115e+01 D_loss: 7.047e-02 D_real: -2.014e-02 D_fake: -2.289e-02 \n",
            "25-09-14 19:10:45.783 : <epoch:  0, iter:   2,400, Avg PSNR: 17.49dB, Avg SSIM: 0.4226, Avg NIQE: 6.24>\n",
            "25-09-14 19:12:07.154 : <epoch:  0, iter:   2,500, lr:1.000e-05, iter_time:1.751s> G_loss: 6.092e-02 F_loss: 8.956e+00 D_loss: 7.204e-02 D_real: -3.498e-02 D_fake: -5.366e-02 \n",
            "25-09-14 19:12:07.155 : Saving the model.\n",
            "25-09-14 19:14:14.231 : <epoch:  0, iter:   2,500, Avg PSNR: 17.47dB, Avg SSIM: 0.4258, Avg NIQE: 6.24>\n",
            "25-09-14 19:15:36.090 : <epoch:  0, iter:   2,600, lr:1.000e-05, iter_time:2.177s> G_loss: 1.053e-01 F_loss: 1.140e+01 D_loss: 7.858e-02 D_real: 2.789e-01 D_fake: -1.724e-01 \n",
            "25-09-14 19:17:42.201 : <epoch:  0, iter:   2,600, Avg PSNR: 17.41dB, Avg SSIM: 0.4197, Avg NIQE: 6.17>\n",
            "25-09-14 19:19:03.911 : <epoch:  0, iter:   2,700, lr:1.000e-05, iter_time:2.160s> G_loss: 5.318e-02 F_loss: 9.326e+00 D_loss: 7.112e-02 D_real: 8.343e-02 D_fake: -3.549e-02 \n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x793dc44fad40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1609, in _shutdown_workers\n",
            "    self._worker_result_queue.put((None, None))\n",
            "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 94, in put\n",
            "    self._start_thread()\n",
            "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 192, in _start_thread\n",
            "    self._thread.start()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 999, in start\n",
            "    self._started.wait()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 655, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 355, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/KAIR/main_train_psnr.py\", line 303, in <module>\n",
            "    main()\n",
            "  File \"/content/KAIR/main_train_psnr.py\", line 259, in main\n",
            "    model.test()\n",
            "  File \"/content/KAIR/models/model_gan.py\", line 295, in test\n",
            "    self.netG_forward()\n",
            "  File \"/content/KAIR/models/model_gan.py\", line 197, in netG_forward\n",
            "    self.E = self.netG(self.L)\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1879, in _call_impl\n",
            "    return inner()\n",
            "           ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1827, in inner\n",
            "    result = forward_call(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/data_parallel.py\", line 192, in forward\n",
            "    return self.module(*inputs[0], **module_kwargs[0])\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/KAIR/models/network_swinir.py\", line 826, in forward\n",
            "    x = self.conv_after_body(self.forward_features(x)) + x\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/KAIR/models/network_swinir.py\", line 795, in forward_features\n",
            "    x = self.pos_drop(x)\n",
            "        ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1778, in _call_impl\n",
            "    forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mSwinIR_finetune\u001b[0m at: \u001b[34mhttps://wandb.ai/dsha43925-middlesex-university-mauritius/SwinIR/runs/h98lkbp4\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250914_185648-h98lkbp4/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Inference with latest generator obtained"
      ],
      "metadata": {
        "id": "X7VvNMrqx_DD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 unzip file with test images"
      ],
      "metadata": {
        "id": "3_mISaVDljgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip dataset of images to test\n",
        "!unzip -q \"/content/drive/MyDrive/Final_project/LRInferenceImages.zip\" -d \"/content\""
      ],
      "metadata": {
        "id": "Owe38PGcTlNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23852a04-a5ad-420d-b193-8629f109641f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace /content/LRInferenceImages/AirbusDhirubhai_AmbaniSolarPlant.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Inference"
      ],
      "metadata": {
        "id": "OPZVO2ilnItF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### for simple image test"
      ],
      "metadata": {
        "id": "lcyjG-dZr86Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#modify with latest generator obtained\n",
        "latest_model = \"/content/KAIR/superresolution/swinir_sr_realworld_x4_gan/models/2000_G.pth\"\n",
        "#latest_model = \"/content/drive/MyDrive/Final_project/SwinIR/2000_G.pth\"\n",
        "image_folder_path = \"/content/LRInferenceImages\""
      ],
      "metadata": {
        "id": "7yuserVLlxVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#/content/KAIR/SwinIR/results/swinir_real_sr_x4_large\n",
        "!python main_test_swinir.py --task real_sr --scale 4 --model_path $latest_model --folder_lq $image_folder_path --tile 256"
      ],
      "metadata": {
        "id": "8IIHeVvF4zjv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82b6b696-ce01-4608-8c5b-d788a9060a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "loading model from /content/KAIR/superresolution/swinir_sr_realworld_x4_gan/models/2000_G.pth\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Testing 0 AirbusDhirubhai_AmbaniSolarPlant\n",
            "Testing 1 AirbusRotterdamPort \n",
            "Testing 2 AirbusSantiagoAirport\n",
            "Testing 3 MaxarDRC            \n",
            "Testing 4 MaxarIndonesia      \n",
            "Testing 5 MaxarNepalPostFlood \n",
            "Testing 6 MaxarYellowStone    \n",
            "Testing 7 TurkeyEarthquake    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For test dataset"
      ],
      "metadata": {
        "id": "9zKeSIcosBiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload the zip file containing low resolution images for inference.\n",
        "code_zip_path = '/content/drive/MyDrive/Final_project/testImages.zip'\n",
        "unzip_path = '/content'\n",
        "!unzip -q $code_zip_path -d $unzip_path\n",
        "!ls -l $unzip_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1ZCqL-Ir8HD",
        "outputId": "86533c8a-2e6e-411e-b2be-745e18077082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 24\n",
            "drwxrwxrwx  4 root root 4096 Aug 24 08:09 dataset\n",
            "drwx------  5 root root 4096 Sep 14 17:09 drive\n",
            "drwxrwxrwx 11 root root 4096 Sep 14 17:14 KAIR\n",
            "drwxrwxrwx  2 root root 4096 Sep  1 20:57 LRInferenceImages\n",
            "drwxr-xr-x  1 root root 4096 Sep  9 13:46 sample_data\n",
            "drwxrwxrwx  7 root root 4096 Sep 11 19:39 testImages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_path = \"/content/testImages\"\n",
        "folders = os.listdir(test_image_path)\n",
        "model_path = \"/content/KAIR/superresolution/swinir_sr_realworld_x4_gan/models/2000_G.pth\"\n",
        "#model_path = \"/content/KAIR/model_zoo/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN-with-dict-keys-params-and-params_ema.pth\"\n",
        "for folder in folders:\n",
        "    image_path = os.path.join(test_image_path, folder)\n",
        "    if os.path.isdir(image_path):\n",
        "        print(f\"Processing {folder}\")\n",
        "        output_folder = f\"/content/KAIR/results/swinir_real_sr_x4/{folder}\"\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "        !python main_test_swinir.py --task real_sr --scale 4 --model_path \"{model_path}\" --folder_lq \"{image_path}\" --tile 256\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TZq1ZvtsLSG",
        "outputId": "d1f18e7e-3bd3-4ad5-ca8e-b90c26b7c36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing buildings\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "loading model from /content/KAIR/superresolution/swinir_sr_realworld_x4_gan/models/2000_G.pth\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Testing 0 buildings2_patch_0  \n",
            "Testing 1 buildings2_patch_1  \n",
            "Testing 2 buildings2_patch_10 \n",
            "Testing 3 buildings2_patch_11 \n",
            "Testing 4 buildings2_patch_12 \n",
            "Testing 5 buildings2_patch_13 \n",
            "Testing 6 buildings2_patch_14 \n",
            "Testing 7 buildings2_patch_15 \n",
            "Testing 8 buildings2_patch_16 \n",
            "Testing 9 buildings2_patch_17 \n",
            "Testing 10 buildings2_patch_18 \n",
            "Testing 11 buildings2_patch_19 \n",
            "Testing 12 buildings2_patch_2  \n",
            "Testing 13 buildings2_patch_20 \n",
            "Testing 14 buildings2_patch_21 \n",
            "Testing 15 buildings2_patch_22 \n",
            "Testing 16 buildings2_patch_23 \n",
            "Testing 17 buildings2_patch_24 \n",
            "Testing 18 buildings2_patch_25 \n",
            "Testing 19 buildings2_patch_26 \n",
            "Testing 20 buildings2_patch_27 \n",
            "Testing 21 buildings2_patch_3  \n",
            "Testing 22 buildings2_patch_4  \n",
            "Testing 23 buildings2_patch_5  \n",
            "Testing 24 buildings2_patch_6  \n",
            "Testing 25 buildings2_patch_7  \n",
            "Testing 26 buildings2_patch_8  \n",
            "Testing 27 buildings2_patch_9  \n",
            "Testing 28 dubaibuildings1_patch_0\n",
            "Testing 29 dubaibuildings1_patch_1\n",
            "Testing 30 dubaibuildings1_patch_10\n",
            "Testing 31 dubaibuildings1_patch_11\n",
            "Testing 32 dubaibuildings1_patch_12\n",
            "Testing 33 dubaibuildings1_patch_13\n",
            "Testing 34 dubaibuildings1_patch_14\n",
            "Testing 35 dubaibuildings1_patch_15\n",
            "Testing 36 dubaibuildings1_patch_16\n",
            "Testing 37 dubaibuildings1_patch_17\n",
            "Testing 38 dubaibuildings1_patch_18\n",
            "Testing 39 dubaibuildings1_patch_19\n",
            "Testing 40 dubaibuildings1_patch_2\n",
            "Testing 41 dubaibuildings1_patch_20\n",
            "Testing 42 dubaibuildings1_patch_21\n",
            "Testing 43 dubaibuildings1_patch_22\n",
            "Testing 44 dubaibuildings1_patch_23\n",
            "Testing 45 dubaibuildings1_patch_24\n",
            "Testing 46 dubaibuildings1_patch_25\n",
            "Testing 47 dubaibuildings1_patch_26\n",
            "Testing 48 dubaibuildings1_patch_27\n",
            "Testing 49 dubaibuildings1_patch_3\n",
            "Testing 50 dubaibuildings1_patch_4\n",
            "Testing 51 dubaibuildings1_patch_5\n",
            "Testing 52 dubaibuildings1_patch_6\n",
            "Testing 53 dubaibuildings1_patch_7\n",
            "Testing 54 dubaibuildings1_patch_8\n",
            "Testing 55 dubaibuildings1_patch_9\n",
            "Processing vegetation\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "loading model from /content/KAIR/superresolution/swinir_sr_realworld_x4_gan/models/2000_G.pth\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Testing 0 vegetation1_patch_10\n",
            "Testing 1 vegetation1_patch_11\n",
            "Testing 2 vegetation1_patch_12\n",
            "Testing 3 vegetation1_patch_13\n",
            "Testing 4 vegetation1_patch_14\n",
            "Testing 5 vegetation1_patch_15\n",
            "Testing 6 vegetation1_patch_16\n",
            "Testing 7 vegetation1_patch_17\n",
            "Testing 8 vegetation1_patch_18\n",
            "Testing 9 vegetation1_patch_19\n",
            "Testing 10 vegetation1_patch_20\n",
            "Testing 11 vegetation1_patch_21\n",
            "Testing 12 vegetation1_patch_22\n",
            "Testing 13 vegetation1_patch_23\n",
            "Testing 14 vegetation1_patch_6 \n",
            "Testing 15 vegetation1_patch_7 \n",
            "Testing 16 vegetation1_patch_8 \n",
            "Testing 17 vegetation1_patch_9 \n",
            "Testing 18 vegetation2_patch_0 \n",
            "Testing 19 vegetation2_patch_1 \n",
            "Testing 20 vegetation3_patch_0 \n",
            "Testing 21 vegetation3_patch_1 \n",
            "Testing 22 vegetation3_patch_10\n",
            "Testing 23 vegetation3_patch_11\n",
            "Testing 24 vegetation3_patch_12\n",
            "Testing 25 vegetation3_patch_13\n",
            "Testing 26 vegetation3_patch_14\n",
            "Testing 27 vegetation3_patch_15\n",
            "Testing 28 vegetation3_patch_16\n",
            "Testing 29 vegetation3_patch_17\n",
            "Testing 30 vegetation3_patch_18\n",
            "Testing 31 vegetation3_patch_19\n",
            "Testing 32 vegetation3_patch_2 \n",
            "Testing 33 vegetation3_patch_20\n",
            "Testing 34 vegetation3_patch_21\n",
            "Testing 35 vegetation3_patch_22\n",
            "Testing 36 vegetation3_patch_23\n",
            "Testing 37 vegetation3_patch_24\n",
            "Testing 38 vegetation3_patch_25\n",
            "Testing 39 vegetation3_patch_26\n",
            "Testing 40 vegetation3_patch_27\n",
            "Testing 41 vegetation3_patch_3 \n",
            "Testing 42 vegetation3_patch_4 \n",
            "Testing 43 vegetation3_patch_5 \n",
            "Testing 44 vegetation3_patch_6 \n",
            "Testing 45 vegetation3_patch_7 \n",
            "Testing 46 vegetation3_patch_8 \n",
            "Testing 47 vegetation3_patch_9 \n",
            "Processing desert\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "loading model from /content/KAIR/superresolution/swinir_sr_realworld_x4_gan/models/2000_G.pth\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Testing 0 desert1_patch_0     \n",
            "Testing 1 desert1_patch_1     \n",
            "Testing 2 desert1_patch_10    \n",
            "Testing 3 desert1_patch_11    \n",
            "Testing 4 desert1_patch_12    \n",
            "Testing 5 desert1_patch_13    \n",
            "Testing 6 desert1_patch_14    \n",
            "Testing 7 desert1_patch_15    \n",
            "Testing 8 desert1_patch_16    \n",
            "Testing 9 desert1_patch_17    \n",
            "Testing 10 desert1_patch_18    \n",
            "Testing 11 desert1_patch_19    \n",
            "Testing 12 desert1_patch_2     \n",
            "Testing 13 desert1_patch_3     \n",
            "Testing 14 desert1_patch_4     \n",
            "Testing 15 desert1_patch_5     \n",
            "Testing 16 desert1_patch_6     \n",
            "Testing 17 desert1_patch_7     \n",
            "Testing 18 desert1_patch_8     \n",
            "Testing 19 desert1_patch_9     \n",
            "Testing 20 desert2_patch_0     \n",
            "Testing 21 desert2_patch_1     \n",
            "Testing 22 desert2_patch_10    \n",
            "Testing 23 desert2_patch_11    \n",
            "Testing 24 desert2_patch_12    \n",
            "Testing 25 desert2_patch_13    \n",
            "Testing 26 desert2_patch_14    \n",
            "Testing 27 desert2_patch_15    \n",
            "Testing 28 desert2_patch_16    \n",
            "Testing 29 desert2_patch_17    \n",
            "Testing 30 desert2_patch_18    \n",
            "Testing 31 desert2_patch_19    \n",
            "Testing 32 desert2_patch_2     \n",
            "Testing 33 desert2_patch_20    \n",
            "Testing 34 desert2_patch_21    \n",
            "Testing 35 desert2_patch_22    \n",
            "Testing 36 desert2_patch_23    \n",
            "Testing 37 desert2_patch_24    \n",
            "Testing 38 desert2_patch_25    \n",
            "Testing 39 desert2_patch_26    \n",
            "Testing 40 desert2_patch_27    \n",
            "Testing 41 desert2_patch_3     \n",
            "Testing 42 desert2_patch_4     \n",
            "Testing 43 desert2_patch_5     \n",
            "Testing 44 desert2_patch_6     \n",
            "Testing 45 desert2_patch_7     \n",
            "Testing 46 desert2_patch_8     \n",
            "Testing 47 desert2_patch_9     \n",
            "Processing snowregion\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "loading model from /content/KAIR/superresolution/swinir_sr_realworld_x4_gan/models/2000_G.pth\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Testing 0 snowregion1_patch_0 \n",
            "Testing 1 snowregion1_patch_1 \n",
            "Testing 2 snowregion1_patch_10\n",
            "Testing 3 snowregion1_patch_11\n",
            "Testing 4 snowregion1_patch_12\n",
            "Testing 5 snowregion1_patch_13\n",
            "Testing 6 snowregion1_patch_14\n",
            "Testing 7 snowregion1_patch_15\n",
            "Testing 8 snowregion1_patch_16\n",
            "Testing 9 snowregion1_patch_17\n",
            "Testing 10 snowregion1_patch_18\n",
            "Testing 11 snowregion1_patch_19\n",
            "Testing 12 snowregion1_patch_2 \n",
            "Testing 13 snowregion1_patch_20\n",
            "Testing 14 snowregion1_patch_21\n",
            "Testing 15 snowregion1_patch_22\n",
            "Testing 16 snowregion1_patch_23\n",
            "Testing 17 snowregion1_patch_24\n",
            "Testing 18 snowregion1_patch_25\n",
            "Testing 19 snowregion1_patch_26\n",
            "Testing 20 snowregion1_patch_27\n",
            "Testing 21 snowregion1_patch_3 \n",
            "Testing 22 snowregion1_patch_4 \n",
            "Testing 23 snowregion1_patch_5 \n",
            "Testing 24 snowregion1_patch_6 \n",
            "Testing 25 snowregion1_patch_7 \n",
            "Testing 26 snowregion1_patch_8 \n",
            "Testing 27 snowregion1_patch_9 \n",
            "Testing 28 snowregion2_patch_0 \n",
            "Testing 29 snowregion2_patch_1 \n",
            "Testing 30 snowregion2_patch_10\n",
            "Testing 31 snowregion2_patch_11\n",
            "Testing 32 snowregion2_patch_12\n",
            "Testing 33 snowregion2_patch_13\n",
            "Testing 34 snowregion2_patch_14\n",
            "Testing 35 snowregion2_patch_15\n",
            "Testing 36 snowregion2_patch_16\n",
            "Testing 37 snowregion2_patch_17\n",
            "Testing 38 snowregion2_patch_18\n",
            "Testing 39 snowregion2_patch_19\n",
            "Testing 40 snowregion2_patch_2 \n",
            "Testing 41 snowregion2_patch_20\n",
            "Testing 42 snowregion2_patch_21\n",
            "Testing 43 snowregion2_patch_22\n",
            "Testing 44 snowregion2_patch_23\n",
            "Testing 45 snowregion2_patch_24\n",
            "Testing 46 snowregion2_patch_25\n",
            "Testing 47 snowregion2_patch_26\n",
            "Testing 48 snowregion2_patch_27\n",
            "Testing 49 snowregion2_patch_3 \n",
            "Testing 50 snowregion2_patch_4 \n",
            "Testing 51 snowregion2_patch_5 \n",
            "Testing 52 snowregion2_patch_6 \n",
            "Testing 53 snowregion2_patch_7 \n",
            "Testing 54 snowregion2_patch_8 \n",
            "Testing 55 snowregion2_patch_9 \n",
            "Processing water\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "loading model from /content/KAIR/superresolution/swinir_sr_realworld_x4_gan/models/2000_G.pth\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Testing 0 water1_patch_0      \n",
            "Testing 1 water1_patch_1      \n",
            "Testing 2 water1_patch_10     \n",
            "Testing 3 water1_patch_11     \n",
            "Testing 4 water1_patch_12     \n",
            "Testing 5 water1_patch_13     \n",
            "Testing 6 water1_patch_14     \n",
            "Testing 7 water1_patch_16     \n",
            "Testing 8 water1_patch_17     \n",
            "Testing 9 water1_patch_18     \n",
            "Testing 10 water1_patch_19     \n",
            "Testing 11 water1_patch_5      \n",
            "Testing 12 water1_patch_6      \n",
            "Testing 13 water1_patch_7      \n",
            "Testing 14 water1_patch_8      \n",
            "Testing 15 water2_patch_3      \n",
            "Testing 16 water2_patch_5      \n",
            "Testing 17 water2_patch_6      \n",
            "Testing 18 water2_patch_7      \n",
            "Testing 19 water3_patch_0      \n",
            "Testing 20 water3_patch_1      \n",
            "Testing 21 water3_patch_10     \n",
            "Testing 22 water3_patch_11     \n",
            "Testing 23 water3_patch_12     \n",
            "Testing 24 water3_patch_15     \n",
            "Testing 25 water3_patch_2      \n",
            "Testing 26 water3_patch_6      \n",
            "Testing 27 water3_patch_8      \n",
            "Testing 28 water3_patch_9      \n",
            "Testing 29 water4_patch_0      \n",
            "Testing 30 water4_patch_1      \n",
            "Testing 31 water4_patch_13     \n",
            "Testing 32 water4_patch_14     \n",
            "Testing 33 water4_patch_2      \n",
            "Testing 34 water4_patch_6      \n",
            "Testing 35 water4_patch_9      \n",
            "Testing 36 water5_patch_0      \n",
            "Testing 37 water5_patch_1      \n",
            "Testing 38 water5_patch_10     \n",
            "Testing 39 water5_patch_14     \n",
            "Testing 40 water5_patch_15     \n",
            "Testing 41 water5_patch_16     \n",
            "Testing 42 water5_patch_17     \n",
            "Testing 43 water5_patch_20     \n",
            "Testing 44 water5_patch_21     \n",
            "Testing 45 water5_patch_22     \n",
            "Testing 46 water5_patch_3      \n",
            "Testing 47 water5_patch_4      \n",
            "Testing 48 water5_patch_6      \n",
            "Testing 49 water5_patch_8      \n",
            "Testing 50 water5_patch_9      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Base results folder\n",
        "base_folder = \"/content/KAIR/results/swinir_real_sr_x4\"\n",
        "\n",
        "# List of category folders\n",
        "categories = [\"buildings\", \"desert\", \"snowregion\", \"vegetation\", \"water\"]\n",
        "\n",
        "# Iterate over all files in the base folder\n",
        "for fname in os.listdir(base_folder):\n",
        "    fpath = os.path.join(base_folder, fname)\n",
        "    if os.path.isfile(fpath):\n",
        "        # Check which category the filename contains\n",
        "        for cat in categories:\n",
        "            if cat in fname.lower():\n",
        "                dest_folder = os.path.join(base_folder, cat)\n",
        "                shutil.move(fpath, os.path.join(dest_folder, fname))\n",
        "                break  # once moved, stop checking other categories\n",
        "\n",
        "# Zip the entire results folder\n",
        "zip_path = \"/content/swinir/FineTuned\"\n",
        "shutil.make_archive(zip_path, 'zip', base_folder)\n",
        "\n",
        "# Download the zip\n",
        "files.download(f\"{zip_path}.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "lZny3aT5u4fp",
        "outputId": "a194d6a6-5c59-4f2c-c72e-540144664af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a3d096f1-cd57-4497-a590-65d812cb8c60\", \"FineTuned.zip\", 34783257)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3o_uivW-v3-z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}